{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sWwewmyoKBOK",
        "outputId": "3c468dc7-2d64-4a3b-9988-8f2180dbcfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "049ee2bb-ba1c-4e05-a1ec-2a951b346ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self,dname=\"Siamese\"):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
        "\n",
        "        with tf.variable_scope(dname) as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = conv+b\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.relu(tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\"))  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "    \n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.85, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GNX5a8KeKBOu",
        "outputId": "f3da0611-e690-4ff4-edf4-c56e7e11549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1Jhj9OazxPnvLcuuZsZvNFfpnnsFg88I7\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  ps = PorterStemmer()\n",
        "  tokens = [ps.stem(word) for word in tokens]\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n",
        "# quest = \"where do i find hot girls\"\n",
        "# hash_var = getThreeHash(quest)\n",
        "#print(hash_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bFQrr4tINCx2",
        "outputId": "c22a698b-70ec-45ce-e44f-189c56d89ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4OzxqKw47mcT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[:2500,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HwGPkK3SON0N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(hashString,dictionary):\n",
        "  \n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer = vectorizer.build_tokenizer()\n",
        "  vec = [0]*64881\n",
        "  \n",
        "  for token in tokenizer(hashString):\n",
        "    try:\n",
        "      vec[dictionary[token]] += 1\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "  return vec\n",
        "\n",
        "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
        "  \n",
        "  good_data = data[goodSet]\n",
        "  bad_quest = data[badSetQues,0]\n",
        "  bad_anser = data[badSetAns,1]\n",
        "  questions = np.concatenate((good_data[:,0],bad_quest))\n",
        "  answers = np.concatenate((good_data[:,1],bad_anser))\n",
        "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
        "  \n",
        "  ques,ans = [],[]\n",
        "  for d in questions:\n",
        "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
        "  for i,d in enumerate(answers):\n",
        "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
        "  return np.array(ques),np.array(ans),label\n",
        "    \n",
        "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
        "  \n",
        "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
        "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KXFS2FUwq1K0",
        "outputId": "f2d0b100-8d86-46c1-d71c-d9d98764904c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "siamese = Siamese('siamese4')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wXmBCVN0V3hv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiameseNetwork(data,siamese,batchSize,epochs,dictionary,lrate):\n",
        "  \n",
        "  #Siamese Network\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    train_Step = tf.train.MomentumOptimizer(lrate,0.05).minimize(siamese.loss)\n",
        "    tf.initialize_all_variables().run()\n",
        "    \n",
        "    while epochs > 0:\n",
        "      \n",
        "      permSet = np.random.permutation(data.shape[0])\n",
        "\n",
        "      for p in range(0,permSet.shape[0],batchSize):\n",
        "\n",
        "        goodSet = np.array(list(range(p,p+batchSize)))\n",
        "        badSetQ = goodSet.copy()\n",
        "        badSetA = np.array(permSet[p:p+batchSize])\n",
        "        ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
        "\n",
        "        \n",
        "        #print(ques.shape,ans.shape,labl.shape)\n",
        "        _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
        "                      siamese.x1: ques,\n",
        "                      siamese.x2: ans,\n",
        "                      siamese.y_: labl})\n",
        "        \n",
        "        print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
        "      \n",
        "      epochs -= 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N9m0FD0wdU-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokenlower = [word.lower() for word in tokens]\n",
        "    \n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    tokenlower = [word for word in stopWords if word not in stopWords]\n",
        "    \n",
        "    tokenDict = nltk.defaultdict(int)\n",
        "    for word in tokens:\n",
        "        tokenDict[word] += 1\n",
        "    return tokenDict\n",
        " \n",
        "\n",
        "def cosDistance(v1,v2):\n",
        "    dotProduct = np.dot(v1,v2)\n",
        "    normV1 = np.linalg.norm(v1)\n",
        "    normV2 = np.linalg.norm(v2)\n",
        "#     print(dotProduct,normV1,normV2)\n",
        "    return dotProduct / (normV1 * normV2)\n",
        "\n",
        "def getSimilarity(vDict1,vDict2):\n",
        "    allWords = []\n",
        "    for key in vDict1:\n",
        "        allWords.append(key)\n",
        "    for key in vDict2:\n",
        "        allWords.append(key)\n",
        "    allWordsSize = len(allWords)\n",
        "    \n",
        "    v1 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    v2 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    \n",
        "    i = 0\n",
        "    for key in allWords:\n",
        "#         print(key)\n",
        "        v1[i] = vDict1.get(key, 0)\n",
        "        v2[i] = vDict2.get(key, 0)\n",
        "        i += 1\n",
        "#     print(v1,v2)    \n",
        "    return cosDistance(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8_vJYXycs0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(qset,ques,siamese,dictionary,alpha=0.5):\n",
        "  \n",
        "  vques  = preprocess(ques)\n",
        "  thQues = np.array(vectorize(getThreeHash(ques.lower()),dictionary)).reshape(1,-1)\n",
        "  \n",
        "  scores = []\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    for q in qset:\n",
        "\n",
        "      oQues = np.array(vectorize(getThreeHash(q.lower()))).reshape(1,-1)\n",
        "      vQues = preprocess(q)\n",
        "      Loss = sess.run([siamese.loss],feed_dict={\n",
        "                      siamese.x1: thQues,\n",
        "                      siamese.x2: oQues,\n",
        "                      siamese.y_: np.array([1.0]).reshape(1,-1)})\n",
        "      tLoss = getSimilarity(vques,vQues)\n",
        "      score += [alpha*Loss + (1-alpha)*tLoss]\n",
        "    \n",
        "  return np.argsort(np.array(scores)) \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "scF-b7JY8NQW",
        "outputId": "0e65cac1-84f7-46c6-cafd-38e04b45c46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25245
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "trainSiameseNetwork(data,siamese,100,100,three_hash_dict,0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 100 Batch 0.0 Loss 0.12550578\n",
            "Epoch 100 Batch 1.0 Loss 0.09228745\n",
            "Epoch 100 Batch 2.0 Loss 0.08253804\n",
            "Epoch 100 Batch 3.0 Loss 0.07925073\n",
            "Epoch 100 Batch 4.0 Loss 0.07405254\n",
            "Epoch 100 Batch 5.0 Loss 0.07136722\n",
            "Epoch 100 Batch 6.0 Loss 0.06787529\n",
            "Epoch 100 Batch 7.0 Loss 0.07214838\n",
            "Epoch 100 Batch 8.0 Loss 0.070221804\n",
            "Epoch 100 Batch 9.0 Loss 0.07007259\n",
            "Epoch 100 Batch 10.0 Loss 0.06565568\n",
            "Epoch 100 Batch 11.0 Loss 0.06987453\n",
            "Epoch 100 Batch 12.0 Loss 0.07012979\n",
            "Epoch 100 Batch 13.0 Loss 0.06801913\n",
            "Epoch 100 Batch 14.0 Loss 0.06617929\n",
            "Epoch 100 Batch 15.0 Loss 0.06388925\n",
            "Epoch 100 Batch 16.0 Loss 0.062067878\n",
            "Epoch 100 Batch 17.0 Loss 0.067275986\n",
            "Epoch 100 Batch 18.0 Loss 0.06678785\n",
            "Epoch 100 Batch 19.0 Loss 0.06563822\n",
            "Epoch 100 Batch 20.0 Loss 0.065542884\n",
            "Epoch 100 Batch 21.0 Loss 0.06333957\n",
            "Epoch 100 Batch 22.0 Loss 0.062446028\n",
            "Epoch 100 Batch 23.0 Loss 0.06757489\n",
            "Epoch 100 Batch 24.0 Loss 0.07030943\n",
            "Epoch 99 Batch 0.0 Loss 0.078366846\n",
            "Epoch 99 Batch 1.0 Loss 0.07250276\n",
            "Epoch 99 Batch 2.0 Loss 0.07273697\n",
            "Epoch 99 Batch 3.0 Loss 0.07131124\n",
            "Epoch 99 Batch 4.0 Loss 0.06341719\n",
            "Epoch 99 Batch 5.0 Loss 0.06833391\n",
            "Epoch 99 Batch 6.0 Loss 0.06759823\n",
            "Epoch 99 Batch 7.0 Loss 0.071018904\n",
            "Epoch 99 Batch 8.0 Loss 0.06777822\n",
            "Epoch 99 Batch 9.0 Loss 0.07064979\n",
            "Epoch 99 Batch 10.0 Loss 0.065016486\n",
            "Epoch 99 Batch 11.0 Loss 0.06681756\n",
            "Epoch 99 Batch 12.0 Loss 0.066602394\n",
            "Epoch 99 Batch 13.0 Loss 0.06829889\n",
            "Epoch 99 Batch 14.0 Loss 0.06438693\n",
            "Epoch 99 Batch 15.0 Loss 0.06069553\n",
            "Epoch 99 Batch 16.0 Loss 0.0664334\n",
            "Epoch 99 Batch 17.0 Loss 0.06740898\n",
            "Epoch 99 Batch 18.0 Loss 0.06455284\n",
            "Epoch 99 Batch 19.0 Loss 0.06441206\n",
            "Epoch 99 Batch 20.0 Loss 0.06486795\n",
            "Epoch 99 Batch 21.0 Loss 0.06370814\n",
            "Epoch 99 Batch 22.0 Loss 0.059184477\n",
            "Epoch 99 Batch 23.0 Loss 0.0659782\n",
            "Epoch 99 Batch 24.0 Loss 0.066126145\n",
            "Epoch 98 Batch 0.0 Loss 0.071452424\n",
            "Epoch 98 Batch 1.0 Loss 0.07039228\n",
            "Epoch 98 Batch 2.0 Loss 0.067244776\n",
            "Epoch 98 Batch 3.0 Loss 0.07021869\n",
            "Epoch 98 Batch 4.0 Loss 0.0696141\n",
            "Epoch 98 Batch 5.0 Loss 0.066437155\n",
            "Epoch 98 Batch 6.0 Loss 0.06940177\n",
            "Epoch 98 Batch 7.0 Loss 0.06492353\n",
            "Epoch 98 Batch 8.0 Loss 0.06707959\n",
            "Epoch 98 Batch 9.0 Loss 0.06542598\n",
            "Epoch 98 Batch 10.0 Loss 0.06598978\n",
            "Epoch 98 Batch 11.0 Loss 0.062977456\n",
            "Epoch 98 Batch 12.0 Loss 0.06456579\n",
            "Epoch 98 Batch 13.0 Loss 0.06775168\n",
            "Epoch 98 Batch 14.0 Loss 0.060816236\n",
            "Epoch 98 Batch 15.0 Loss 0.06096819\n",
            "Epoch 98 Batch 16.0 Loss 0.06456429\n",
            "Epoch 98 Batch 17.0 Loss 0.06590233\n",
            "Epoch 98 Batch 18.0 Loss 0.065986805\n",
            "Epoch 98 Batch 19.0 Loss 0.06413893\n",
            "Epoch 98 Batch 20.0 Loss 0.06166259\n",
            "Epoch 98 Batch 21.0 Loss 0.06220294\n",
            "Epoch 98 Batch 22.0 Loss 0.061417133\n",
            "Epoch 98 Batch 23.0 Loss 0.06790986\n",
            "Epoch 98 Batch 24.0 Loss 0.06752991\n",
            "Epoch 97 Batch 0.0 Loss 0.067044266\n",
            "Epoch 97 Batch 1.0 Loss 0.07109227\n",
            "Epoch 97 Batch 2.0 Loss 0.07027954\n",
            "Epoch 97 Batch 3.0 Loss 0.06747117\n",
            "Epoch 97 Batch 4.0 Loss 0.0683709\n",
            "Epoch 97 Batch 5.0 Loss 0.06735519\n",
            "Epoch 97 Batch 6.0 Loss 0.06006716\n",
            "Epoch 97 Batch 7.0 Loss 0.06450549\n",
            "Epoch 97 Batch 8.0 Loss 0.064635016\n",
            "Epoch 97 Batch 9.0 Loss 0.06306103\n",
            "Epoch 97 Batch 10.0 Loss 0.0642961\n",
            "Epoch 97 Batch 11.0 Loss 0.062094904\n",
            "Epoch 97 Batch 12.0 Loss 0.064502046\n",
            "Epoch 97 Batch 13.0 Loss 0.0649343\n",
            "Epoch 97 Batch 14.0 Loss 0.06127247\n",
            "Epoch 97 Batch 15.0 Loss 0.061483014\n",
            "Epoch 97 Batch 16.0 Loss 0.0657001\n",
            "Epoch 97 Batch 17.0 Loss 0.062593125\n",
            "Epoch 97 Batch 18.0 Loss 0.066132605\n",
            "Epoch 97 Batch 19.0 Loss 0.06362753\n",
            "Epoch 97 Batch 20.0 Loss 0.06419238\n",
            "Epoch 97 Batch 21.0 Loss 0.06600146\n",
            "Epoch 97 Batch 22.0 Loss 0.059727512\n",
            "Epoch 97 Batch 23.0 Loss 0.067439884\n",
            "Epoch 97 Batch 24.0 Loss 0.06723568\n",
            "Epoch 96 Batch 0.0 Loss 0.064181104\n",
            "Epoch 96 Batch 1.0 Loss 0.06935589\n",
            "Epoch 96 Batch 2.0 Loss 0.06617837\n",
            "Epoch 96 Batch 3.0 Loss 0.06536207\n",
            "Epoch 96 Batch 4.0 Loss 0.06813109\n",
            "Epoch 96 Batch 5.0 Loss 0.0648041\n",
            "Epoch 96 Batch 6.0 Loss 0.06182258\n",
            "Epoch 96 Batch 7.0 Loss 0.06349766\n",
            "Epoch 96 Batch 8.0 Loss 0.06450904\n",
            "Epoch 96 Batch 9.0 Loss 0.06420914\n",
            "Epoch 96 Batch 10.0 Loss 0.06162841\n",
            "Epoch 96 Batch 11.0 Loss 0.060243815\n",
            "Epoch 96 Batch 12.0 Loss 0.06365761\n",
            "Epoch 96 Batch 13.0 Loss 0.06417331\n",
            "Epoch 96 Batch 14.0 Loss 0.061112147\n",
            "Epoch 96 Batch 15.0 Loss 0.060871504\n",
            "Epoch 96 Batch 16.0 Loss 0.0665405\n",
            "Epoch 96 Batch 17.0 Loss 0.065952614\n",
            "Epoch 96 Batch 18.0 Loss 0.06320434\n",
            "Epoch 96 Batch 19.0 Loss 0.062390007\n",
            "Epoch 96 Batch 20.0 Loss 0.06269057\n",
            "Epoch 96 Batch 21.0 Loss 0.06428394\n",
            "Epoch 96 Batch 22.0 Loss 0.060823563\n",
            "Epoch 96 Batch 23.0 Loss 0.06691007\n",
            "Epoch 96 Batch 24.0 Loss 0.064847305\n",
            "Epoch 95 Batch 0.0 Loss 0.06376794\n",
            "Epoch 95 Batch 1.0 Loss 0.066430554\n",
            "Epoch 95 Batch 2.0 Loss 0.06739702\n",
            "Epoch 95 Batch 3.0 Loss 0.06438722\n",
            "Epoch 95 Batch 4.0 Loss 0.067065865\n",
            "Epoch 95 Batch 5.0 Loss 0.06465066\n",
            "Epoch 95 Batch 6.0 Loss 0.06041145\n",
            "Epoch 95 Batch 7.0 Loss 0.061838657\n",
            "Epoch 95 Batch 8.0 Loss 0.06556146\n",
            "Epoch 95 Batch 9.0 Loss 0.06320136\n",
            "Epoch 95 Batch 10.0 Loss 0.06441064\n",
            "Epoch 95 Batch 11.0 Loss 0.058810215\n",
            "Epoch 95 Batch 12.0 Loss 0.06303757\n",
            "Epoch 95 Batch 13.0 Loss 0.0640226\n",
            "Epoch 95 Batch 14.0 Loss 0.062382046\n",
            "Epoch 95 Batch 15.0 Loss 0.05790977\n",
            "Epoch 95 Batch 16.0 Loss 0.06542748\n",
            "Epoch 95 Batch 17.0 Loss 0.06695641\n",
            "Epoch 95 Batch 18.0 Loss 0.067014456\n",
            "Epoch 95 Batch 19.0 Loss 0.062429477\n",
            "Epoch 95 Batch 20.0 Loss 0.06041215\n",
            "Epoch 95 Batch 21.0 Loss 0.06502893\n",
            "Epoch 95 Batch 22.0 Loss 0.061024856\n",
            "Epoch 95 Batch 23.0 Loss 0.06744254\n",
            "Epoch 95 Batch 24.0 Loss 0.06472832\n",
            "Epoch 94 Batch 0.0 Loss 0.064263254\n",
            "Epoch 94 Batch 1.0 Loss 0.06639765\n",
            "Epoch 94 Batch 2.0 Loss 0.06417863\n",
            "Epoch 94 Batch 3.0 Loss 0.062471196\n",
            "Epoch 94 Batch 4.0 Loss 0.0658437\n",
            "Epoch 94 Batch 5.0 Loss 0.06354871\n",
            "Epoch 94 Batch 6.0 Loss 0.062672906\n",
            "Epoch 94 Batch 7.0 Loss 0.060670126\n",
            "Epoch 94 Batch 8.0 Loss 0.06346927\n",
            "Epoch 94 Batch 9.0 Loss 0.063172966\n",
            "Epoch 94 Batch 10.0 Loss 0.061436072\n",
            "Epoch 94 Batch 11.0 Loss 0.06239227\n",
            "Epoch 94 Batch 12.0 Loss 0.06204979\n",
            "Epoch 94 Batch 13.0 Loss 0.060654245\n",
            "Epoch 94 Batch 14.0 Loss 0.06288178\n",
            "Epoch 94 Batch 15.0 Loss 0.06016823\n",
            "Epoch 94 Batch 16.0 Loss 0.065135546\n",
            "Epoch 94 Batch 17.0 Loss 0.065230355\n",
            "Epoch 94 Batch 18.0 Loss 0.065371074\n",
            "Epoch 94 Batch 19.0 Loss 0.06353561\n",
            "Epoch 94 Batch 20.0 Loss 0.06193938\n",
            "Epoch 94 Batch 21.0 Loss 0.065121576\n",
            "Epoch 94 Batch 22.0 Loss 0.061810493\n",
            "Epoch 94 Batch 23.0 Loss 0.06724562\n",
            "Epoch 94 Batch 24.0 Loss 0.06596109\n",
            "Epoch 93 Batch 0.0 Loss 0.060642492\n",
            "Epoch 93 Batch 1.0 Loss 0.06619984\n",
            "Epoch 93 Batch 2.0 Loss 0.06373056\n",
            "Epoch 93 Batch 3.0 Loss 0.06241831\n",
            "Epoch 93 Batch 4.0 Loss 0.06522093\n",
            "Epoch 93 Batch 5.0 Loss 0.060980767\n",
            "Epoch 93 Batch 6.0 Loss 0.061859313\n",
            "Epoch 93 Batch 7.0 Loss 0.06011503\n",
            "Epoch 93 Batch 8.0 Loss 0.06305239\n",
            "Epoch 93 Batch 9.0 Loss 0.06385808\n",
            "Epoch 93 Batch 10.0 Loss 0.0635852\n",
            "Epoch 93 Batch 11.0 Loss 0.05960207\n",
            "Epoch 93 Batch 12.0 Loss 0.061674997\n",
            "Epoch 93 Batch 13.0 Loss 0.061402373\n",
            "Epoch 93 Batch 14.0 Loss 0.06125716\n",
            "Epoch 93 Batch 15.0 Loss 0.05953717\n",
            "Epoch 93 Batch 16.0 Loss 0.0666153\n",
            "Epoch 93 Batch 17.0 Loss 0.065835506\n",
            "Epoch 93 Batch 18.0 Loss 0.06484552\n",
            "Epoch 93 Batch 19.0 Loss 0.062003225\n",
            "Epoch 93 Batch 20.0 Loss 0.06084137\n",
            "Epoch 93 Batch 21.0 Loss 0.066067636\n",
            "Epoch 93 Batch 22.0 Loss 0.06161778\n",
            "Epoch 93 Batch 23.0 Loss 0.06608548\n",
            "Epoch 93 Batch 24.0 Loss 0.06582478\n",
            "Epoch 92 Batch 0.0 Loss 0.06152771\n",
            "Epoch 92 Batch 1.0 Loss 0.06365573\n",
            "Epoch 92 Batch 2.0 Loss 0.06402787\n",
            "Epoch 92 Batch 3.0 Loss 0.060794964\n",
            "Epoch 92 Batch 4.0 Loss 0.066196226\n",
            "Epoch 92 Batch 5.0 Loss 0.06195162\n",
            "Epoch 92 Batch 6.0 Loss 0.06192138\n",
            "Epoch 92 Batch 7.0 Loss 0.059884414\n",
            "Epoch 92 Batch 8.0 Loss 0.06328832\n",
            "Epoch 92 Batch 9.0 Loss 0.061268948\n",
            "Epoch 92 Batch 10.0 Loss 0.06178444\n",
            "Epoch 92 Batch 11.0 Loss 0.060554776\n",
            "Epoch 92 Batch 12.0 Loss 0.06143464\n",
            "Epoch 92 Batch 13.0 Loss 0.06294482\n",
            "Epoch 92 Batch 14.0 Loss 0.059050612\n",
            "Epoch 92 Batch 15.0 Loss 0.06081206\n",
            "Epoch 92 Batch 16.0 Loss 0.06628936\n",
            "Epoch 92 Batch 17.0 Loss 0.06458465\n",
            "Epoch 92 Batch 18.0 Loss 0.064249426\n",
            "Epoch 92 Batch 19.0 Loss 0.06258392\n",
            "Epoch 92 Batch 20.0 Loss 0.060102817\n",
            "Epoch 92 Batch 21.0 Loss 0.06545821\n",
            "Epoch 92 Batch 22.0 Loss 0.061591115\n",
            "Epoch 92 Batch 23.0 Loss 0.0662634\n",
            "Epoch 92 Batch 24.0 Loss 0.0650555\n",
            "Epoch 91 Batch 0.0 Loss 0.06263307\n",
            "Epoch 91 Batch 1.0 Loss 0.064566\n",
            "Epoch 91 Batch 2.0 Loss 0.06432971\n",
            "Epoch 91 Batch 3.0 Loss 0.05863089\n",
            "Epoch 91 Batch 4.0 Loss 0.06649114\n",
            "Epoch 91 Batch 5.0 Loss 0.06421452\n",
            "Epoch 91 Batch 6.0 Loss 0.059315223\n",
            "Epoch 91 Batch 7.0 Loss 0.061207667\n",
            "Epoch 91 Batch 8.0 Loss 0.06336677\n",
            "Epoch 91 Batch 9.0 Loss 0.061894983\n",
            "Epoch 91 Batch 10.0 Loss 0.06310728\n",
            "Epoch 91 Batch 11.0 Loss 0.060278516\n",
            "Epoch 91 Batch 12.0 Loss 0.061261438\n",
            "Epoch 91 Batch 13.0 Loss 0.06059837\n",
            "Epoch 91 Batch 14.0 Loss 0.059065957\n",
            "Epoch 91 Batch 15.0 Loss 0.060597774\n",
            "Epoch 91 Batch 16.0 Loss 0.066689484\n",
            "Epoch 91 Batch 17.0 Loss 0.06497626\n",
            "Epoch 91 Batch 18.0 Loss 0.06531691\n",
            "Epoch 91 Batch 19.0 Loss 0.062018953\n",
            "Epoch 91 Batch 20.0 Loss 0.06134871\n",
            "Epoch 91 Batch 21.0 Loss 0.0647022\n",
            "Epoch 91 Batch 22.0 Loss 0.06113487\n",
            "Epoch 91 Batch 23.0 Loss 0.064370744\n",
            "Epoch 91 Batch 24.0 Loss 0.06625517\n",
            "Epoch 90 Batch 0.0 Loss 0.06169115\n",
            "Epoch 90 Batch 1.0 Loss 0.063297294\n",
            "Epoch 90 Batch 2.0 Loss 0.06252603\n",
            "Epoch 90 Batch 3.0 Loss 0.061708644\n",
            "Epoch 90 Batch 4.0 Loss 0.0663042\n",
            "Epoch 90 Batch 5.0 Loss 0.06445468\n",
            "Epoch 90 Batch 6.0 Loss 0.06065865\n",
            "Epoch 90 Batch 7.0 Loss 0.05984275\n",
            "Epoch 90 Batch 8.0 Loss 0.061926436\n",
            "Epoch 90 Batch 9.0 Loss 0.06260669\n",
            "Epoch 90 Batch 10.0 Loss 0.06273893\n",
            "Epoch 90 Batch 11.0 Loss 0.060745943\n",
            "Epoch 90 Batch 12.0 Loss 0.05902387\n",
            "Epoch 90 Batch 13.0 Loss 0.05858764\n",
            "Epoch 90 Batch 14.0 Loss 0.060224038\n",
            "Epoch 90 Batch 15.0 Loss 0.059858285\n",
            "Epoch 90 Batch 16.0 Loss 0.065898255\n",
            "Epoch 90 Batch 17.0 Loss 0.06485133\n",
            "Epoch 90 Batch 18.0 Loss 0.064390905\n",
            "Epoch 90 Batch 19.0 Loss 0.0630431\n",
            "Epoch 90 Batch 20.0 Loss 0.061546985\n",
            "Epoch 90 Batch 21.0 Loss 0.063847445\n",
            "Epoch 90 Batch 22.0 Loss 0.06232521\n",
            "Epoch 90 Batch 23.0 Loss 0.06562791\n",
            "Epoch 90 Batch 24.0 Loss 0.062988274\n",
            "Epoch 89 Batch 0.0 Loss 0.060688753\n",
            "Epoch 89 Batch 1.0 Loss 0.06414065\n",
            "Epoch 89 Batch 2.0 Loss 0.061732993\n",
            "Epoch 89 Batch 3.0 Loss 0.06046959\n",
            "Epoch 89 Batch 4.0 Loss 0.06694895\n",
            "Epoch 89 Batch 5.0 Loss 0.06172539\n",
            "Epoch 89 Batch 6.0 Loss 0.061519414\n",
            "Epoch 89 Batch 7.0 Loss 0.05945424\n",
            "Epoch 89 Batch 8.0 Loss 0.062349506\n",
            "Epoch 89 Batch 9.0 Loss 0.062072173\n",
            "Epoch 89 Batch 10.0 Loss 0.060579978\n",
            "Epoch 89 Batch 11.0 Loss 0.058164142\n",
            "Epoch 89 Batch 12.0 Loss 0.059690766\n",
            "Epoch 89 Batch 13.0 Loss 0.06013256\n",
            "Epoch 89 Batch 14.0 Loss 0.060769733\n",
            "Epoch 89 Batch 15.0 Loss 0.059735145\n",
            "Epoch 89 Batch 16.0 Loss 0.06400885\n",
            "Epoch 89 Batch 17.0 Loss 0.06543273\n",
            "Epoch 89 Batch 18.0 Loss 0.06781143\n",
            "Epoch 89 Batch 19.0 Loss 0.061431702\n",
            "Epoch 89 Batch 20.0 Loss 0.061404433\n",
            "Epoch 89 Batch 21.0 Loss 0.062764056\n",
            "Epoch 89 Batch 22.0 Loss 0.060275644\n",
            "Epoch 89 Batch 23.0 Loss 0.063517645\n",
            "Epoch 89 Batch 24.0 Loss 0.06486781\n",
            "Epoch 88 Batch 0.0 Loss 0.060868196\n",
            "Epoch 88 Batch 1.0 Loss 0.06478751\n",
            "Epoch 88 Batch 2.0 Loss 0.06280275\n",
            "Epoch 88 Batch 3.0 Loss 0.060877476\n",
            "Epoch 88 Batch 4.0 Loss 0.06322236\n",
            "Epoch 88 Batch 5.0 Loss 0.06419438\n",
            "Epoch 88 Batch 6.0 Loss 0.0578406\n",
            "Epoch 88 Batch 7.0 Loss 0.059359755\n",
            "Epoch 88 Batch 8.0 Loss 0.062076613\n",
            "Epoch 88 Batch 9.0 Loss 0.062196054\n",
            "Epoch 88 Batch 10.0 Loss 0.06327562\n",
            "Epoch 88 Batch 11.0 Loss 0.06114938\n",
            "Epoch 88 Batch 12.0 Loss 0.05975359\n",
            "Epoch 88 Batch 13.0 Loss 0.0608433\n",
            "Epoch 88 Batch 14.0 Loss 0.05868316\n",
            "Epoch 88 Batch 15.0 Loss 0.061044954\n",
            "Epoch 88 Batch 16.0 Loss 0.06640377\n",
            "Epoch 88 Batch 17.0 Loss 0.06460192\n",
            "Epoch 88 Batch 18.0 Loss 0.06462262\n",
            "Epoch 88 Batch 19.0 Loss 0.061035108\n",
            "Epoch 88 Batch 20.0 Loss 0.060089946\n",
            "Epoch 88 Batch 21.0 Loss 0.063832775\n",
            "Epoch 88 Batch 22.0 Loss 0.060591046\n",
            "Epoch 88 Batch 23.0 Loss 0.06349915\n",
            "Epoch 88 Batch 24.0 Loss 0.06683877\n",
            "Epoch 87 Batch 0.0 Loss 0.061636854\n",
            "Epoch 87 Batch 1.0 Loss 0.06316711\n",
            "Epoch 87 Batch 2.0 Loss 0.06487061\n",
            "Epoch 87 Batch 3.0 Loss 0.06034042\n",
            "Epoch 87 Batch 4.0 Loss 0.0644596\n",
            "Epoch 87 Batch 5.0 Loss 0.06283456\n",
            "Epoch 87 Batch 6.0 Loss 0.05980704\n",
            "Epoch 87 Batch 7.0 Loss 0.060418405\n",
            "Epoch 87 Batch 8.0 Loss 0.061498545\n",
            "Epoch 87 Batch 9.0 Loss 0.061644215\n",
            "Epoch 87 Batch 10.0 Loss 0.061177623\n",
            "Epoch 87 Batch 11.0 Loss 0.058255214\n",
            "Epoch 87 Batch 12.0 Loss 0.06121608\n",
            "Epoch 87 Batch 13.0 Loss 0.060157213\n",
            "Epoch 87 Batch 14.0 Loss 0.059623275\n",
            "Epoch 87 Batch 15.0 Loss 0.059910618\n",
            "Epoch 87 Batch 16.0 Loss 0.065578446\n",
            "Epoch 87 Batch 17.0 Loss 0.06278456\n",
            "Epoch 87 Batch 18.0 Loss 0.06411071\n",
            "Epoch 87 Batch 19.0 Loss 0.06248007\n",
            "Epoch 87 Batch 20.0 Loss 0.059540596\n",
            "Epoch 87 Batch 21.0 Loss 0.06424027\n",
            "Epoch 87 Batch 22.0 Loss 0.062646694\n",
            "Epoch 87 Batch 23.0 Loss 0.0658948\n",
            "Epoch 87 Batch 24.0 Loss 0.064515546\n",
            "Epoch 86 Batch 0.0 Loss 0.06180825\n",
            "Epoch 86 Batch 1.0 Loss 0.0635657\n",
            "Epoch 86 Batch 2.0 Loss 0.06359002\n",
            "Epoch 86 Batch 3.0 Loss 0.05993789\n",
            "Epoch 86 Batch 4.0 Loss 0.06340513\n",
            "Epoch 86 Batch 5.0 Loss 0.060489226\n",
            "Epoch 86 Batch 6.0 Loss 0.057607498\n",
            "Epoch 86 Batch 7.0 Loss 0.058452107\n",
            "Epoch 86 Batch 8.0 Loss 0.06269007\n",
            "Epoch 86 Batch 9.0 Loss 0.061079882\n",
            "Epoch 86 Batch 10.0 Loss 0.059168696\n",
            "Epoch 86 Batch 11.0 Loss 0.060817033\n",
            "Epoch 86 Batch 12.0 Loss 0.061347924\n",
            "Epoch 86 Batch 13.0 Loss 0.059933748\n",
            "Epoch 86 Batch 14.0 Loss 0.058077108\n",
            "Epoch 86 Batch 15.0 Loss 0.0609004\n",
            "Epoch 86 Batch 16.0 Loss 0.064946376\n",
            "Epoch 86 Batch 17.0 Loss 0.063637786\n",
            "Epoch 86 Batch 18.0 Loss 0.063760675\n",
            "Epoch 86 Batch 19.0 Loss 0.0633272\n",
            "Epoch 86 Batch 20.0 Loss 0.061042495\n",
            "Epoch 86 Batch 21.0 Loss 0.06328203\n",
            "Epoch 86 Batch 22.0 Loss 0.059752878\n",
            "Epoch 86 Batch 23.0 Loss 0.0634358\n",
            "Epoch 86 Batch 24.0 Loss 0.06519721\n",
            "Epoch 85 Batch 0.0 Loss 0.060494844\n",
            "Epoch 85 Batch 1.0 Loss 0.062909365\n",
            "Epoch 85 Batch 2.0 Loss 0.061667345\n",
            "Epoch 85 Batch 3.0 Loss 0.05924096\n",
            "Epoch 85 Batch 4.0 Loss 0.06620302\n",
            "Epoch 85 Batch 5.0 Loss 0.06115105\n",
            "Epoch 85 Batch 6.0 Loss 0.0589718\n",
            "Epoch 85 Batch 7.0 Loss 0.0571405\n",
            "Epoch 85 Batch 8.0 Loss 0.061639376\n",
            "Epoch 85 Batch 9.0 Loss 0.061467342\n",
            "Epoch 85 Batch 10.0 Loss 0.060865644\n",
            "Epoch 85 Batch 11.0 Loss 0.058044344\n",
            "Epoch 85 Batch 12.0 Loss 0.061067928\n",
            "Epoch 85 Batch 13.0 Loss 0.05947914\n",
            "Epoch 85 Batch 14.0 Loss 0.05886139\n",
            "Epoch 85 Batch 15.0 Loss 0.059867248\n",
            "Epoch 85 Batch 16.0 Loss 0.06389804\n",
            "Epoch 85 Batch 17.0 Loss 0.06328876\n",
            "Epoch 85 Batch 18.0 Loss 0.06376307\n",
            "Epoch 85 Batch 19.0 Loss 0.061106216\n",
            "Epoch 85 Batch 20.0 Loss 0.061113335\n",
            "Epoch 85 Batch 21.0 Loss 0.06440869\n",
            "Epoch 85 Batch 22.0 Loss 0.059388466\n",
            "Epoch 85 Batch 23.0 Loss 0.064674035\n",
            "Epoch 85 Batch 24.0 Loss 0.06485869\n",
            "Epoch 84 Batch 0.0 Loss 0.060486503\n",
            "Epoch 84 Batch 1.0 Loss 0.062438402\n",
            "Epoch 84 Batch 2.0 Loss 0.06150424\n",
            "Epoch 84 Batch 3.0 Loss 0.05998024\n",
            "Epoch 84 Batch 4.0 Loss 0.064833574\n",
            "Epoch 84 Batch 5.0 Loss 0.060586322\n",
            "Epoch 84 Batch 6.0 Loss 0.058536176\n",
            "Epoch 84 Batch 7.0 Loss 0.059175424\n",
            "Epoch 84 Batch 8.0 Loss 0.06182559\n",
            "Epoch 84 Batch 9.0 Loss 0.060881685\n",
            "Epoch 84 Batch 10.0 Loss 0.061483547\n",
            "Epoch 84 Batch 11.0 Loss 0.059705984\n",
            "Epoch 84 Batch 12.0 Loss 0.057993617\n",
            "Epoch 84 Batch 13.0 Loss 0.060086437\n",
            "Epoch 84 Batch 14.0 Loss 0.056079913\n",
            "Epoch 84 Batch 15.0 Loss 0.059081677\n",
            "Epoch 84 Batch 16.0 Loss 0.064121895\n",
            "Epoch 84 Batch 17.0 Loss 0.06468204\n",
            "Epoch 84 Batch 18.0 Loss 0.06518393\n",
            "Epoch 84 Batch 19.0 Loss 0.062847234\n",
            "Epoch 84 Batch 20.0 Loss 0.059187986\n",
            "Epoch 84 Batch 21.0 Loss 0.06248905\n",
            "Epoch 84 Batch 22.0 Loss 0.06102982\n",
            "Epoch 84 Batch 23.0 Loss 0.065197825\n",
            "Epoch 84 Batch 24.0 Loss 0.064638294\n",
            "Epoch 83 Batch 0.0 Loss 0.061696976\n",
            "Epoch 83 Batch 1.0 Loss 0.059241716\n",
            "Epoch 83 Batch 2.0 Loss 0.062262584\n",
            "Epoch 83 Batch 3.0 Loss 0.05692177\n",
            "Epoch 83 Batch 4.0 Loss 0.06626893\n",
            "Epoch 83 Batch 5.0 Loss 0.06083366\n",
            "Epoch 83 Batch 6.0 Loss 0.05999616\n",
            "Epoch 83 Batch 7.0 Loss 0.059537075\n",
            "Epoch 83 Batch 8.0 Loss 0.06217275\n",
            "Epoch 83 Batch 9.0 Loss 0.058914118\n",
            "Epoch 83 Batch 10.0 Loss 0.061178207\n",
            "Epoch 83 Batch 11.0 Loss 0.06097574\n",
            "Epoch 83 Batch 12.0 Loss 0.061080877\n",
            "Epoch 83 Batch 13.0 Loss 0.059549723\n",
            "Epoch 83 Batch 14.0 Loss 0.05839613\n",
            "Epoch 83 Batch 15.0 Loss 0.059344836\n",
            "Epoch 83 Batch 16.0 Loss 0.06321734\n",
            "Epoch 83 Batch 17.0 Loss 0.06569141\n",
            "Epoch 83 Batch 18.0 Loss 0.06293635\n",
            "Epoch 83 Batch 19.0 Loss 0.063691765\n",
            "Epoch 83 Batch 20.0 Loss 0.058309287\n",
            "Epoch 83 Batch 21.0 Loss 0.06394995\n",
            "Epoch 83 Batch 22.0 Loss 0.061747123\n",
            "Epoch 83 Batch 23.0 Loss 0.06367638\n",
            "Epoch 83 Batch 24.0 Loss 0.063378744\n",
            "Epoch 82 Batch 0.0 Loss 0.05969439\n",
            "Epoch 82 Batch 1.0 Loss 0.06201547\n",
            "Epoch 82 Batch 2.0 Loss 0.060833536\n",
            "Epoch 82 Batch 3.0 Loss 0.056839522\n",
            "Epoch 82 Batch 4.0 Loss 0.06386768\n",
            "Epoch 82 Batch 5.0 Loss 0.060422\n",
            "Epoch 82 Batch 6.0 Loss 0.058870345\n",
            "Epoch 82 Batch 7.0 Loss 0.05982081\n",
            "Epoch 82 Batch 8.0 Loss 0.060989853\n",
            "Epoch 82 Batch 9.0 Loss 0.059892643\n",
            "Epoch 82 Batch 10.0 Loss 0.06013136\n",
            "Epoch 82 Batch 11.0 Loss 0.058944758\n",
            "Epoch 82 Batch 12.0 Loss 0.059914663\n",
            "Epoch 82 Batch 13.0 Loss 0.05872028\n",
            "Epoch 82 Batch 14.0 Loss 0.06094553\n",
            "Epoch 82 Batch 15.0 Loss 0.060435925\n",
            "Epoch 82 Batch 16.0 Loss 0.0652066\n",
            "Epoch 82 Batch 17.0 Loss 0.06250798\n",
            "Epoch 82 Batch 18.0 Loss 0.06281156\n",
            "Epoch 82 Batch 19.0 Loss 0.061774667\n",
            "Epoch 82 Batch 20.0 Loss 0.05855954\n",
            "Epoch 82 Batch 21.0 Loss 0.06263691\n",
            "Epoch 82 Batch 22.0 Loss 0.061440807\n",
            "Epoch 82 Batch 23.0 Loss 0.065160185\n",
            "Epoch 82 Batch 24.0 Loss 0.06241766\n",
            "Epoch 81 Batch 0.0 Loss 0.060279425\n",
            "Epoch 81 Batch 1.0 Loss 0.062445827\n",
            "Epoch 81 Batch 2.0 Loss 0.061670102\n",
            "Epoch 81 Batch 3.0 Loss 0.058769092\n",
            "Epoch 81 Batch 4.0 Loss 0.06453293\n",
            "Epoch 81 Batch 5.0 Loss 0.06175097\n",
            "Epoch 81 Batch 6.0 Loss 0.059755735\n",
            "Epoch 81 Batch 7.0 Loss 0.059076697\n",
            "Epoch 81 Batch 8.0 Loss 0.060345195\n",
            "Epoch 81 Batch 9.0 Loss 0.059621982\n",
            "Epoch 81 Batch 10.0 Loss 0.062055178\n",
            "Epoch 81 Batch 11.0 Loss 0.06047264\n",
            "Epoch 81 Batch 12.0 Loss 0.058907326\n",
            "Epoch 81 Batch 13.0 Loss 0.057638455\n",
            "Epoch 81 Batch 14.0 Loss 0.06035016\n",
            "Epoch 81 Batch 15.0 Loss 0.059921823\n",
            "Epoch 81 Batch 16.0 Loss 0.065667674\n",
            "Epoch 81 Batch 17.0 Loss 0.060657725\n",
            "Epoch 81 Batch 18.0 Loss 0.06337548\n",
            "Epoch 81 Batch 19.0 Loss 0.061567865\n",
            "Epoch 81 Batch 20.0 Loss 0.058935188\n",
            "Epoch 81 Batch 21.0 Loss 0.063028954\n",
            "Epoch 81 Batch 22.0 Loss 0.059987295\n",
            "Epoch 81 Batch 23.0 Loss 0.065089576\n",
            "Epoch 81 Batch 24.0 Loss 0.06436847\n",
            "Epoch 80 Batch 0.0 Loss 0.058400072\n",
            "Epoch 80 Batch 1.0 Loss 0.059035946\n",
            "Epoch 80 Batch 2.0 Loss 0.06310465\n",
            "Epoch 80 Batch 3.0 Loss 0.058631502\n",
            "Epoch 80 Batch 4.0 Loss 0.06307404\n",
            "Epoch 80 Batch 5.0 Loss 0.06371794\n",
            "Epoch 80 Batch 6.0 Loss 0.057610102\n",
            "Epoch 80 Batch 7.0 Loss 0.058912862\n",
            "Epoch 80 Batch 8.0 Loss 0.062144995\n",
            "Epoch 80 Batch 9.0 Loss 0.06107116\n",
            "Epoch 80 Batch 10.0 Loss 0.060365696\n",
            "Epoch 80 Batch 11.0 Loss 0.056031175\n",
            "Epoch 80 Batch 12.0 Loss 0.05833413\n",
            "Epoch 80 Batch 13.0 Loss 0.05991323\n",
            "Epoch 80 Batch 14.0 Loss 0.059311986\n",
            "Epoch 80 Batch 15.0 Loss 0.059451837\n",
            "Epoch 80 Batch 16.0 Loss 0.06241325\n",
            "Epoch 80 Batch 17.0 Loss 0.063549735\n",
            "Epoch 80 Batch 18.0 Loss 0.06403898\n",
            "Epoch 80 Batch 19.0 Loss 0.060878\n",
            "Epoch 80 Batch 20.0 Loss 0.06059674\n",
            "Epoch 80 Batch 21.0 Loss 0.064679\n",
            "Epoch 80 Batch 22.0 Loss 0.06208752\n",
            "Epoch 80 Batch 23.0 Loss 0.060723726\n",
            "Epoch 80 Batch 24.0 Loss 0.067048185\n",
            "Epoch 79 Batch 0.0 Loss 0.057408266\n",
            "Epoch 79 Batch 1.0 Loss 0.059890393\n",
            "Epoch 79 Batch 2.0 Loss 0.06289406\n",
            "Epoch 79 Batch 3.0 Loss 0.060284562\n",
            "Epoch 79 Batch 4.0 Loss 0.06569283\n",
            "Epoch 79 Batch 5.0 Loss 0.061106686\n",
            "Epoch 79 Batch 6.0 Loss 0.059269384\n",
            "Epoch 79 Batch 7.0 Loss 0.05900866\n",
            "Epoch 79 Batch 8.0 Loss 0.0618656\n",
            "Epoch 79 Batch 9.0 Loss 0.05894525\n",
            "Epoch 79 Batch 10.0 Loss 0.060325757\n",
            "Epoch 79 Batch 11.0 Loss 0.058100156\n",
            "Epoch 79 Batch 12.0 Loss 0.05930136\n",
            "Epoch 79 Batch 13.0 Loss 0.05893997\n",
            "Epoch 79 Batch 14.0 Loss 0.058792014\n",
            "Epoch 79 Batch 15.0 Loss 0.05975504\n",
            "Epoch 79 Batch 16.0 Loss 0.06216293\n",
            "Epoch 79 Batch 17.0 Loss 0.06389185\n",
            "Epoch 79 Batch 18.0 Loss 0.06410604\n",
            "Epoch 79 Batch 19.0 Loss 0.061551914\n",
            "Epoch 79 Batch 20.0 Loss 0.058528043\n",
            "Epoch 79 Batch 21.0 Loss 0.06346712\n",
            "Epoch 79 Batch 22.0 Loss 0.05915558\n",
            "Epoch 79 Batch 23.0 Loss 0.060442004\n",
            "Epoch 79 Batch 24.0 Loss 0.06266413\n",
            "Epoch 78 Batch 0.0 Loss 0.059929837\n",
            "Epoch 78 Batch 1.0 Loss 0.06375369\n",
            "Epoch 78 Batch 2.0 Loss 0.06059457\n",
            "Epoch 78 Batch 3.0 Loss 0.05663775\n",
            "Epoch 78 Batch 4.0 Loss 0.06229055\n",
            "Epoch 78 Batch 5.0 Loss 0.060576867\n",
            "Epoch 78 Batch 6.0 Loss 0.059065424\n",
            "Epoch 78 Batch 7.0 Loss 0.059634615\n",
            "Epoch 78 Batch 8.0 Loss 0.05973802\n",
            "Epoch 78 Batch 9.0 Loss 0.060838956\n",
            "Epoch 78 Batch 10.0 Loss 0.060157914\n",
            "Epoch 78 Batch 11.0 Loss 0.05809753\n",
            "Epoch 78 Batch 12.0 Loss 0.058615647\n",
            "Epoch 78 Batch 13.0 Loss 0.057110876\n",
            "Epoch 78 Batch 14.0 Loss 0.060730573\n",
            "Epoch 78 Batch 15.0 Loss 0.059525765\n",
            "Epoch 78 Batch 16.0 Loss 0.062778994\n",
            "Epoch 78 Batch 17.0 Loss 0.06436167\n",
            "Epoch 78 Batch 18.0 Loss 0.061890315\n",
            "Epoch 78 Batch 19.0 Loss 0.062583715\n",
            "Epoch 78 Batch 20.0 Loss 0.058465526\n",
            "Epoch 78 Batch 21.0 Loss 0.06249149\n",
            "Epoch 78 Batch 22.0 Loss 0.061413802\n",
            "Epoch 78 Batch 23.0 Loss 0.06161545\n",
            "Epoch 78 Batch 24.0 Loss 0.062671125\n",
            "Epoch 77 Batch 0.0 Loss 0.0613222\n",
            "Epoch 77 Batch 1.0 Loss 0.064084865\n",
            "Epoch 77 Batch 2.0 Loss 0.059653316\n",
            "Epoch 77 Batch 3.0 Loss 0.057877455\n",
            "Epoch 77 Batch 4.0 Loss 0.060580663\n",
            "Epoch 77 Batch 5.0 Loss 0.059882738\n",
            "Epoch 77 Batch 6.0 Loss 0.058906388\n",
            "Epoch 77 Batch 7.0 Loss 0.059771296\n",
            "Epoch 77 Batch 8.0 Loss 0.058279704\n",
            "Epoch 77 Batch 9.0 Loss 0.060345154\n",
            "Epoch 77 Batch 10.0 Loss 0.057253104\n",
            "Epoch 77 Batch 11.0 Loss 0.05906919\n",
            "Epoch 77 Batch 12.0 Loss 0.057874404\n",
            "Epoch 77 Batch 13.0 Loss 0.05819232\n",
            "Epoch 77 Batch 14.0 Loss 0.058812905\n",
            "Epoch 77 Batch 15.0 Loss 0.060181215\n",
            "Epoch 77 Batch 16.0 Loss 0.06493842\n",
            "Epoch 77 Batch 17.0 Loss 0.06279281\n",
            "Epoch 77 Batch 18.0 Loss 0.0630669\n",
            "Epoch 77 Batch 19.0 Loss 0.062185273\n",
            "Epoch 77 Batch 20.0 Loss 0.058999877\n",
            "Epoch 77 Batch 21.0 Loss 0.06205158\n",
            "Epoch 77 Batch 22.0 Loss 0.06103631\n",
            "Epoch 77 Batch 23.0 Loss 0.061237205\n",
            "Epoch 77 Batch 24.0 Loss 0.063848965\n",
            "Epoch 76 Batch 0.0 Loss 0.058006935\n",
            "Epoch 76 Batch 1.0 Loss 0.0617105\n",
            "Epoch 76 Batch 2.0 Loss 0.058926612\n",
            "Epoch 76 Batch 3.0 Loss 0.0567958\n",
            "Epoch 76 Batch 4.0 Loss 0.06263176\n",
            "Epoch 76 Batch 5.0 Loss 0.06134198\n",
            "Epoch 76 Batch 6.0 Loss 0.05745037\n",
            "Epoch 76 Batch 7.0 Loss 0.061115675\n",
            "Epoch 76 Batch 8.0 Loss 0.0606521\n",
            "Epoch 76 Batch 9.0 Loss 0.060964398\n",
            "Epoch 76 Batch 10.0 Loss 0.058504324\n",
            "Epoch 76 Batch 11.0 Loss 0.057850648\n",
            "Epoch 76 Batch 12.0 Loss 0.05738162\n",
            "Epoch 76 Batch 13.0 Loss 0.059580326\n",
            "Epoch 76 Batch 14.0 Loss 0.057603106\n",
            "Epoch 76 Batch 15.0 Loss 0.05941324\n",
            "Epoch 76 Batch 16.0 Loss 0.06373951\n",
            "Epoch 76 Batch 17.0 Loss 0.06356995\n",
            "Epoch 76 Batch 18.0 Loss 0.06256351\n",
            "Epoch 76 Batch 19.0 Loss 0.06201602\n",
            "Epoch 76 Batch 20.0 Loss 0.059344128\n",
            "Epoch 76 Batch 21.0 Loss 0.062211618\n",
            "Epoch 76 Batch 22.0 Loss 0.06025542\n",
            "Epoch 76 Batch 23.0 Loss 0.062434353\n",
            "Epoch 76 Batch 24.0 Loss 0.063179664\n",
            "Epoch 75 Batch 0.0 Loss 0.060745966\n",
            "Epoch 75 Batch 1.0 Loss 0.060674835\n",
            "Epoch 75 Batch 2.0 Loss 0.060921945\n",
            "Epoch 75 Batch 3.0 Loss 0.056252766\n",
            "Epoch 75 Batch 4.0 Loss 0.06150833\n",
            "Epoch 75 Batch 5.0 Loss 0.05982441\n",
            "Epoch 75 Batch 6.0 Loss 0.058778547\n",
            "Epoch 75 Batch 7.0 Loss 0.058988463\n",
            "Epoch 75 Batch 8.0 Loss 0.059615634\n",
            "Epoch 75 Batch 9.0 Loss 0.061113596\n",
            "Epoch 75 Batch 10.0 Loss 0.061780464\n",
            "Epoch 75 Batch 11.0 Loss 0.057090636\n",
            "Epoch 75 Batch 12.0 Loss 0.058519807\n",
            "Epoch 75 Batch 13.0 Loss 0.0585577\n",
            "Epoch 75 Batch 14.0 Loss 0.058151975\n",
            "Epoch 75 Batch 15.0 Loss 0.059502654\n",
            "Epoch 75 Batch 16.0 Loss 0.064548165\n",
            "Epoch 75 Batch 17.0 Loss 0.061980084\n",
            "Epoch 75 Batch 18.0 Loss 0.061962623\n",
            "Epoch 75 Batch 19.0 Loss 0.061877858\n",
            "Epoch 75 Batch 20.0 Loss 0.058268536\n",
            "Epoch 75 Batch 21.0 Loss 0.06296483\n",
            "Epoch 75 Batch 22.0 Loss 0.062015858\n",
            "Epoch 75 Batch 23.0 Loss 0.06105287\n",
            "Epoch 75 Batch 24.0 Loss 0.06235113\n",
            "Epoch 74 Batch 0.0 Loss 0.060132813\n",
            "Epoch 74 Batch 1.0 Loss 0.06056632\n",
            "Epoch 74 Batch 2.0 Loss 0.061507538\n",
            "Epoch 74 Batch 3.0 Loss 0.05671559\n",
            "Epoch 74 Batch 4.0 Loss 0.062079895\n",
            "Epoch 74 Batch 5.0 Loss 0.06077332\n",
            "Epoch 74 Batch 6.0 Loss 0.059035458\n",
            "Epoch 74 Batch 7.0 Loss 0.057340674\n",
            "Epoch 74 Batch 8.0 Loss 0.060694337\n",
            "Epoch 74 Batch 9.0 Loss 0.058322962\n",
            "Epoch 74 Batch 10.0 Loss 0.060804293\n",
            "Epoch 74 Batch 11.0 Loss 0.056780882\n",
            "Epoch 74 Batch 12.0 Loss 0.05751462\n",
            "Epoch 74 Batch 13.0 Loss 0.05893038\n",
            "Epoch 74 Batch 14.0 Loss 0.059759244\n",
            "Epoch 74 Batch 15.0 Loss 0.059691884\n",
            "Epoch 74 Batch 16.0 Loss 0.0640958\n",
            "Epoch 74 Batch 17.0 Loss 0.06333425\n",
            "Epoch 74 Batch 18.0 Loss 0.06297658\n",
            "Epoch 74 Batch 19.0 Loss 0.061858926\n",
            "Epoch 74 Batch 20.0 Loss 0.057850383\n",
            "Epoch 74 Batch 21.0 Loss 0.06288581\n",
            "Epoch 74 Batch 22.0 Loss 0.059540614\n",
            "Epoch 74 Batch 23.0 Loss 0.063098356\n",
            "Epoch 74 Batch 24.0 Loss 0.06236912\n",
            "Epoch 73 Batch 0.0 Loss 0.059033252\n",
            "Epoch 73 Batch 1.0 Loss 0.059664313\n",
            "Epoch 73 Batch 2.0 Loss 0.060155064\n",
            "Epoch 73 Batch 3.0 Loss 0.056950536\n",
            "Epoch 73 Batch 4.0 Loss 0.063486084\n",
            "Epoch 73 Batch 5.0 Loss 0.061241575\n",
            "Epoch 73 Batch 6.0 Loss 0.057158485\n",
            "Epoch 73 Batch 7.0 Loss 0.05701469\n",
            "Epoch 73 Batch 8.0 Loss 0.060941663\n",
            "Epoch 73 Batch 9.0 Loss 0.05916848\n",
            "Epoch 73 Batch 10.0 Loss 0.05913133\n",
            "Epoch 73 Batch 11.0 Loss 0.05868242\n",
            "Epoch 73 Batch 12.0 Loss 0.0576093\n",
            "Epoch 73 Batch 13.0 Loss 0.058225103\n",
            "Epoch 73 Batch 14.0 Loss 0.060132194\n",
            "Epoch 73 Batch 15.0 Loss 0.061142787\n",
            "Epoch 73 Batch 16.0 Loss 0.06271481\n",
            "Epoch 73 Batch 17.0 Loss 0.06142957\n",
            "Epoch 73 Batch 18.0 Loss 0.061395273\n",
            "Epoch 73 Batch 19.0 Loss 0.061661758\n",
            "Epoch 73 Batch 20.0 Loss 0.058180377\n",
            "Epoch 73 Batch 21.0 Loss 0.06207093\n",
            "Epoch 73 Batch 22.0 Loss 0.06110462\n",
            "Epoch 73 Batch 23.0 Loss 0.06220426\n",
            "Epoch 73 Batch 24.0 Loss 0.06127105\n",
            "Epoch 72 Batch 0.0 Loss 0.057218894\n",
            "Epoch 72 Batch 1.0 Loss 0.058722615\n",
            "Epoch 72 Batch 2.0 Loss 0.060668334\n",
            "Epoch 72 Batch 3.0 Loss 0.058197808\n",
            "Epoch 72 Batch 4.0 Loss 0.06305549\n",
            "Epoch 72 Batch 5.0 Loss 0.059330493\n",
            "Epoch 72 Batch 6.0 Loss 0.05755791\n",
            "Epoch 72 Batch 7.0 Loss 0.056358796\n",
            "Epoch 72 Batch 8.0 Loss 0.056241907\n",
            "Epoch 72 Batch 9.0 Loss 0.059329566\n",
            "Epoch 72 Batch 10.0 Loss 0.059075423\n",
            "Epoch 72 Batch 11.0 Loss 0.057430077\n",
            "Epoch 72 Batch 12.0 Loss 0.05961827\n",
            "Epoch 72 Batch 13.0 Loss 0.058172096\n",
            "Epoch 72 Batch 14.0 Loss 0.05920042\n",
            "Epoch 72 Batch 15.0 Loss 0.060030032\n",
            "Epoch 72 Batch 16.0 Loss 0.0634085\n",
            "Epoch 72 Batch 17.0 Loss 0.06289991\n",
            "Epoch 72 Batch 18.0 Loss 0.06346028\n",
            "Epoch 72 Batch 19.0 Loss 0.061442822\n",
            "Epoch 72 Batch 20.0 Loss 0.05916578\n",
            "Epoch 72 Batch 21.0 Loss 0.06207987\n",
            "Epoch 72 Batch 22.0 Loss 0.061783276\n",
            "Epoch 72 Batch 23.0 Loss 0.062281467\n",
            "Epoch 72 Batch 24.0 Loss 0.06422333\n",
            "Epoch 71 Batch 0.0 Loss 0.05823595\n",
            "Epoch 71 Batch 1.0 Loss 0.060021315\n",
            "Epoch 71 Batch 2.0 Loss 0.060937017\n",
            "Epoch 71 Batch 3.0 Loss 0.056547694\n",
            "Epoch 71 Batch 4.0 Loss 0.06100685\n",
            "Epoch 71 Batch 5.0 Loss 0.061231323\n",
            "Epoch 71 Batch 6.0 Loss 0.057873383\n",
            "Epoch 71 Batch 7.0 Loss 0.056173462\n",
            "Epoch 71 Batch 8.0 Loss 0.05887732\n",
            "Epoch 71 Batch 9.0 Loss 0.06050372\n",
            "Epoch 71 Batch 10.0 Loss 0.060163636\n",
            "Epoch 71 Batch 11.0 Loss 0.058132507\n",
            "Epoch 71 Batch 12.0 Loss 0.058163136\n",
            "Epoch 71 Batch 13.0 Loss 0.058585003\n",
            "Epoch 71 Batch 14.0 Loss 0.059024945\n",
            "Epoch 71 Batch 15.0 Loss 0.06027224\n",
            "Epoch 71 Batch 16.0 Loss 0.062464\n",
            "Epoch 71 Batch 17.0 Loss 0.06327109\n",
            "Epoch 71 Batch 18.0 Loss 0.06319959\n",
            "Epoch 71 Batch 19.0 Loss 0.061688486\n",
            "Epoch 71 Batch 20.0 Loss 0.057320524\n",
            "Epoch 71 Batch 21.0 Loss 0.060405463\n",
            "Epoch 71 Batch 22.0 Loss 0.058688655\n",
            "Epoch 71 Batch 23.0 Loss 0.061608028\n",
            "Epoch 71 Batch 24.0 Loss 0.062132392\n",
            "Epoch 70 Batch 0.0 Loss 0.057922807\n",
            "Epoch 70 Batch 1.0 Loss 0.05896597\n",
            "Epoch 70 Batch 2.0 Loss 0.05851092\n",
            "Epoch 70 Batch 3.0 Loss 0.05651789\n",
            "Epoch 70 Batch 4.0 Loss 0.0622577\n",
            "Epoch 70 Batch 5.0 Loss 0.059919935\n",
            "Epoch 70 Batch 6.0 Loss 0.057615127\n",
            "Epoch 70 Batch 7.0 Loss 0.057047825\n",
            "Epoch 70 Batch 8.0 Loss 0.05978686\n",
            "Epoch 70 Batch 9.0 Loss 0.058305196\n",
            "Epoch 70 Batch 10.0 Loss 0.060181733\n",
            "Epoch 70 Batch 11.0 Loss 0.057598878\n",
            "Epoch 70 Batch 12.0 Loss 0.058398314\n",
            "Epoch 70 Batch 13.0 Loss 0.05982186\n",
            "Epoch 70 Batch 14.0 Loss 0.05907553\n",
            "Epoch 70 Batch 15.0 Loss 0.0610635\n",
            "Epoch 70 Batch 16.0 Loss 0.06339891\n",
            "Epoch 70 Batch 17.0 Loss 0.064202815\n",
            "Epoch 70 Batch 18.0 Loss 0.06380964\n",
            "Epoch 70 Batch 19.0 Loss 0.062626466\n",
            "Epoch 70 Batch 20.0 Loss 0.059088066\n",
            "Epoch 70 Batch 21.0 Loss 0.060264837\n",
            "Epoch 70 Batch 22.0 Loss 0.05863932\n",
            "Epoch 70 Batch 23.0 Loss 0.06026552\n",
            "Epoch 70 Batch 24.0 Loss 0.061270557\n",
            "Epoch 69 Batch 0.0 Loss 0.05811667\n",
            "Epoch 69 Batch 1.0 Loss 0.060984045\n",
            "Epoch 69 Batch 2.0 Loss 0.059391737\n",
            "Epoch 69 Batch 3.0 Loss 0.056018833\n",
            "Epoch 69 Batch 4.0 Loss 0.061957136\n",
            "Epoch 69 Batch 5.0 Loss 0.05857785\n",
            "Epoch 69 Batch 6.0 Loss 0.056827575\n",
            "Epoch 69 Batch 7.0 Loss 0.057932343\n",
            "Epoch 69 Batch 8.0 Loss 0.059172343\n",
            "Epoch 69 Batch 9.0 Loss 0.05790647\n",
            "Epoch 69 Batch 10.0 Loss 0.059722204\n",
            "Epoch 69 Batch 11.0 Loss 0.057853002\n",
            "Epoch 69 Batch 12.0 Loss 0.056637887\n",
            "Epoch 69 Batch 13.0 Loss 0.057086278\n",
            "Epoch 69 Batch 14.0 Loss 0.05954428\n",
            "Epoch 69 Batch 15.0 Loss 0.061984576\n",
            "Epoch 69 Batch 16.0 Loss 0.06157964\n",
            "Epoch 69 Batch 17.0 Loss 0.06279708\n",
            "Epoch 69 Batch 18.0 Loss 0.06500596\n",
            "Epoch 69 Batch 19.0 Loss 0.060448095\n",
            "Epoch 69 Batch 20.0 Loss 0.058449246\n",
            "Epoch 69 Batch 21.0 Loss 0.06273589\n",
            "Epoch 69 Batch 22.0 Loss 0.059278775\n",
            "Epoch 69 Batch 23.0 Loss 0.061973754\n",
            "Epoch 69 Batch 24.0 Loss 0.062418386\n",
            "Epoch 68 Batch 0.0 Loss 0.060073398\n",
            "Epoch 68 Batch 1.0 Loss 0.059237663\n",
            "Epoch 68 Batch 2.0 Loss 0.057870664\n",
            "Epoch 68 Batch 3.0 Loss 0.055000644\n",
            "Epoch 68 Batch 4.0 Loss 0.059863582\n",
            "Epoch 68 Batch 5.0 Loss 0.058591112\n",
            "Epoch 68 Batch 6.0 Loss 0.0577946\n",
            "Epoch 68 Batch 7.0 Loss 0.056003764\n",
            "Epoch 68 Batch 8.0 Loss 0.05847403\n",
            "Epoch 68 Batch 9.0 Loss 0.05787247\n",
            "Epoch 68 Batch 10.0 Loss 0.058067784\n",
            "Epoch 68 Batch 11.0 Loss 0.057094403\n",
            "Epoch 68 Batch 12.0 Loss 0.0575105\n",
            "Epoch 68 Batch 13.0 Loss 0.059580483\n",
            "Epoch 68 Batch 14.0 Loss 0.05955279\n",
            "Epoch 68 Batch 15.0 Loss 0.059990607\n",
            "Epoch 68 Batch 16.0 Loss 0.06442531\n",
            "Epoch 68 Batch 17.0 Loss 0.063517466\n",
            "Epoch 68 Batch 18.0 Loss 0.06426408\n",
            "Epoch 68 Batch 19.0 Loss 0.061926655\n",
            "Epoch 68 Batch 20.0 Loss 0.057478722\n",
            "Epoch 68 Batch 21.0 Loss 0.06266424\n",
            "Epoch 68 Batch 22.0 Loss 0.060386386\n",
            "Epoch 68 Batch 23.0 Loss 0.06158031\n",
            "Epoch 68 Batch 24.0 Loss 0.06433113\n",
            "Epoch 67 Batch 0.0 Loss 0.057823874\n",
            "Epoch 67 Batch 1.0 Loss 0.06072765\n",
            "Epoch 67 Batch 2.0 Loss 0.057167355\n",
            "Epoch 67 Batch 3.0 Loss 0.0557477\n",
            "Epoch 67 Batch 4.0 Loss 0.06082873\n",
            "Epoch 67 Batch 5.0 Loss 0.060113117\n",
            "Epoch 67 Batch 6.0 Loss 0.055944346\n",
            "Epoch 67 Batch 7.0 Loss 0.05685505\n",
            "Epoch 67 Batch 8.0 Loss 0.05821604\n",
            "Epoch 67 Batch 9.0 Loss 0.05923316\n",
            "Epoch 67 Batch 10.0 Loss 0.057142075\n",
            "Epoch 67 Batch 11.0 Loss 0.056021515\n",
            "Epoch 67 Batch 12.0 Loss 0.055489134\n",
            "Epoch 67 Batch 13.0 Loss 0.058396567\n",
            "Epoch 67 Batch 14.0 Loss 0.059498176\n",
            "Epoch 67 Batch 15.0 Loss 0.06139661\n",
            "Epoch 67 Batch 16.0 Loss 0.060192104\n",
            "Epoch 67 Batch 17.0 Loss 0.061469827\n",
            "Epoch 67 Batch 18.0 Loss 0.06382558\n",
            "Epoch 67 Batch 19.0 Loss 0.06163325\n",
            "Epoch 67 Batch 20.0 Loss 0.05864888\n",
            "Epoch 67 Batch 21.0 Loss 0.06236778\n",
            "Epoch 67 Batch 22.0 Loss 0.058830068\n",
            "Epoch 67 Batch 23.0 Loss 0.06284583\n",
            "Epoch 67 Batch 24.0 Loss 0.0638544\n",
            "Epoch 66 Batch 0.0 Loss 0.056797255\n",
            "Epoch 66 Batch 1.0 Loss 0.061087076\n",
            "Epoch 66 Batch 2.0 Loss 0.05924993\n",
            "Epoch 66 Batch 3.0 Loss 0.05650279\n",
            "Epoch 66 Batch 4.0 Loss 0.06308897\n",
            "Epoch 66 Batch 5.0 Loss 0.05857682\n",
            "Epoch 66 Batch 6.0 Loss 0.056713395\n",
            "Epoch 66 Batch 7.0 Loss 0.057819303\n",
            "Epoch 66 Batch 8.0 Loss 0.059014786\n",
            "Epoch 66 Batch 9.0 Loss 0.05907239\n",
            "Epoch 66 Batch 10.0 Loss 0.057948638\n",
            "Epoch 66 Batch 11.0 Loss 0.0564676\n",
            "Epoch 66 Batch 12.0 Loss 0.05621927\n",
            "Epoch 66 Batch 13.0 Loss 0.05865068\n",
            "Epoch 66 Batch 14.0 Loss 0.058154255\n",
            "Epoch 66 Batch 15.0 Loss 0.05908557\n",
            "Epoch 66 Batch 16.0 Loss 0.06504288\n",
            "Epoch 66 Batch 17.0 Loss 0.060152475\n",
            "Epoch 66 Batch 18.0 Loss 0.06215846\n",
            "Epoch 66 Batch 19.0 Loss 0.061973635\n",
            "Epoch 66 Batch 20.0 Loss 0.05635127\n",
            "Epoch 66 Batch 21.0 Loss 0.061615363\n",
            "Epoch 66 Batch 22.0 Loss 0.059774157\n",
            "Epoch 66 Batch 23.0 Loss 0.061202794\n",
            "Epoch 66 Batch 24.0 Loss 0.06191274\n",
            "Epoch 65 Batch 0.0 Loss 0.057353213\n",
            "Epoch 65 Batch 1.0 Loss 0.061336484\n",
            "Epoch 65 Batch 2.0 Loss 0.057760313\n",
            "Epoch 65 Batch 3.0 Loss 0.05834203\n",
            "Epoch 65 Batch 4.0 Loss 0.062381815\n",
            "Epoch 65 Batch 5.0 Loss 0.05915764\n",
            "Epoch 65 Batch 6.0 Loss 0.05808283\n",
            "Epoch 65 Batch 7.0 Loss 0.057826724\n",
            "Epoch 65 Batch 8.0 Loss 0.057263497\n",
            "Epoch 65 Batch 9.0 Loss 0.056608297\n",
            "Epoch 65 Batch 10.0 Loss 0.056266837\n",
            "Epoch 65 Batch 11.0 Loss 0.057510212\n",
            "Epoch 65 Batch 12.0 Loss 0.056615695\n",
            "Epoch 65 Batch 13.0 Loss 0.05957059\n",
            "Epoch 65 Batch 14.0 Loss 0.058519006\n",
            "Epoch 65 Batch 15.0 Loss 0.05962758\n",
            "Epoch 65 Batch 16.0 Loss 0.061622243\n",
            "Epoch 65 Batch 17.0 Loss 0.061816346\n",
            "Epoch 65 Batch 18.0 Loss 0.061723255\n",
            "Epoch 65 Batch 19.0 Loss 0.061085083\n",
            "Epoch 65 Batch 20.0 Loss 0.056475773\n",
            "Epoch 65 Batch 21.0 Loss 0.063100435\n",
            "Epoch 65 Batch 22.0 Loss 0.060089942\n",
            "Epoch 65 Batch 23.0 Loss 0.059287008\n",
            "Epoch 65 Batch 24.0 Loss 0.06269187\n",
            "Epoch 64 Batch 0.0 Loss 0.057405047\n",
            "Epoch 64 Batch 1.0 Loss 0.059101526\n",
            "Epoch 64 Batch 2.0 Loss 0.059050128\n",
            "Epoch 64 Batch 3.0 Loss 0.056316476\n",
            "Epoch 64 Batch 4.0 Loss 0.062230963\n",
            "Epoch 64 Batch 5.0 Loss 0.058054112\n",
            "Epoch 64 Batch 6.0 Loss 0.056260347\n",
            "Epoch 64 Batch 7.0 Loss 0.05791197\n",
            "Epoch 64 Batch 8.0 Loss 0.06041051\n",
            "Epoch 64 Batch 9.0 Loss 0.060030326\n",
            "Epoch 64 Batch 10.0 Loss 0.05917099\n",
            "Epoch 64 Batch 11.0 Loss 0.057362285\n",
            "Epoch 64 Batch 12.0 Loss 0.055211686\n",
            "Epoch 64 Batch 13.0 Loss 0.056278262\n",
            "Epoch 64 Batch 14.0 Loss 0.058976613\n",
            "Epoch 64 Batch 15.0 Loss 0.059567284\n",
            "Epoch 64 Batch 16.0 Loss 0.0631975\n",
            "Epoch 64 Batch 17.0 Loss 0.0637115\n",
            "Epoch 64 Batch 18.0 Loss 0.0620945\n",
            "Epoch 64 Batch 19.0 Loss 0.06008008\n",
            "Epoch 64 Batch 20.0 Loss 0.057383712\n",
            "Epoch 64 Batch 21.0 Loss 0.06070185\n",
            "Epoch 64 Batch 22.0 Loss 0.060026288\n",
            "Epoch 64 Batch 23.0 Loss 0.061504215\n",
            "Epoch 64 Batch 24.0 Loss 0.06226437\n",
            "Epoch 63 Batch 0.0 Loss 0.05556177\n",
            "Epoch 63 Batch 1.0 Loss 0.05906887\n",
            "Epoch 63 Batch 2.0 Loss 0.058184534\n",
            "Epoch 63 Batch 3.0 Loss 0.055614866\n",
            "Epoch 63 Batch 4.0 Loss 0.06205179\n",
            "Epoch 63 Batch 5.0 Loss 0.060265563\n",
            "Epoch 63 Batch 6.0 Loss 0.05602272\n",
            "Epoch 63 Batch 7.0 Loss 0.057202157\n",
            "Epoch 63 Batch 8.0 Loss 0.060144175\n",
            "Epoch 63 Batch 9.0 Loss 0.057372216\n",
            "Epoch 63 Batch 10.0 Loss 0.058019675\n",
            "Epoch 63 Batch 11.0 Loss 0.056369215\n",
            "Epoch 63 Batch 12.0 Loss 0.055764418\n",
            "Epoch 63 Batch 13.0 Loss 0.058557156\n",
            "Epoch 63 Batch 14.0 Loss 0.057636496\n",
            "Epoch 63 Batch 15.0 Loss 0.059742786\n",
            "Epoch 63 Batch 16.0 Loss 0.062391397\n",
            "Epoch 63 Batch 17.0 Loss 0.06237532\n",
            "Epoch 63 Batch 18.0 Loss 0.06228961\n",
            "Epoch 63 Batch 19.0 Loss 0.060849376\n",
            "Epoch 63 Batch 20.0 Loss 0.057366084\n",
            "Epoch 63 Batch 21.0 Loss 0.061570473\n",
            "Epoch 63 Batch 22.0 Loss 0.05852435\n",
            "Epoch 63 Batch 23.0 Loss 0.060420815\n",
            "Epoch 63 Batch 24.0 Loss 0.06136378\n",
            "Epoch 62 Batch 0.0 Loss 0.056639332\n",
            "Epoch 62 Batch 1.0 Loss 0.05921525\n",
            "Epoch 62 Batch 2.0 Loss 0.058060776\n",
            "Epoch 62 Batch 3.0 Loss 0.056523427\n",
            "Epoch 62 Batch 4.0 Loss 0.06292388\n",
            "Epoch 62 Batch 5.0 Loss 0.05955025\n",
            "Epoch 62 Batch 6.0 Loss 0.055560764\n",
            "Epoch 62 Batch 7.0 Loss 0.056909375\n",
            "Epoch 62 Batch 8.0 Loss 0.058938123\n",
            "Epoch 62 Batch 9.0 Loss 0.059610933\n",
            "Epoch 62 Batch 10.0 Loss 0.05900459\n",
            "Epoch 62 Batch 11.0 Loss 0.054248005\n",
            "Epoch 62 Batch 12.0 Loss 0.05562837\n",
            "Epoch 62 Batch 13.0 Loss 0.057574067\n",
            "Epoch 62 Batch 14.0 Loss 0.05826745\n",
            "Epoch 62 Batch 15.0 Loss 0.059431333\n",
            "Epoch 62 Batch 16.0 Loss 0.0637605\n",
            "Epoch 62 Batch 17.0 Loss 0.063629344\n",
            "Epoch 62 Batch 18.0 Loss 0.06329572\n",
            "Epoch 62 Batch 19.0 Loss 0.059410453\n",
            "Epoch 62 Batch 20.0 Loss 0.057059318\n",
            "Epoch 62 Batch 21.0 Loss 0.061342545\n",
            "Epoch 62 Batch 22.0 Loss 0.058871064\n",
            "Epoch 62 Batch 23.0 Loss 0.062659726\n",
            "Epoch 62 Batch 24.0 Loss 0.06194159\n",
            "Epoch 61 Batch 0.0 Loss 0.056016885\n",
            "Epoch 61 Batch 1.0 Loss 0.059432417\n",
            "Epoch 61 Batch 2.0 Loss 0.05678734\n",
            "Epoch 61 Batch 3.0 Loss 0.057626754\n",
            "Epoch 61 Batch 4.0 Loss 0.06110014\n",
            "Epoch 61 Batch 5.0 Loss 0.059257537\n",
            "Epoch 61 Batch 6.0 Loss 0.05605921\n",
            "Epoch 61 Batch 7.0 Loss 0.05560788\n",
            "Epoch 61 Batch 8.0 Loss 0.05804461\n",
            "Epoch 61 Batch 9.0 Loss 0.0576542\n",
            "Epoch 61 Batch 10.0 Loss 0.059768\n",
            "Epoch 61 Batch 11.0 Loss 0.05647031\n",
            "Epoch 61 Batch 12.0 Loss 0.05704527\n",
            "Epoch 61 Batch 13.0 Loss 0.05769042\n",
            "Epoch 61 Batch 14.0 Loss 0.05765477\n",
            "Epoch 61 Batch 15.0 Loss 0.060285926\n",
            "Epoch 61 Batch 16.0 Loss 0.06264861\n",
            "Epoch 61 Batch 17.0 Loss 0.06264206\n",
            "Epoch 61 Batch 18.0 Loss 0.062489737\n",
            "Epoch 61 Batch 19.0 Loss 0.05877294\n",
            "Epoch 61 Batch 20.0 Loss 0.057456568\n",
            "Epoch 61 Batch 21.0 Loss 0.06118623\n",
            "Epoch 61 Batch 22.0 Loss 0.058435604\n",
            "Epoch 61 Batch 23.0 Loss 0.061874837\n",
            "Epoch 61 Batch 24.0 Loss 0.06379999\n",
            "Epoch 60 Batch 0.0 Loss 0.058589924\n",
            "Epoch 60 Batch 1.0 Loss 0.059986282\n",
            "Epoch 60 Batch 2.0 Loss 0.05902786\n",
            "Epoch 60 Batch 3.0 Loss 0.05583103\n",
            "Epoch 60 Batch 4.0 Loss 0.061643377\n",
            "Epoch 60 Batch 5.0 Loss 0.058976468\n",
            "Epoch 60 Batch 6.0 Loss 0.054717388\n",
            "Epoch 60 Batch 7.0 Loss 0.05538859\n",
            "Epoch 60 Batch 8.0 Loss 0.05832934\n",
            "Epoch 60 Batch 9.0 Loss 0.056076206\n",
            "Epoch 60 Batch 10.0 Loss 0.059343487\n",
            "Epoch 60 Batch 11.0 Loss 0.05699582\n",
            "Epoch 60 Batch 12.0 Loss 0.057907663\n",
            "Epoch 60 Batch 13.0 Loss 0.0568952\n",
            "Epoch 60 Batch 14.0 Loss 0.058011435\n",
            "Epoch 60 Batch 15.0 Loss 0.06034368\n",
            "Epoch 60 Batch 16.0 Loss 0.06220281\n",
            "Epoch 60 Batch 17.0 Loss 0.062011834\n",
            "Epoch 60 Batch 18.0 Loss 0.0625385\n",
            "Epoch 60 Batch 19.0 Loss 0.058751665\n",
            "Epoch 60 Batch 20.0 Loss 0.056250516\n",
            "Epoch 60 Batch 21.0 Loss 0.06160867\n",
            "Epoch 60 Batch 22.0 Loss 0.05899174\n",
            "Epoch 60 Batch 23.0 Loss 0.06048152\n",
            "Epoch 60 Batch 24.0 Loss 0.06163888\n",
            "Epoch 59 Batch 0.0 Loss 0.057807036\n",
            "Epoch 59 Batch 1.0 Loss 0.058802277\n",
            "Epoch 59 Batch 2.0 Loss 0.05847776\n",
            "Epoch 59 Batch 3.0 Loss 0.056660876\n",
            "Epoch 59 Batch 4.0 Loss 0.062351517\n",
            "Epoch 59 Batch 5.0 Loss 0.05905186\n",
            "Epoch 59 Batch 6.0 Loss 0.056839366\n",
            "Epoch 59 Batch 7.0 Loss 0.05507128\n",
            "Epoch 59 Batch 8.0 Loss 0.058433782\n",
            "Epoch 59 Batch 9.0 Loss 0.05759593\n",
            "Epoch 59 Batch 10.0 Loss 0.057806134\n",
            "Epoch 59 Batch 11.0 Loss 0.05686219\n",
            "Epoch 59 Batch 12.0 Loss 0.056903783\n",
            "Epoch 59 Batch 13.0 Loss 0.057921413\n",
            "Epoch 59 Batch 14.0 Loss 0.05793564\n",
            "Epoch 59 Batch 15.0 Loss 0.06067354\n",
            "Epoch 59 Batch 16.0 Loss 0.062150255\n",
            "Epoch 59 Batch 17.0 Loss 0.060862496\n",
            "Epoch 59 Batch 18.0 Loss 0.062213816\n",
            "Epoch 59 Batch 19.0 Loss 0.0600794\n",
            "Epoch 59 Batch 20.0 Loss 0.05658943\n",
            "Epoch 59 Batch 21.0 Loss 0.060769\n",
            "Epoch 59 Batch 22.0 Loss 0.05899406\n",
            "Epoch 59 Batch 23.0 Loss 0.06235673\n",
            "Epoch 59 Batch 24.0 Loss 0.0617922\n",
            "Epoch 58 Batch 0.0 Loss 0.056535743\n",
            "Epoch 58 Batch 1.0 Loss 0.05750419\n",
            "Epoch 58 Batch 2.0 Loss 0.05815579\n",
            "Epoch 58 Batch 3.0 Loss 0.055626467\n",
            "Epoch 58 Batch 4.0 Loss 0.05970369\n",
            "Epoch 58 Batch 5.0 Loss 0.05891338\n",
            "Epoch 58 Batch 6.0 Loss 0.056549888\n",
            "Epoch 58 Batch 7.0 Loss 0.054832056\n",
            "Epoch 58 Batch 8.0 Loss 0.057462074\n",
            "Epoch 58 Batch 9.0 Loss 0.058370475\n",
            "Epoch 58 Batch 10.0 Loss 0.058227062\n",
            "Epoch 58 Batch 11.0 Loss 0.056468133\n",
            "Epoch 58 Batch 12.0 Loss 0.057516903\n",
            "Epoch 58 Batch 13.0 Loss 0.057408396\n",
            "Epoch 58 Batch 14.0 Loss 0.057655245\n",
            "Epoch 58 Batch 15.0 Loss 0.058572378\n",
            "Epoch 58 Batch 16.0 Loss 0.062281538\n",
            "Epoch 58 Batch 17.0 Loss 0.060874637\n",
            "Epoch 58 Batch 18.0 Loss 0.06101359\n",
            "Epoch 58 Batch 19.0 Loss 0.0599238\n",
            "Epoch 58 Batch 20.0 Loss 0.057040952\n",
            "Epoch 58 Batch 21.0 Loss 0.060726777\n",
            "Epoch 58 Batch 22.0 Loss 0.05809716\n",
            "Epoch 58 Batch 23.0 Loss 0.060485944\n",
            "Epoch 58 Batch 24.0 Loss 0.0653798\n",
            "Epoch 57 Batch 0.0 Loss 0.056867674\n",
            "Epoch 57 Batch 1.0 Loss 0.056771163\n",
            "Epoch 57 Batch 2.0 Loss 0.058092915\n",
            "Epoch 57 Batch 3.0 Loss 0.05526488\n",
            "Epoch 57 Batch 4.0 Loss 0.06136986\n",
            "Epoch 57 Batch 5.0 Loss 0.05770081\n",
            "Epoch 57 Batch 6.0 Loss 0.055456925\n",
            "Epoch 57 Batch 7.0 Loss 0.057830617\n",
            "Epoch 57 Batch 8.0 Loss 0.059091724\n",
            "Epoch 57 Batch 9.0 Loss 0.059030615\n",
            "Epoch 57 Batch 10.0 Loss 0.05835385\n",
            "Epoch 57 Batch 11.0 Loss 0.055046096\n",
            "Epoch 57 Batch 12.0 Loss 0.05593515\n",
            "Epoch 57 Batch 13.0 Loss 0.05738011\n",
            "Epoch 57 Batch 14.0 Loss 0.057223275\n",
            "Epoch 57 Batch 15.0 Loss 0.060593113\n",
            "Epoch 57 Batch 16.0 Loss 0.061556716\n",
            "Epoch 57 Batch 17.0 Loss 0.06258271\n",
            "Epoch 57 Batch 18.0 Loss 0.061329592\n",
            "Epoch 57 Batch 19.0 Loss 0.058995694\n",
            "Epoch 57 Batch 20.0 Loss 0.056298796\n",
            "Epoch 57 Batch 21.0 Loss 0.059822287\n",
            "Epoch 57 Batch 22.0 Loss 0.058961377\n",
            "Epoch 57 Batch 23.0 Loss 0.05883479\n",
            "Epoch 57 Batch 24.0 Loss 0.059096336\n",
            "Epoch 56 Batch 0.0 Loss 0.060091224\n",
            "Epoch 56 Batch 1.0 Loss 0.059490994\n",
            "Epoch 56 Batch 2.0 Loss 0.057777997\n",
            "Epoch 56 Batch 3.0 Loss 0.05564416\n",
            "Epoch 56 Batch 4.0 Loss 0.06178895\n",
            "Epoch 56 Batch 5.0 Loss 0.057956062\n",
            "Epoch 56 Batch 6.0 Loss 0.054858595\n",
            "Epoch 56 Batch 7.0 Loss 0.05747765\n",
            "Epoch 56 Batch 8.0 Loss 0.058595467\n",
            "Epoch 56 Batch 9.0 Loss 0.056904305\n",
            "Epoch 56 Batch 10.0 Loss 0.058217492\n",
            "Epoch 56 Batch 11.0 Loss 0.054709088\n",
            "Epoch 56 Batch 12.0 Loss 0.056520957\n",
            "Epoch 56 Batch 13.0 Loss 0.055939265\n",
            "Epoch 56 Batch 14.0 Loss 0.056968402\n",
            "Epoch 56 Batch 15.0 Loss 0.05948861\n",
            "Epoch 56 Batch 16.0 Loss 0.06274105\n",
            "Epoch 56 Batch 17.0 Loss 0.063016035\n",
            "Epoch 56 Batch 18.0 Loss 0.05990113\n",
            "Epoch 56 Batch 19.0 Loss 0.05986775\n",
            "Epoch 56 Batch 20.0 Loss 0.058459364\n",
            "Epoch 56 Batch 21.0 Loss 0.059159413\n",
            "Epoch 56 Batch 22.0 Loss 0.058964495\n",
            "Epoch 56 Batch 23.0 Loss 0.059061397\n",
            "Epoch 56 Batch 24.0 Loss 0.060895305\n",
            "Epoch 55 Batch 0.0 Loss 0.05823311\n",
            "Epoch 55 Batch 1.0 Loss 0.058822773\n",
            "Epoch 55 Batch 2.0 Loss 0.05576578\n",
            "Epoch 55 Batch 3.0 Loss 0.0546788\n",
            "Epoch 55 Batch 4.0 Loss 0.06205379\n",
            "Epoch 55 Batch 5.0 Loss 0.058339257\n",
            "Epoch 55 Batch 6.0 Loss 0.055872157\n",
            "Epoch 55 Batch 7.0 Loss 0.05499947\n",
            "Epoch 55 Batch 8.0 Loss 0.058983613\n",
            "Epoch 55 Batch 9.0 Loss 0.05728334\n",
            "Epoch 55 Batch 10.0 Loss 0.057471953\n",
            "Epoch 55 Batch 11.0 Loss 0.05497977\n",
            "Epoch 55 Batch 12.0 Loss 0.05331888\n",
            "Epoch 55 Batch 13.0 Loss 0.057580058\n",
            "Epoch 55 Batch 14.0 Loss 0.057461802\n",
            "Epoch 55 Batch 15.0 Loss 0.059855387\n",
            "Epoch 55 Batch 16.0 Loss 0.061357368\n",
            "Epoch 55 Batch 17.0 Loss 0.061136156\n",
            "Epoch 55 Batch 18.0 Loss 0.06170493\n",
            "Epoch 55 Batch 19.0 Loss 0.059011016\n",
            "Epoch 55 Batch 20.0 Loss 0.056831695\n",
            "Epoch 55 Batch 21.0 Loss 0.062306643\n",
            "Epoch 55 Batch 22.0 Loss 0.05951807\n",
            "Epoch 55 Batch 23.0 Loss 0.059354816\n",
            "Epoch 55 Batch 24.0 Loss 0.0618437\n",
            "Epoch 54 Batch 0.0 Loss 0.05666386\n",
            "Epoch 54 Batch 1.0 Loss 0.058185592\n",
            "Epoch 54 Batch 2.0 Loss 0.057286683\n",
            "Epoch 54 Batch 3.0 Loss 0.055478964\n",
            "Epoch 54 Batch 4.0 Loss 0.060094498\n",
            "Epoch 54 Batch 5.0 Loss 0.058071\n",
            "Epoch 54 Batch 6.0 Loss 0.055195652\n",
            "Epoch 54 Batch 7.0 Loss 0.056773234\n",
            "Epoch 54 Batch 8.0 Loss 0.059041843\n",
            "Epoch 54 Batch 9.0 Loss 0.056268204\n",
            "Epoch 54 Batch 10.0 Loss 0.05822658\n",
            "Epoch 54 Batch 11.0 Loss 0.05537393\n",
            "Epoch 54 Batch 12.0 Loss 0.055291705\n",
            "Epoch 54 Batch 13.0 Loss 0.056229077\n",
            "Epoch 54 Batch 14.0 Loss 0.05805829\n",
            "Epoch 54 Batch 15.0 Loss 0.058722883\n",
            "Epoch 54 Batch 16.0 Loss 0.060007226\n",
            "Epoch 54 Batch 17.0 Loss 0.06119655\n",
            "Epoch 54 Batch 18.0 Loss 0.06308618\n",
            "Epoch 54 Batch 19.0 Loss 0.059260808\n",
            "Epoch 54 Batch 20.0 Loss 0.05716581\n",
            "Epoch 54 Batch 21.0 Loss 0.060643397\n",
            "Epoch 54 Batch 22.0 Loss 0.059361503\n",
            "Epoch 54 Batch 23.0 Loss 0.059834246\n",
            "Epoch 54 Batch 24.0 Loss 0.061956186\n",
            "Epoch 53 Batch 0.0 Loss 0.058597002\n",
            "Epoch 53 Batch 1.0 Loss 0.059082367\n",
            "Epoch 53 Batch 2.0 Loss 0.05914058\n",
            "Epoch 53 Batch 3.0 Loss 0.055243682\n",
            "Epoch 53 Batch 4.0 Loss 0.061837286\n",
            "Epoch 53 Batch 5.0 Loss 0.058827423\n",
            "Epoch 53 Batch 6.0 Loss 0.056261033\n",
            "Epoch 53 Batch 7.0 Loss 0.055980634\n",
            "Epoch 53 Batch 8.0 Loss 0.058333784\n",
            "Epoch 53 Batch 9.0 Loss 0.057371095\n",
            "Epoch 53 Batch 10.0 Loss 0.057025395\n",
            "Epoch 53 Batch 11.0 Loss 0.055793747\n",
            "Epoch 53 Batch 12.0 Loss 0.05552137\n",
            "Epoch 53 Batch 13.0 Loss 0.056126207\n",
            "Epoch 53 Batch 14.0 Loss 0.059082076\n",
            "Epoch 53 Batch 15.0 Loss 0.06070123\n",
            "Epoch 53 Batch 16.0 Loss 0.061234377\n",
            "Epoch 53 Batch 17.0 Loss 0.060713753\n",
            "Epoch 53 Batch 18.0 Loss 0.061010066\n",
            "Epoch 53 Batch 19.0 Loss 0.059785213\n",
            "Epoch 53 Batch 20.0 Loss 0.054138493\n",
            "Epoch 53 Batch 21.0 Loss 0.0594668\n",
            "Epoch 53 Batch 22.0 Loss 0.056890782\n",
            "Epoch 53 Batch 23.0 Loss 0.058550015\n",
            "Epoch 53 Batch 24.0 Loss 0.06252318\n",
            "Epoch 52 Batch 0.0 Loss 0.057867907\n",
            "Epoch 52 Batch 1.0 Loss 0.060236387\n",
            "Epoch 52 Batch 2.0 Loss 0.05816928\n",
            "Epoch 52 Batch 3.0 Loss 0.053966843\n",
            "Epoch 52 Batch 4.0 Loss 0.060024466\n",
            "Epoch 52 Batch 5.0 Loss 0.05734387\n",
            "Epoch 52 Batch 6.0 Loss 0.056083083\n",
            "Epoch 52 Batch 7.0 Loss 0.057026364\n",
            "Epoch 52 Batch 8.0 Loss 0.05531033\n",
            "Epoch 52 Batch 9.0 Loss 0.056888223\n",
            "Epoch 52 Batch 10.0 Loss 0.05874081\n",
            "Epoch 52 Batch 11.0 Loss 0.055927068\n",
            "Epoch 52 Batch 12.0 Loss 0.05813633\n",
            "Epoch 52 Batch 13.0 Loss 0.056538664\n",
            "Epoch 52 Batch 14.0 Loss 0.056418266\n",
            "Epoch 52 Batch 15.0 Loss 0.058649816\n",
            "Epoch 52 Batch 16.0 Loss 0.06261921\n",
            "Epoch 52 Batch 17.0 Loss 0.060350284\n",
            "Epoch 52 Batch 18.0 Loss 0.05939032\n",
            "Epoch 52 Batch 19.0 Loss 0.05931784\n",
            "Epoch 52 Batch 20.0 Loss 0.05580643\n",
            "Epoch 52 Batch 21.0 Loss 0.059909016\n",
            "Epoch 52 Batch 22.0 Loss 0.0575098\n",
            "Epoch 52 Batch 23.0 Loss 0.06002263\n",
            "Epoch 52 Batch 24.0 Loss 0.059867978\n",
            "Epoch 51 Batch 0.0 Loss 0.05651537\n",
            "Epoch 51 Batch 1.0 Loss 0.0566826\n",
            "Epoch 51 Batch 2.0 Loss 0.058032516\n",
            "Epoch 51 Batch 3.0 Loss 0.056178167\n",
            "Epoch 51 Batch 4.0 Loss 0.060628716\n",
            "Epoch 51 Batch 5.0 Loss 0.058361545\n",
            "Epoch 51 Batch 6.0 Loss 0.054625273\n",
            "Epoch 51 Batch 7.0 Loss 0.05558031\n",
            "Epoch 51 Batch 8.0 Loss 0.05563543\n",
            "Epoch 51 Batch 9.0 Loss 0.057717208\n",
            "Epoch 51 Batch 10.0 Loss 0.056373343\n",
            "Epoch 51 Batch 11.0 Loss 0.055947505\n",
            "Epoch 51 Batch 12.0 Loss 0.055209685\n",
            "Epoch 51 Batch 13.0 Loss 0.05737786\n",
            "Epoch 51 Batch 14.0 Loss 0.0585844\n",
            "Epoch 51 Batch 15.0 Loss 0.05866187\n",
            "Epoch 51 Batch 16.0 Loss 0.06183001\n",
            "Epoch 51 Batch 17.0 Loss 0.062270947\n",
            "Epoch 51 Batch 18.0 Loss 0.06143824\n",
            "Epoch 51 Batch 19.0 Loss 0.060081463\n",
            "Epoch 51 Batch 20.0 Loss 0.05641808\n",
            "Epoch 51 Batch 21.0 Loss 0.060146738\n",
            "Epoch 51 Batch 22.0 Loss 0.05868657\n",
            "Epoch 51 Batch 23.0 Loss 0.06085599\n",
            "Epoch 51 Batch 24.0 Loss 0.060739323\n",
            "Epoch 50 Batch 0.0 Loss 0.055123843\n",
            "Epoch 50 Batch 1.0 Loss 0.057930123\n",
            "Epoch 50 Batch 2.0 Loss 0.055428468\n",
            "Epoch 50 Batch 3.0 Loss 0.055032797\n",
            "Epoch 50 Batch 4.0 Loss 0.059273697\n",
            "Epoch 50 Batch 5.0 Loss 0.059075784\n",
            "Epoch 50 Batch 6.0 Loss 0.056492854\n",
            "Epoch 50 Batch 7.0 Loss 0.055501767\n",
            "Epoch 50 Batch 8.0 Loss 0.057159267\n",
            "Epoch 50 Batch 9.0 Loss 0.05793507\n",
            "Epoch 50 Batch 10.0 Loss 0.05635161\n",
            "Epoch 50 Batch 11.0 Loss 0.05553283\n",
            "Epoch 50 Batch 12.0 Loss 0.054973707\n",
            "Epoch 50 Batch 13.0 Loss 0.056307152\n",
            "Epoch 50 Batch 14.0 Loss 0.057170037\n",
            "Epoch 50 Batch 15.0 Loss 0.05948705\n",
            "Epoch 50 Batch 16.0 Loss 0.060406476\n",
            "Epoch 50 Batch 17.0 Loss 0.060017556\n",
            "Epoch 50 Batch 18.0 Loss 0.061111163\n",
            "Epoch 50 Batch 19.0 Loss 0.0594268\n",
            "Epoch 50 Batch 20.0 Loss 0.05624758\n",
            "Epoch 50 Batch 21.0 Loss 0.059096877\n",
            "Epoch 50 Batch 22.0 Loss 0.058727026\n",
            "Epoch 50 Batch 23.0 Loss 0.05923307\n",
            "Epoch 50 Batch 24.0 Loss 0.06324782\n",
            "Epoch 49 Batch 0.0 Loss 0.05448202\n",
            "Epoch 49 Batch 1.0 Loss 0.0585289\n",
            "Epoch 49 Batch 2.0 Loss 0.056041904\n",
            "Epoch 49 Batch 3.0 Loss 0.054706458\n",
            "Epoch 49 Batch 4.0 Loss 0.05983043\n",
            "Epoch 49 Batch 5.0 Loss 0.05839403\n",
            "Epoch 49 Batch 6.0 Loss 0.055858243\n",
            "Epoch 49 Batch 7.0 Loss 0.05558467\n",
            "Epoch 49 Batch 8.0 Loss 0.05781955\n",
            "Epoch 49 Batch 9.0 Loss 0.057514537\n",
            "Epoch 49 Batch 10.0 Loss 0.057413682\n",
            "Epoch 49 Batch 11.0 Loss 0.05664392\n",
            "Epoch 49 Batch 12.0 Loss 0.054994687\n",
            "Epoch 49 Batch 13.0 Loss 0.056888465\n",
            "Epoch 49 Batch 14.0 Loss 0.05929225\n",
            "Epoch 49 Batch 15.0 Loss 0.058498215\n",
            "Epoch 49 Batch 16.0 Loss 0.060375918\n",
            "Epoch 49 Batch 17.0 Loss 0.0622758\n",
            "Epoch 49 Batch 18.0 Loss 0.060137715\n",
            "Epoch 49 Batch 19.0 Loss 0.061180316\n",
            "Epoch 49 Batch 20.0 Loss 0.056233905\n",
            "Epoch 49 Batch 21.0 Loss 0.06028348\n",
            "Epoch 49 Batch 22.0 Loss 0.05823723\n",
            "Epoch 49 Batch 23.0 Loss 0.0603151\n",
            "Epoch 49 Batch 24.0 Loss 0.061213516\n",
            "Epoch 48 Batch 0.0 Loss 0.056116704\n",
            "Epoch 48 Batch 1.0 Loss 0.059542924\n",
            "Epoch 48 Batch 2.0 Loss 0.058053747\n",
            "Epoch 48 Batch 3.0 Loss 0.055551976\n",
            "Epoch 48 Batch 4.0 Loss 0.061902713\n",
            "Epoch 48 Batch 5.0 Loss 0.057726275\n",
            "Epoch 48 Batch 6.0 Loss 0.054580178\n",
            "Epoch 48 Batch 7.0 Loss 0.05483937\n",
            "Epoch 48 Batch 8.0 Loss 0.05525992\n",
            "Epoch 48 Batch 9.0 Loss 0.056917433\n",
            "Epoch 48 Batch 10.0 Loss 0.05677732\n",
            "Epoch 48 Batch 11.0 Loss 0.056179337\n",
            "Epoch 48 Batch 12.0 Loss 0.055160508\n",
            "Epoch 48 Batch 13.0 Loss 0.055854317\n",
            "Epoch 48 Batch 14.0 Loss 0.058004912\n",
            "Epoch 48 Batch 15.0 Loss 0.058669716\n",
            "Epoch 48 Batch 16.0 Loss 0.059772752\n",
            "Epoch 48 Batch 17.0 Loss 0.060273387\n",
            "Epoch 48 Batch 18.0 Loss 0.061824184\n",
            "Epoch 48 Batch 19.0 Loss 0.059114046\n",
            "Epoch 48 Batch 20.0 Loss 0.055282615\n",
            "Epoch 48 Batch 21.0 Loss 0.06102228\n",
            "Epoch 48 Batch 22.0 Loss 0.0590596\n",
            "Epoch 48 Batch 23.0 Loss 0.058163267\n",
            "Epoch 48 Batch 24.0 Loss 0.061108846\n",
            "Epoch 47 Batch 0.0 Loss 0.057084277\n",
            "Epoch 47 Batch 1.0 Loss 0.058034558\n",
            "Epoch 47 Batch 2.0 Loss 0.056890305\n",
            "Epoch 47 Batch 3.0 Loss 0.055119954\n",
            "Epoch 47 Batch 4.0 Loss 0.05868532\n",
            "Epoch 47 Batch 5.0 Loss 0.057238523\n",
            "Epoch 47 Batch 6.0 Loss 0.057086166\n",
            "Epoch 47 Batch 7.0 Loss 0.056663495\n",
            "Epoch 47 Batch 8.0 Loss 0.056940757\n",
            "Epoch 47 Batch 9.0 Loss 0.05587939\n",
            "Epoch 47 Batch 10.0 Loss 0.056357555\n",
            "Epoch 47 Batch 11.0 Loss 0.05538411\n",
            "Epoch 47 Batch 12.0 Loss 0.055614904\n",
            "Epoch 47 Batch 13.0 Loss 0.056447964\n",
            "Epoch 47 Batch 14.0 Loss 0.05660094\n",
            "Epoch 47 Batch 15.0 Loss 0.05914012\n",
            "Epoch 47 Batch 16.0 Loss 0.059522368\n",
            "Epoch 47 Batch 17.0 Loss 0.059644755\n",
            "Epoch 47 Batch 18.0 Loss 0.06021546\n",
            "Epoch 47 Batch 19.0 Loss 0.05909461\n",
            "Epoch 47 Batch 20.0 Loss 0.058048848\n",
            "Epoch 47 Batch 21.0 Loss 0.06053659\n",
            "Epoch 47 Batch 22.0 Loss 0.058203883\n",
            "Epoch 47 Batch 23.0 Loss 0.0614814\n",
            "Epoch 47 Batch 24.0 Loss 0.060144853\n",
            "Epoch 46 Batch 0.0 Loss 0.056673966\n",
            "Epoch 46 Batch 1.0 Loss 0.057496205\n",
            "Epoch 46 Batch 2.0 Loss 0.05618711\n",
            "Epoch 46 Batch 3.0 Loss 0.05430217\n",
            "Epoch 46 Batch 4.0 Loss 0.06362708\n",
            "Epoch 46 Batch 5.0 Loss 0.058126926\n",
            "Epoch 46 Batch 6.0 Loss 0.05560757\n",
            "Epoch 46 Batch 7.0 Loss 0.055026356\n",
            "Epoch 46 Batch 8.0 Loss 0.057594415\n",
            "Epoch 46 Batch 9.0 Loss 0.057963744\n",
            "Epoch 46 Batch 10.0 Loss 0.058036923\n",
            "Epoch 46 Batch 11.0 Loss 0.055650197\n",
            "Epoch 46 Batch 12.0 Loss 0.05565527\n",
            "Epoch 46 Batch 13.0 Loss 0.056893207\n",
            "Epoch 46 Batch 14.0 Loss 0.05554999\n",
            "Epoch 46 Batch 15.0 Loss 0.060755946\n",
            "Epoch 46 Batch 16.0 Loss 0.061488796\n",
            "Epoch 46 Batch 17.0 Loss 0.059055854\n",
            "Epoch 46 Batch 18.0 Loss 0.061004855\n",
            "Epoch 46 Batch 19.0 Loss 0.05953664\n",
            "Epoch 46 Batch 20.0 Loss 0.05545717\n",
            "Epoch 46 Batch 21.0 Loss 0.05912473\n",
            "Epoch 46 Batch 22.0 Loss 0.059264783\n",
            "Epoch 46 Batch 23.0 Loss 0.058206692\n",
            "Epoch 46 Batch 24.0 Loss 0.062459566\n",
            "Epoch 45 Batch 0.0 Loss 0.05587478\n",
            "Epoch 45 Batch 1.0 Loss 0.058492366\n",
            "Epoch 45 Batch 2.0 Loss 0.05674642\n",
            "Epoch 45 Batch 3.0 Loss 0.055826224\n",
            "Epoch 45 Batch 4.0 Loss 0.05799048\n",
            "Epoch 45 Batch 5.0 Loss 0.055709686\n",
            "Epoch 45 Batch 6.0 Loss 0.056528192\n",
            "Epoch 45 Batch 7.0 Loss 0.054219812\n",
            "Epoch 45 Batch 8.0 Loss 0.056569885\n",
            "Epoch 45 Batch 9.0 Loss 0.056072365\n",
            "Epoch 45 Batch 10.0 Loss 0.058128506\n",
            "Epoch 45 Batch 11.0 Loss 0.055314645\n",
            "Epoch 45 Batch 12.0 Loss 0.054443493\n",
            "Epoch 45 Batch 13.0 Loss 0.05602169\n",
            "Epoch 45 Batch 14.0 Loss 0.058090698\n",
            "Epoch 45 Batch 15.0 Loss 0.05912939\n",
            "Epoch 45 Batch 16.0 Loss 0.061479963\n",
            "Epoch 45 Batch 17.0 Loss 0.059811268\n",
            "Epoch 45 Batch 18.0 Loss 0.060723595\n",
            "Epoch 45 Batch 19.0 Loss 0.057595845\n",
            "Epoch 45 Batch 20.0 Loss 0.056369618\n",
            "Epoch 45 Batch 21.0 Loss 0.06095231\n",
            "Epoch 45 Batch 22.0 Loss 0.059090596\n",
            "Epoch 45 Batch 23.0 Loss 0.057880234\n",
            "Epoch 45 Batch 24.0 Loss 0.06164949\n",
            "Epoch 44 Batch 0.0 Loss 0.054608118\n",
            "Epoch 44 Batch 1.0 Loss 0.05570207\n",
            "Epoch 44 Batch 2.0 Loss 0.05734267\n",
            "Epoch 44 Batch 3.0 Loss 0.053796597\n",
            "Epoch 44 Batch 4.0 Loss 0.05975063\n",
            "Epoch 44 Batch 5.0 Loss 0.05505802\n",
            "Epoch 44 Batch 6.0 Loss 0.0560328\n",
            "Epoch 44 Batch 7.0 Loss 0.054526538\n",
            "Epoch 44 Batch 8.0 Loss 0.05711944\n",
            "Epoch 44 Batch 9.0 Loss 0.056396402\n",
            "Epoch 44 Batch 10.0 Loss 0.05623969\n",
            "Epoch 44 Batch 11.0 Loss 0.053451873\n",
            "Epoch 44 Batch 12.0 Loss 0.05517386\n",
            "Epoch 44 Batch 13.0 Loss 0.05549555\n",
            "Epoch 44 Batch 14.0 Loss 0.058001097\n",
            "Epoch 44 Batch 15.0 Loss 0.060292147\n",
            "Epoch 44 Batch 16.0 Loss 0.06029579\n",
            "Epoch 44 Batch 17.0 Loss 0.061477315\n",
            "Epoch 44 Batch 18.0 Loss 0.06160176\n",
            "Epoch 44 Batch 19.0 Loss 0.058788303\n",
            "Epoch 44 Batch 20.0 Loss 0.05827881\n",
            "Epoch 44 Batch 21.0 Loss 0.059085023\n",
            "Epoch 44 Batch 22.0 Loss 0.05683647\n",
            "Epoch 44 Batch 23.0 Loss 0.059843607\n",
            "Epoch 44 Batch 24.0 Loss 0.06042038\n",
            "Epoch 43 Batch 0.0 Loss 0.054073963\n",
            "Epoch 43 Batch 1.0 Loss 0.0563284\n",
            "Epoch 43 Batch 2.0 Loss 0.05574466\n",
            "Epoch 43 Batch 3.0 Loss 0.05516381\n",
            "Epoch 43 Batch 4.0 Loss 0.060607344\n",
            "Epoch 43 Batch 5.0 Loss 0.058121342\n",
            "Epoch 43 Batch 6.0 Loss 0.054203138\n",
            "Epoch 43 Batch 7.0 Loss 0.055542126\n",
            "Epoch 43 Batch 8.0 Loss 0.05788725\n",
            "Epoch 43 Batch 9.0 Loss 0.058219954\n",
            "Epoch 43 Batch 10.0 Loss 0.057493433\n",
            "Epoch 43 Batch 11.0 Loss 0.055897284\n",
            "Epoch 43 Batch 12.0 Loss 0.05435444\n",
            "Epoch 43 Batch 13.0 Loss 0.055822454\n",
            "Epoch 43 Batch 14.0 Loss 0.056418438\n",
            "Epoch 43 Batch 15.0 Loss 0.06069705\n",
            "Epoch 43 Batch 16.0 Loss 0.061130304\n",
            "Epoch 43 Batch 17.0 Loss 0.060839243\n",
            "Epoch 43 Batch 18.0 Loss 0.06349417\n",
            "Epoch 43 Batch 19.0 Loss 0.06019629\n",
            "Epoch 43 Batch 20.0 Loss 0.055938344\n",
            "Epoch 43 Batch 21.0 Loss 0.058504153\n",
            "Epoch 43 Batch 22.0 Loss 0.055986628\n",
            "Epoch 43 Batch 23.0 Loss 0.059024516\n",
            "Epoch 43 Batch 24.0 Loss 0.060058735\n",
            "Epoch 42 Batch 0.0 Loss 0.054339387\n",
            "Epoch 42 Batch 1.0 Loss 0.057421576\n",
            "Epoch 42 Batch 2.0 Loss 0.057417057\n",
            "Epoch 42 Batch 3.0 Loss 0.054170083\n",
            "Epoch 42 Batch 4.0 Loss 0.060610108\n",
            "Epoch 42 Batch 5.0 Loss 0.0590555\n",
            "Epoch 42 Batch 6.0 Loss 0.0547049\n",
            "Epoch 42 Batch 7.0 Loss 0.05504992\n",
            "Epoch 42 Batch 8.0 Loss 0.05661727\n",
            "Epoch 42 Batch 9.0 Loss 0.056709122\n",
            "Epoch 42 Batch 10.0 Loss 0.056475163\n",
            "Epoch 42 Batch 11.0 Loss 0.055737093\n",
            "Epoch 42 Batch 12.0 Loss 0.05469065\n",
            "Epoch 42 Batch 13.0 Loss 0.055205144\n",
            "Epoch 42 Batch 14.0 Loss 0.055317774\n",
            "Epoch 42 Batch 15.0 Loss 0.058781892\n",
            "Epoch 42 Batch 16.0 Loss 0.06009296\n",
            "Epoch 42 Batch 17.0 Loss 0.060840268\n",
            "Epoch 42 Batch 18.0 Loss 0.063253716\n",
            "Epoch 42 Batch 19.0 Loss 0.06068284\n",
            "Epoch 42 Batch 20.0 Loss 0.055082146\n",
            "Epoch 42 Batch 21.0 Loss 0.06078682\n",
            "Epoch 42 Batch 22.0 Loss 0.05885212\n",
            "Epoch 42 Batch 23.0 Loss 0.05676271\n",
            "Epoch 42 Batch 24.0 Loss 0.061835747\n",
            "Epoch 41 Batch 0.0 Loss 0.054630738\n",
            "Epoch 41 Batch 1.0 Loss 0.056996416\n",
            "Epoch 41 Batch 2.0 Loss 0.05447524\n",
            "Epoch 41 Batch 3.0 Loss 0.05401923\n",
            "Epoch 41 Batch 4.0 Loss 0.058057558\n",
            "Epoch 41 Batch 5.0 Loss 0.05697997\n",
            "Epoch 41 Batch 6.0 Loss 0.05385969\n",
            "Epoch 41 Batch 7.0 Loss 0.054726884\n",
            "Epoch 41 Batch 8.0 Loss 0.056714267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "InihtcorSzMn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainQues = data[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGJe9ZPRq7Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBLV9NhirZ5u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = predict(trainQues,ques,siamese,three_has_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oadBrReYKBPU",
        "outputId": "b53a87c0-f7cc-4d45-f85b-a319a168ef52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare data and tf.session\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() \n",
        "vectorizer = CountVectorizer()\n",
        "tokenizer=vectorizer.build_tokenizer()\n",
        "rowcount = 0\n",
        "qList,aList = [],[]\n",
        "for row in data[0:100]:\n",
        "  rowcount += 1\n",
        "  quesVec,ansVec = [0]*64881,[0]*64881\n",
        "  quesHash = getThreeHash(row[1].lower())\n",
        "  ansHash = getThreeHash(row[3].lower())\n",
        "#   print(quesHash)\n",
        "  for token in tokenizer(quesHash.lower()):\n",
        "     quesVec[three_hash_dict[token]] += 1\n",
        "  for token in tokenizer(ansHash.lower()):\n",
        "     ansVec[three_hash_dict[token]] += 1\n",
        "  qList.append(quesVec)\n",
        "  aList.append(ansVec)\n",
        "  if rowcount % 10 == 0:    \n",
        "    trainSiamese(qList,aList)\n",
        "    qList,aList = [],[]\n",
        "sess.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n",
            "step 0: loss 9.297\n",
            "step 10: loss 0.676\n",
            "step 20: loss 0.385\n",
            "step 30: loss 0.253\n",
            "step 40: loss 0.169\n",
            "step 50: loss 0.113\n",
            "step 60: loss 0.075\n",
            "step 70: loss 0.050\n",
            "step 80: loss 0.034\n",
            "step 90: loss 0.022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3db983ea131f>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesVec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# setup siamese network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msiamese\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Siamese\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64881\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mactivated_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmaxpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxp_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(self, name, inputs, cur_channel)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(prev_channel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_w\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable Siamese/conv_1_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-19-91aa96e5ce29>\", line 36, in conv_layer\n    w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 21, in network\n    activated_conv1 = self.conv_layer('conv_1',x,3)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 9, in __init__\n    self.o1 = self.network(self.x1)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qdLocomkdJ4p",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}