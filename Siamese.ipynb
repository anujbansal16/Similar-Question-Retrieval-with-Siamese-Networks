{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sWwewmyoKBOK",
        "colab_type": "code",
        "outputId": "c535388f-b46d-4bf4-c78c-688d8ab6ce70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 31.7MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.2MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.1MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.4MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 4.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 4.9MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 6.9MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 6.9MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 12.6MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 12.7MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 12.6MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 12.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 12.8MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 12.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 51.2MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 14.8MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 14.7MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 15.0MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 15.0MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 15.0MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.5MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 14.7MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 14.7MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 14.7MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 15.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 61.8MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 62.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 63.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 56.0MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 55.9MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 65.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 65.4MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 66.1MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 17.7MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 17.3MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 17.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 17.1MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 17.1MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 17.3MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 17.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 17.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 17.3MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 17.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 61.0MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 60.7MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 61.7MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 62.9MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 64.3MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 71.7MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 72.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 67.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 65.1MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 63.7MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 64.3MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 69.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 69.0MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 68.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 67.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 66.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 52.4MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 23.8MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 23.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 23.6MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 23.4MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 23.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 23.0MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 22.9MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 22.9MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 22.8MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 24.9MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 58.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 58.1MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 57.1MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 56.3MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 54.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 53.6MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 52.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 51.7MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 46.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 45.1MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 46.1MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 46.2MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.4MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 46.5MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 47.5MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 47.9MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 48.1MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 47.8MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 53.8MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 53.6MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 53.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 21.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "81afacf8-67ce-4680-e069-815959f8f5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
        "\n",
        "        with tf.variable_scope(\"Siamese\") as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        #self.loss = self.loss_with_spring()\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = tf.nn.relu(conv+b)\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\")  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "\n",
        "\n",
        "    def loss_with_spring(self):\n",
        "        margin = 5.0\n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        eucd2 = tf.pow(tf.subtract(self.o1, self.o2), 2)\n",
        "        eucd2 = tf.reduce_sum(eucd2, 1)\n",
        "        eucd = tf.sqrt(eucd2+1e-6, name=\"eucd\")\n",
        "        \n",
        "        C = tf.constant(margin, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t, eucd2, name=\"yi_x_eucd2\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.pow(tf.maximum(tf.subtract(C, eucd), 0), 2), name=\"Nyi_x_C-eucd_xx_2\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "\n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.4, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t, cosines, name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNX5a8KeKBOu",
        "colab_type": "code",
        "outputId": "116a21f3-ccd9-4ad5-cd05-1607a971d316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1AycFi7m8_NYsK0zTuaAFrci0cLQdE51i\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n",
        "# quest = \"where do i find hot girls\"\n",
        "# hash_var = getThreeHash(quest)\n",
        "#print(hash_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFQrr4tINCx2",
        "colab_type": "code",
        "outputId": "c6e6065f-f9d0-44ae-b69f-9f5b61055fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "data = pd.read_csv('QA.csv',error_bad_lines=False).values"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5KHuloK2OV8k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   q_list.append(quesVec)\n",
        "#   a_list.append(ansVec)\n",
        "#   q_list,a_list = np.array(q_list),np.array(a_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eSyhN4yTNM1",
        "colab_type": "code",
        "outputId": "8bc8fdc0-c990-4be5-bb41-6231b3057a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# print(list(q_list[1]).count(1))\n",
        "# print(q_list.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 64881)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vtzXfaq-sfv2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# new_hash_vec = [0]*64881\n",
        "# vectorizer = CountVectorizer()\n",
        "# tokenizer=vectorizer.build_tokenizer()\n",
        "# for token in tokenizer(hash_var):\n",
        "#   new_hash_vec[three_hash_dict[token]] = 1\n",
        "# #print(new_hash_vec.count(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9qXk85hdLIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiamese(quesVec,ansVec):\n",
        "  # setup siamese network\n",
        "  siamese = Siamese();\n",
        "  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(siamese.loss)\n",
        "\n",
        "  tf.initialize_all_variables().run()\n",
        "\n",
        "\n",
        "  for step in range(100):\n",
        "      batch_x1, batch_y1 = quesVec,np.array([1]*10)\n",
        "#       print(batch_x1)\n",
        "#       print(batch_y1)\n",
        "      batch_x2, batch_y2 = ansVec,np.array([1]*10)\n",
        "      batch_y = (batch_y1 == batch_y2).astype('float')\n",
        "\n",
        "      _, loss_v = sess.run([train_step,siamese.loss], feed_dict={\n",
        "                          siamese.x1: batch_x1,\n",
        "                          siamese.x2: batch_x2,\n",
        "                          siamese.y_: batch_y})\n",
        "      \n",
        "      if np.isnan(loss_v):\n",
        "          print('Model diverged with loss = NaN')\n",
        "          quit()\n",
        "\n",
        "      if step % 10 == 0:\n",
        "          print ('step %d: loss %.3f' % (step, loss_v))\n",
        "\n",
        "      if step % 1000 == 0 and step > 0:\n",
        "          #saver.save(sess, './model')\n",
        "          embed = siamese.o1.eval({siamese.x1: mnist.test.images})\n",
        "          embed.tofile('embed.txt')\n",
        "\n",
        "  # visualize result\n",
        "  x_test = mnist.test.images.reshape([-1, 28, 28])\n",
        "  y_test = mnist.test.labels\n",
        "  #visualize.visualize(embed, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwGPkK3SON0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oadBrReYKBPU",
        "colab_type": "code",
        "outputId": "2890f7dc-6b3e-45e1-dea9-a91f9f8552c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2308
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare data and tf.session\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() \n",
        "vectorizer = CountVectorizer()\n",
        "tokenizer=vectorizer.build_tokenizer()\n",
        "rowcount = 0\n",
        "qList,aList = [],[]\n",
        "for row in data[0:100]:\n",
        "  rowcount += 1\n",
        "  quesVec,ansVec = [0]*64881,[0]*64881\n",
        "  quesHash = getThreeHash(row[1].lower())\n",
        "  ansHash = getThreeHash(row[3].lower())\n",
        "#   print(quesHash)\n",
        "  for token in tokenizer(quesHash.lower()):\n",
        "     quesVec[three_hash_dict[token]] += 1\n",
        "  for token in tokenizer(ansHash.lower()):\n",
        "     ansVec[three_hash_dict[token]] += 1\n",
        "  qList.append(quesVec)\n",
        "  aList.append(ansVec)\n",
        "  if rowcount % 10 == 0:    \n",
        "    trainSiamese(qList,aList)\n",
        "    qList,aList = [],[]\n",
        "sess.close()\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n",
            "22.066473\n",
            "step 0: loss 22.066\n",
            "31971.613\n",
            "12.097725\n",
            "11.618654\n",
            "11.158556\n",
            "10.716678\n",
            "10.292297\n",
            "9.884722\n",
            "9.493287\n",
            "9.117353\n",
            "8.756306\n",
            "step 10: loss 8.756\n",
            "8.409556\n",
            "8.076538\n",
            "7.756706\n",
            "7.4495416\n",
            "7.15454\n",
            "6.8712206\n",
            "6.5991197\n",
            "6.3377943\n",
            "6.0868177\n",
            "5.8457804\n",
            "step 20: loss 5.846\n",
            "5.6142864\n",
            "5.391961\n",
            "5.1784396\n",
            "4.9733734\n",
            "4.7764273\n",
            "4.587281\n",
            "4.405625\n",
            "4.231162\n",
            "4.063608\n",
            "3.9026895\n",
            "step 30: loss 3.903\n",
            "3.7481427\n",
            "3.5997162\n",
            "3.4571679\n",
            "3.3202636\n",
            "3.188781\n",
            "3.0625052\n",
            "2.9412303\n",
            "2.8247573\n",
            "2.7128973\n",
            "2.6054664\n",
            "step 40: loss 2.605\n",
            "2.50229\n",
            "2.4031992\n",
            "2.3080325\n",
            "2.2166348\n",
            "2.128856\n",
            "2.0445533\n",
            "1.963589\n",
            "1.8858306\n",
            "1.8111519\n",
            "1.7394302\n",
            "step 50: loss 1.739\n",
            "1.6705487\n",
            "1.6043952\n",
            "1.540861\n",
            "1.4798429\n",
            "1.4212412\n",
            "1.36496\n",
            "1.3109076\n",
            "1.2589955\n",
            "1.2091393\n",
            "1.1612575\n",
            "step 60: loss 1.161\n",
            "1.1152717\n",
            "1.0711069\n",
            "1.028691\n",
            "0.9879549\n",
            "0.94883186\n",
            "0.9112581\n",
            "0.87517226\n",
            "0.8405155\n",
            "0.80723107\n",
            "0.77526474\n",
            "step 70: loss 0.775\n",
            "0.74456424\n",
            "0.7150795\n",
            "0.68676233\n",
            "0.6595665\n",
            "0.63344765\n",
            "0.60836315\n",
            "0.584272\n",
            "0.5611348\n",
            "0.5389139\n",
            "0.5175729\n",
            "step 80: loss 0.518\n",
            "0.49707705\n",
            "0.47739276\n",
            "0.45848805\n",
            "0.44033188\n",
            "0.42289478\n",
            "0.4061481\n",
            "0.39006466\n",
            "0.37461808\n",
            "0.35978323\n",
            "0.3455358\n",
            "step 90: loss 0.346\n",
            "0.3318526\n",
            "0.31871122\n",
            "0.30609027\n",
            "0.2939691\n",
            "0.2823279\n",
            "0.27114773\n",
            "0.26041028\n",
            "0.25009805\n",
            "0.24019413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ac17a09393ff>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# visualize result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#visualize.visualize(embed, x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qdLocomkdJ4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}