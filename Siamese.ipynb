{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "o5SKv74zumzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network Code \n",
        "\n",
        "The following modules in python has been used \n",
        "<ul>\n",
        "  <li>Tensorflow for constructing Siamese Networks</li>\n",
        "  <li>NLTK library for stemming and stop words removal</li>\n",
        "  <li>Google Drive library for data strage</li>\n",
        "  <li>Scikit, Numpy and Pandas for misc. tensor computations</li>\n",
        "<ul>\n",
        "  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sWwewmyoKBOK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "5db2d1f6-78e9-4cc3-f67e-0b340cc2459f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PpbKs-NxHTnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network Implementaiton\n",
        "\n",
        "Implementation is mostly done using barebones tensorflow with tensor variables, placeholders and constants . No predefined library was used in this implementation. The structure below is not a generic siamese network but can be made one by generalizing a few parameters."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idim = 30628\n",
        "\n",
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self,dname=\"Siamese\"):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, idim])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, idim])\n",
        "\n",
        "        with tf.variable_scope(dname) as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    #create the network \n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,idim,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    #create the convolution layer \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = conv+b\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.relu(tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\"))  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "    \n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.5, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvefB7RvORvC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Pickled Dictionary stored in Drive\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GNX5a8KeKBOu",
        "outputId": "3539073c-dd0d-49bf-9ff6-cf1ffcbe677c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1Jhj9OazxPnvLcuuZsZvNFfpnnsFg88I7\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Ocjg2ZXOb6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Three hash generation function\n",
        "\n",
        "This function takes the text and tokenizes it, performs steming and stop words removal using NLTK. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  ps = PorterStemmer()\n",
        "  tokens = [ps.stem(word) for word in tokens]\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zHnVKtSGOtz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Data Set"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bFQrr4tINCx2",
        "outputId": "8bdc89a8-c13e-4bf2-9459-73e28a8235c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fKboKIIMOzxo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Take only a subset as training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4OzxqKw47mcT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[:5000,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-jVsYHKO5Q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cosine Distance SImilarity \n",
        "\n",
        "This is done so that systactically similar questions are taken into consideration. The distance metric used is a cosine distance on simple word hashed representation of texts."
      ]
    },
    {
      "metadata": {
        "id": "9u09aOmJXNfY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokenlower = [word.lower() for word in tokens]\n",
        "    \n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    tokenlower = [word for word in stopWords if word not in stopWords]\n",
        "    \n",
        "    tokenDict = nltk.defaultdict(int)\n",
        "    for word in tokens:\n",
        "        tokenDict[word] += 1\n",
        "    return tokenDict\n",
        "\n",
        "def cosDistance(v1,v2):\n",
        "    dotProduct = np.dot(v1,v2)\n",
        "    normV1 = np.linalg.norm(v1)\n",
        "    normV2 = np.linalg.norm(v2)\n",
        "#     print(dotProduct,normV1,normV2)\n",
        "    return dotProduct / (normV1 * normV2)\n",
        "\n",
        "def getSimilarity(vDict1,vDict2):\n",
        "    allWords = []\n",
        "    for key in vDict1:\n",
        "        allWords.append(key)\n",
        "    for key in vDict2:\n",
        "        allWords.append(key)\n",
        "    allWordsSize = len(allWords)\n",
        "    \n",
        "    v1 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    v2 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    \n",
        "    i = 0\n",
        "    for key in allWords:\n",
        "#         print(key)\n",
        "        v1[i] = vDict1.get(key, 0)\n",
        "        v2[i] = vDict2.get(key, 0)\n",
        "        i += 1\n",
        "#     print(v1,v2)    \n",
        "    return cosDistance(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTXCXyVpPyQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch Creation and vectorization of text"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HwGPkK3SON0N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(hashString,dictionary):\n",
        "  \n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer = vectorizer.build_tokenizer()\n",
        "  vec = [0]*idim\n",
        "  \n",
        "  for token in tokenizer(hashString):\n",
        "    try:\n",
        "      vec[dictionary[token]] += 1\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "  return vec\n",
        "\n",
        "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
        "  \n",
        "  good_data = data[goodSet]\n",
        "  bad_quest = data[badSetQues,0]\n",
        "  bad_anser = data[badSetAns,1]\n",
        "  questions = np.concatenate((good_data[:,0],bad_quest))\n",
        "  answers = np.concatenate((good_data[:,1],bad_anser))\n",
        "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
        "  \n",
        "  ques,ans = [],[]\n",
        "  for d in questions:\n",
        "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
        "  for i,d in enumerate(answers):\n",
        "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
        "  return np.array(ques),np.array(ans),label\n",
        "    \n",
        "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
        "  \n",
        "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
        "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pc_BuyTKQUGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initializing Siamese networks and optimizer"
      ]
    },
    {
      "metadata": {
        "id": "2fI2XdkYUpxY",
        "colab_type": "code",
        "outputId": "fc18a0df-66bc-48dc-a65c-d7d97900080c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "siamese = Siamese('siamese6')\n",
        "try:\n",
        "  sess.close()\n",
        "except:\n",
        "  pass\n",
        "sess = tf.InteractiveSession()\n",
        "train_Step = tf.train.MomentumOptimizer(0.01,0.05).minimize(siamese.loss)\n",
        "tf.initialize_all_variables().run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 921)\n",
            "(?, 921)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rtu0mur1QZe2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training with siamese Networs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wXmBCVN0V3hv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiameseNetwork(data,siamese,sess,batchSize,epochs,dictionary):\n",
        "  \n",
        "  #Siamese Network\n",
        "  #sess = tf.Session()\n",
        "  #tf.reset_default_graph()\n",
        "  #siamese = Siamese()\n",
        "  \n",
        "  while epochs > 0:\n",
        "    \n",
        "    avg = 0.0\n",
        "    permSet = np.random.permutation(data.shape[0])\n",
        "\n",
        "    for p in range(0,permSet.shape[0],batchSize):\n",
        "\n",
        "      goodSet = np.array(list(range(p,p+batchSize)))\n",
        "      badSetQ = goodSet.copy()\n",
        "      badSetA = np.array(permSet[p:p+batchSize])\n",
        "      ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
        "\n",
        "\n",
        "      #print(ques.shape,ans.shape,labl.shape)\n",
        "      _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
        "                    siamese.x1: ques,\n",
        "                    siamese.x2: ans,\n",
        "                    siamese.y_: labl})\n",
        "      avg += Loss\n",
        "      #print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
        "    \n",
        "    print(\"Average Epoch \",epochs,\"Loss \",avg/(data.shape[0]/batchSize))\n",
        "    epochs -= 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8_vJYXycs0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(qset,ques,siamese,sess,dictionary,alpha=0.5,best=3):\n",
        "  \n",
        "  vques  = preprocess(ques)\n",
        "  thQues = np.array(vectorize(getThreeHash(ques.lower()),dictionary)).reshape(1,-1)\n",
        "\n",
        "  scores = []\n",
        "\n",
        "  for q in qset:\n",
        "\n",
        "    oQues = np.array(vectorize(getThreeHash(str(q).lower()),dictionary)).reshape(1,-1)\n",
        "    vQues = preprocess(q)\n",
        "    Loss = sess.run([siamese.loss],feed_dict={\n",
        "                    siamese.x1: thQues,\n",
        "                    siamese.x2: oQues,\n",
        "                    siamese.y_: np.array([1.0])})\n",
        "    tLoss = getSimilarity(vques,vQues)\n",
        "    #print(alpha*Loss[0],(1-alpha)*(1-tLoss))\n",
        "    scores += [alpha*Loss[0] + (1-alpha)*(1-tLoss)]\n",
        "\n",
        "  sscores = np.argsort(np.array(scores))[:best]\n",
        "  return qset[sscores],scores\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sa67sxPHQDl4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training With Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "scF-b7JY8NQW",
        "outputId": "79dc06e4-48d9-43a5-b919-2f22049074a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "trainSiameseNetwork(data,siamese,sess,100,30,three_hash_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Epoch  30 Loss  0.2242837056517601\n",
            "Average Epoch  29 Loss  0.21048615053296088\n",
            "Average Epoch  28 Loss  0.2039389818906784\n",
            "Average Epoch  27 Loss  0.20255933552980424\n",
            "Average Epoch  26 Loss  0.20143184140324594\n",
            "Average Epoch  25 Loss  0.20044633969664574\n",
            "Average Epoch  24 Loss  0.20001706555485727\n",
            "Average Epoch  23 Loss  0.19893319755792618\n",
            "Average Epoch  22 Loss  0.1970414862036705\n",
            "Average Epoch  21 Loss  0.19635800570249556\n",
            "Average Epoch  20 Loss  0.19561513587832452\n",
            "Average Epoch  19 Loss  0.1954384095966816\n",
            "Average Epoch  18 Loss  0.19422918558120728\n",
            "Average Epoch  17 Loss  0.19333649873733522\n",
            "Average Epoch  16 Loss  0.1926034264266491\n",
            "Average Epoch  15 Loss  0.19089846909046174\n",
            "Average Epoch  14 Loss  0.1923725850880146\n",
            "Average Epoch  13 Loss  0.18939405381679536\n",
            "Average Epoch  12 Loss  0.1894181936979294\n",
            "Average Epoch  11 Loss  0.1890207976102829\n",
            "Average Epoch  10 Loss  0.18854231387376785\n",
            "Average Epoch  9 Loss  0.18777668699622155\n",
            "Average Epoch  8 Loss  0.1863868422806263\n",
            "Average Epoch  7 Loss  0.18551967963576316\n",
            "Average Epoch  6 Loss  0.18597794622182845\n",
            "Average Epoch  5 Loss  0.18312337771058082\n",
            "Average Epoch  4 Loss  0.18357508182525634\n",
            "Average Epoch  3 Loss  0.18175122812390326\n",
            "Average Epoch  2 Loss  0.1830047145485878\n",
            "Average Epoch  1 Loss  0.18210964575409888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "InihtcorSzMn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainQues = ndata[:15000,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJRxxT7LQG77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some Results"
      ]
    },
    {
      "metadata": {
        "id": "KGJe9ZPRq7Rj",
        "colab_type": "code",
        "outputId": "92f3fb13-dca8-4c63-eb54-b9d8808a4a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - how to attract a girl\n",
            "Best Three Answers \n",
            "How should I act on a date?\n",
            "Is a transponder required to fly in class C airspace?\n",
            "How do I end a date with a girl I'm not interested in?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWWsqQPa2Dzu",
        "colab_type": "code",
        "outputId": "9843ee45-4bc4-4b9e-d6d4-bdd59b500564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - best place to eat\n",
            "Best Three Answers \n",
            "best place to meet guys in the bay area?\n",
            "Whats a good place to eat in LA?\n",
            "Where's the best place to get my FICO score?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsISYpVV2RzA",
        "colab_type": "code",
        "outputId": "7466fbb4-0b30-440c-b6fe-38f78a51ea0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - best internet site\n",
            "Best Three Answers \n",
            "what is the best news site on the net?\n",
            "How many websites are on the internet?\n",
            "What is the singel most important thing you are missing on the internet ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Efkkr3Gh6oOp",
        "colab_type": "code",
        "outputId": "54d8578c-1b28-4405-e2cc-d7be818ee512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - find peace\n",
            "Best Three Answers \n",
            "Who won the first nobel peace prize?\n",
            "Where can I find help about a war?\n",
            "need to find reserch about work ethics?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qCp4Lupv7RUt",
        "colab_type": "code",
        "outputId": "44a1bc1e-496f-463f-c2a0-76166374f8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - which fruit is good for health\n",
            "Best Three Answers \n",
            "Why is red wine good for your heart?\n",
            "Why are blueberries so good for your health?\n",
            "How do you get a toddler to eat what's good for him?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "40h8xem77nCo",
        "colab_type": "code",
        "outputId": "4e006089-24a2-45b5-c958-3445ae31dd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - Who is most famous celebrity\n",
            "Best Three Answers \n",
            "Who is the most famous woman athlete of all time?\n",
            "What is emo?\n",
            "What is \"I\"?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AW-oaJXB7z6n",
        "colab_type": "code",
        "outputId": "5fb8a2bc-ef62-43c9-b3b1-74f096b4017d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - Who is most famous celebrity\n",
            "Best Three Answers \n",
            "Who is the most famous hacker?\n",
            "What is the most famous breed of horse?\n",
            "Which celebrity would you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-eTPs-VOICSu",
        "colab_type": "code",
        "outputId": "b7322ab3-010d-40f6-eed7-63b996c11066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - best way to publish a book\n",
            "Best Three Answers \n",
            "how do you get a book published?\n",
            "whats the best way to get blood stains out of a white t-shirt?\n",
            "What is the best way to stop smoking?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P2sWtDAKJEno",
        "colab_type": "code",
        "outputId": "2e134afa-f7a1-4ed6-b40e-435454d758fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - How to win a game?\n",
            "Best Three Answers \n",
            "How long have you been a gamer? And Why?\n",
            "game zone.?\n",
            "What or who is a Gant?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1epBB-CJs7M",
        "colab_type": "code",
        "outputId": "654154ac-815f-4d17-8ad1-57d97b94855f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.9)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - What are the advantages of Facebook?\n",
            "Best Three Answers \n",
            "Facebook or MySpace?\n",
            "what is rebate?and what the advantage?\n",
            "what are the advantages of antibacterial products?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zTizHWL8Ju5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}