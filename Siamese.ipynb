{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sWwewmyoKBOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ac19ec3-538f-4c53-c94d-b0a638634ac2"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "eaa95031-d9cf-4508-a051-a45cbc409589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
        "\n",
        "        with tf.variable_scope(\"Siamese\") as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        self.loss = self.loss_with_spring()\n",
        "\n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = tf.nn.relu(conv+b)\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,2,1],padding=\"SAME\")  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "\n",
        "\n",
        "    def loss_with_spring(self):\n",
        "        margin = 5.0\n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        eucd2 = tf.pow(tf.subtract(self.o1, self.o2), 2)\n",
        "        eucd2 = tf.reduce_sum(eucd2, 1)\n",
        "        eucd = tf.sqrt(eucd2+1e-6, name=\"eucd\")\n",
        "        \n",
        "        C = tf.constant(margin, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t, eucd2, name=\"yi_x_eucd2\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.pow(tf.maximum(tf.subtract(C, eucd), 0), 2), name=\"Nyi_x_C-eucd_xx_2\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNX5a8KeKBOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f1b6343-cd08-407f-f711-18e489e16a0e"
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1AycFi7m8_NYsK0zTuaAFrci0cLQdE51i\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n",
        "# quest = \"where do i find hot girls\"\n",
        "# hash_var = getThreeHash(quest)\n",
        "#print(hash_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFQrr4tINCx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f38e9142-7edb-4bdf-c973-2b30039be3b3"
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "data = pd.read_csv('QA.csv',error_bad_lines=False).values"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5KHuloK2OV8k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   q_list.append(quesVec)\n",
        "#   a_list.append(ansVec)\n",
        "#   q_list,a_list = np.array(q_list),np.array(a_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eSyhN4yTNM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bc8fdc0-c990-4be5-bb41-6231b3057a6b"
      },
      "cell_type": "code",
      "source": [
        "# print(list(q_list[1]).count(1))\n",
        "# print(q_list.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 64881)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vtzXfaq-sfv2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# new_hash_vec = [0]*64881\n",
        "# vectorizer = CountVectorizer()\n",
        "# tokenizer=vectorizer.build_tokenizer()\n",
        "# for token in tokenizer(hash_var):\n",
        "#   new_hash_vec[three_hash_dict[token]] = 1\n",
        "# #print(new_hash_vec.count(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9qXk85hdLIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiamese(quesVec,ansVec):\n",
        "  # setup siamese network\n",
        "  siamese = Siamese();\n",
        "  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(siamese.loss)\n",
        "\n",
        "  tf.initialize_all_variables().run()\n",
        "\n",
        "\n",
        "  for step in range(100):\n",
        "      batch_x1, batch_y1 = quesVec,np.array([1]*10)\n",
        "#       print(batch_x1)\n",
        "#       print(batch_y1)\n",
        "      batch_x2, batch_y2 = ansVec,np.array([1]*10)\n",
        "      batch_y = (batch_y1 == batch_y2).astype('float')\n",
        "\n",
        "      _, loss_v = sess.run([train_step,siamese.loss], feed_dict={\n",
        "                          siamese.x1: batch_x1,\n",
        "                          siamese.x2: batch_x2,\n",
        "                          siamese.y_: batch_y})\n",
        "\n",
        "      if np.isnan(loss_v):\n",
        "          print('Model diverged with loss = NaN')\n",
        "          quit()\n",
        "\n",
        "      if step % 10 == 0:\n",
        "          print ('step %d: loss %.3f' % (step, loss_v))\n",
        "\n",
        "      if step % 1000 == 0 and step > 0:\n",
        "          #saver.save(sess, './model')\n",
        "          embed = siamese.o1.eval({siamese.x1: mnist.test.images})\n",
        "          embed.tofile('embed.txt')\n",
        "\n",
        "  # visualize result\n",
        "  x_test = mnist.test.images.reshape([-1, 28, 28])\n",
        "  y_test = mnist.test.labels\n",
        "  #visualize.visualize(embed, x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oadBrReYKBPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "8f0a8bdc-c9af-490b-cc2f-f574780cef7c"
      },
      "cell_type": "code",
      "source": [
        "# prepare data and tf.session\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() \n",
        "vectorizer = CountVectorizer()\n",
        "tokenizer=vectorizer.build_tokenizer()\n",
        "rowcount = 0\n",
        "qList,aList = [],[]\n",
        "for row in data[0:100]:\n",
        "  rowcount += 1\n",
        "  quesVec,ansVec = [0]*64881,[0]*64881\n",
        "  quesHash = getThreeHash(row[1].lower())\n",
        "  ansHash = getThreeHash(row[3].lower())\n",
        "#   print(quesHash)\n",
        "  for token in tokenizer(quesHash.lower()):\n",
        "     quesVec[three_hash_dict[token]] += 1\n",
        "  for token in tokenizer(ansHash.lower()):\n",
        "     ansVec[three_hash_dict[token]] += 1\n",
        "  qList.append(quesVec)\n",
        "  aList.append(ansVec)\n",
        "  if rowcount % 10 == 0:    \n",
        "    trainSiamese(qList,aList)\n",
        "    qList,aList = [],[]\n",
        "sess.close()\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 97323)\n",
            "(?, 97323)\n",
            "step 0: loss 250.736\n",
            "step 10: loss 0.000\n",
            "step 20: loss 0.000\n",
            "step 30: loss 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-ab37a9722ec9>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                           \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                           siamese.y_: batch_y})\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HwGPkK3SON0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdLocomkdJ4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}