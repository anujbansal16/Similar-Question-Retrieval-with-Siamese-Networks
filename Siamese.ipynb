{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWwewmyoKBOK"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from builtins import input\n",
    "\n",
    "#import system things\n",
    "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "\n",
    "\n",
    "# Code to read csv file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "import tensorflow as tf\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "cd76XW7KpHCy",
    "outputId": "02ff2724-7826-4281-dec7-20dde54d46f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoYoHnhXx2zt"
   },
   "outputs": [],
   "source": [
    "class Siamese:\n",
    "\n",
    "    # Create model\n",
    "    def __init__(self,dname=\"Siamese\"):\n",
    "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
    "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
    "\n",
    "        with tf.variable_scope(dname) as scope:\n",
    "            self.o1 = self.network(self.x1)\n",
    "            scope.reuse_variables()\n",
    "            self.o2 = self.network(self.x2)\n",
    "\n",
    "        # Create loss\n",
    "        self.y_ = tf.placeholder(tf.float32, [None])\n",
    "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
    "        self.loss = self.cosineLoss()\n",
    "\n",
    "    def network(self, x):\n",
    "        \n",
    "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
    "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
    "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
    "        \n",
    "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
    "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
    "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
    "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
    "        \n",
    "        return activated_fc1\n",
    "        \n",
    "    def conv_layer(self,name,inputs,cur_channel):\n",
    "        #print(inputs.get_shape())\n",
    "        prev_channel = inputs.get_shape()[-1]\n",
    "        #print(prev_channel)\n",
    "        init = tf.variance_scaling_initializer(scale=2.0)\n",
    "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
    "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
    "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
    "        activation = conv+b\n",
    "        return activation  \n",
    "      \n",
    "    def maxpool_layer(self,name,inputs):\n",
    "        return tf.nn.relu(tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\"))  \n",
    "    \n",
    "    def fc_layer(self,name,inputs,cur_layer):\n",
    "        print(inputs.get_shape())\n",
    "        prev_layer = inputs.get_shape()[-1]\n",
    "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
    "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
    "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
    "        activation = tf.matmul(inputs,w)+b\n",
    "        return activation\n",
    "    \n",
    "        \n",
    "    def cosineLoss(self):\n",
    "        \n",
    "        norms1 = tf.norm(self.o1,axis=1)\n",
    "        norms2 = tf.norm(self.o2,axis=1)\n",
    "        norm = tf.multiply(norms1,norms2)\n",
    "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
    "        \n",
    "        labels_t = self.y_\n",
    "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
    "        \n",
    "        \n",
    "        C = tf.constant(0.85, name=\"C\")\n",
    "        \n",
    "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
    "        \n",
    "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
    "        losses = tf.add(pos, neg, name=\"losses\")\n",
    "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "        return loss\n",
    "        \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GNX5a8KeKBOu",
    "outputId": "cdada79b-ba24-441a-a261-4cd6277d8b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30628\n"
     ]
    }
   ],
   "source": [
    "link=\"https://drive.google.com/open?id=1Jhj9OazxPnvLcuuZsZvNFfpnnsFg88I7\"\n",
    "fluff, id = link.split('=')\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('three_hash')  \n",
    "with open('three_hash','rb') as f:\n",
    "   three_hash_dict = pickle.load(f)\n",
    "print(len(three_hash_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kt-JQ95JojbW"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def getThreeHash(text):\n",
    "  vectorizer = CountVectorizer()\n",
    "  tokenizer=vectorizer.build_tokenizer()\n",
    "  \n",
    "  hashes=\"\"\n",
    "  tokens=tokenizer(text)\n",
    "  ps = PorterStemmer()\n",
    "  tokens = [ps.stem(word) for word in tokens]\n",
    "  for token in tokens:\n",
    "    if token not in stop_words:\n",
    "      tokenModi=\"#\"+token+\"#\"\n",
    "      output = list(ngrams(tokenModi, 3))\n",
    "      for a in output:\n",
    "        hashes+=(''.join(a))+\" \"\n",
    "  \n",
    "  return(hashes)\n",
    "# quest = \"where do i find hot girls\"\n",
    "# hash_var = getThreeHash(quest)\n",
    "#print(hash_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bFQrr4tINCx2",
    "outputId": "7db19b53-f0f5-4145-f1c2-42989482c6f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\n"
     ]
    }
   ],
   "source": [
    "link=\"https://drive.google.com/open?id=17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\" #create shareable link of google drive file\n",
    "fluff, id = link.split('=')\n",
    "print (id) # Verify that you have everything after '='\n",
    "\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('QA.csv')  \n",
    "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OzxqKw47mcT"
   },
   "outputs": [],
   "source": [
    "data = ndata[:1000,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9u09aOmJXNfY"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokenlower = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    tokenlower = [word for word in stopWords if word not in stopWords]\n",
    "    \n",
    "    tokenDict = nltk.defaultdict(int)\n",
    "    for word in tokens:\n",
    "        tokenDict[word] += 1\n",
    "    return tokenDict\n",
    "\n",
    "def cosDistance(v1,v2):\n",
    "    dotProduct = np.dot(v1,v2)\n",
    "    normV1 = np.linalg.norm(v1)\n",
    "    normV2 = np.linalg.norm(v2)\n",
    "#     print(dotProduct,normV1,normV2)\n",
    "    return dotProduct / (normV1 * normV2)\n",
    "\n",
    "def getSimilarity(vDict1,vDict2):\n",
    "    allWords = []\n",
    "    for key in vDict1:\n",
    "        allWords.append(key)\n",
    "    for key in vDict2:\n",
    "        allWords.append(key)\n",
    "    allWordsSize = len(allWords)\n",
    "    \n",
    "    v1 = np.zeros(allWordsSize,dtype=np.int)\n",
    "    v2 = np.zeros(allWordsSize,dtype=np.int)\n",
    "    \n",
    "    i = 0\n",
    "    for key in allWords:\n",
    "#         print(key)\n",
    "        v1[i] = vDict1.get(key, 0)\n",
    "        v2[i] = vDict2.get(key, 0)\n",
    "        i += 1\n",
    "#     print(v1,v2)    \n",
    "    return cosDistance(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwGPkK3SON0N"
   },
   "outputs": [],
   "source": [
    "def vectorize(hashString,dictionary):\n",
    "  \n",
    "  vectorizer = CountVectorizer()\n",
    "  tokenizer = vectorizer.build_tokenizer()\n",
    "  vec = [0]*64881\n",
    "  \n",
    "  for token in tokenizer(hashString):\n",
    "    try:\n",
    "      vec[dictionary[token]] += 1\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "  return vec\n",
    "\n",
    "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
    "  \n",
    "  good_data = data[goodSet]\n",
    "  bad_quest = data[badSetQues,0]\n",
    "  bad_anser = data[badSetAns,1]\n",
    "  questions = np.concatenate((good_data[:,0],bad_quest))\n",
    "  answers = np.concatenate((good_data[:,1],bad_anser))\n",
    "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
    "  \n",
    "  ques,ans = [],[]\n",
    "  for d in questions:\n",
    "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
    "  for i,d in enumerate(answers):\n",
    "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
    "  return np.array(ques),np.array(ans),label\n",
    "    \n",
    "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
    "  \n",
    "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
    "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
    "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
    "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2fI2XdkYUpxY",
    "outputId": "7b2be88a-8b28-4791-e6b0-76a8c10e09d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1947)\n",
      "(?, 1947)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "siamese = Siamese('siamese6')\n",
    "try:\n",
    "  sess.close()\n",
    "except:\n",
    "  pass\n",
    "sess = tf.InteractiveSession()\n",
    "train_Step = tf.train.MomentumOptimizer(0.01,0.05).minimize(siamese.loss)\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXmBCVN0V3hv"
   },
   "outputs": [],
   "source": [
    "def trainSiameseNetwork(data,siamese,sess,batchSize,epochs,dictionary):\n",
    "  \n",
    "  #Siamese Network\n",
    "  #sess = tf.Session()\n",
    "  #tf.reset_default_graph()\n",
    "  #siamese = Siamese()\n",
    "  \n",
    "  while epochs > 0:\n",
    "    \n",
    "    avg = 0.0\n",
    "    permSet = np.random.permutation(data.shape[0])\n",
    "\n",
    "    for p in range(0,permSet.shape[0],batchSize):\n",
    "\n",
    "      goodSet = np.array(list(range(p,p+batchSize)))\n",
    "      badSetQ = goodSet.copy()\n",
    "      badSetA = np.array(permSet[p:p+batchSize])\n",
    "      ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
    "\n",
    "\n",
    "      #print(ques.shape,ans.shape,labl.shape)\n",
    "      _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
    "                    siamese.x1: ques,\n",
    "                    siamese.x2: ans,\n",
    "                    siamese.y_: labl})\n",
    "      avg += Loss\n",
    "      #print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
    "    \n",
    "    print(\"Average Epoch \",epochs,\"Loss \",avg/(data.shape[0]/batchSize))\n",
    "    epochs -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8_vJYXycs0P"
   },
   "outputs": [],
   "source": [
    "def predict(qset,ques,siamese,sess,dictionary,alpha=0.5,best=3):\n",
    "  \n",
    "  vques  = preprocess(ques)\n",
    "  thQues = np.array(vectorize(getThreeHash(ques.lower()),dictionary)).reshape(1,-1)\n",
    "\n",
    "  scores = []\n",
    "\n",
    "  for q in qset:\n",
    "\n",
    "    oQues = np.array(vectorize(getThreeHash(q.lower()),dictionary)).reshape(1,-1)\n",
    "    vQues = preprocess(q)\n",
    "    Loss = sess.run([siamese.loss],feed_dict={\n",
    "                    siamese.x1: thQues,\n",
    "                    siamese.x2: oQues,\n",
    "                    siamese.y_: np.array([1.0])})\n",
    "    tLoss = getSimilarity(vques,vQues)\n",
    "    print(alpha*Loss,(1-alpha)alphatLoss)\n",
    "    scores += [alpha*Loss[0] + (1-alpha)*(1-tLoss)]\n",
    "\n",
    "  sscores = np.argsort(np.array(scores))[:best]\n",
    "  return qset[sscores],scores\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "scF-b7JY8NQW",
    "outputId": "bd64e36b-9ebb-496f-9d54-d510a829df31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch  10 Loss  0.10446406602859497\n",
      "Average Epoch  9 Loss  0.0641596931964159\n",
      "Average Epoch  8 Loss  0.06335491687059402\n",
      "Average Epoch  7 Loss  0.06259525753557682\n",
      "Average Epoch  6 Loss  0.06240963861346245\n",
      "Average Epoch  5 Loss  0.06196700893342495\n",
      "Average Epoch  4 Loss  0.06205567419528961\n",
      "Average Epoch  3 Loss  0.06167623475193977\n",
      "Average Epoch  2 Loss  0.06178317740559578\n",
      "Average Epoch  1 Loss  0.06152181327342987\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainSiameseNetwork(data,siamese,sess,100,10,three_hash_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InihtcorSzMn"
   },
   "outputs": [],
   "source": [
    "trainQues = data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KGJe9ZPRq7Rj",
    "outputId": "3b84ad58-ae55-463c-e5c9-560579f4a9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a question what is a stork\n"
     ]
    }
   ],
   "source": [
    "ques = input(\"Enter a question \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17017
    },
    "colab_type": "code",
    "id": "kBLV9NhirZ5u",
    "outputId": "cc9020ce-6c46-4ae5-c426-c58f012c4e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.065969646] 0.0\n",
      "[0.07350582] 0.21081851067789195\n",
      "[0.06668651] 0.0\n",
      "[0.10774422] 0.5222329678670935\n",
      "[0.09777391] 0.3651483716701107\n",
      "[0.04108715] 0.24806946917841693\n",
      "[0.057490468] 0.0\n",
      "[0.06376827] 0.0\n",
      "[0.06710744] 0.29814239699997197\n",
      "[0.058106244] 0.24806946917841693\n",
      "[0.09434676] 0.4472135954999579\n",
      "[0.08108848] 0.0\n",
      "[0.04227364] 0.0\n",
      "[0.07564771] 0.0\n",
      "[0.070854545] 0.0\n",
      "[0.05643195] 0.26967994498529685\n",
      "[0.07441187] 0.5163977794943223\n",
      "[0.076114535] 0.0\n",
      "[0.049179673] 0.5443310539518174\n",
      "[0.0751034] 0.26967994498529685\n",
      "[0.09462625] 0.26967994498529685\n",
      "[0.060709536] 0.3651483716701107\n",
      "[0.07563734] 0.4364357804719848\n",
      "[0.055894673] 0.21693045781865616\n",
      "[0.05292499] 0.0\n",
      "[0.08568245] 0.4216370213557839\n",
      "[0.039692104] 0.3999999999999999\n",
      "[0.053568065] 0.0\n",
      "[0.06931257] 0.0\n",
      "[0.05138862] 0.28284271247461895\n",
      "[0.07618058] 0.0\n",
      "[0.06558061] 0.0\n",
      "[0.071058095] 0.2581988897471611\n",
      "[0.055615842] 0.0\n",
      "[0.0876829] 0.2581988897471611\n",
      "[0.07597017] 0.0\n",
      "[0.081896484] 0.4364357804719848\n",
      "[0.074851096] 0.3481553119113957\n",
      "[0.06841165] 0.29814239699997197\n",
      "[0.04669863] 0.0\n",
      "[0.067753494] 0.0\n",
      "[0.08315098] 0.48989794855663565\n",
      "[0.06486583] 0.23094010767585027\n",
      "[0.05902964] 0.4472135954999579\n",
      "[0.06981063] 0.4472135954999579\n",
      "[0.08604151] 0.0\n",
      "[0.06853634] 0.0\n",
      "[0.062474787] 0.0\n",
      "[0.098139584] 0.5163977794943223\n",
      "[0.10217404] 0.47140452079103173\n",
      "[0.08452481] 0.0\n",
      "[0.05832714] 0.0\n",
      "[0.06641573] 0.0\n",
      "[0.07625544] 0.0\n",
      "[0.052491903] 0.5443310539518174\n",
      "[0.077017546] 0.0\n",
      "[0.062370002] 0.0\n",
      "[0.04714191] 0.0\n",
      "[0.07480872] 0.0\n",
      "[0.07980287] 0.2581988897471611\n",
      "[0.090745986] 0.35777087639996635\n",
      "[0.056931674] 0.29814239699997197\n",
      "[0.07095969] 0.23094010767585027\n",
      "[0.073010206] 0.0\n",
      "[0.077498436] 0.0\n",
      "[0.05458212] 0.0\n",
      "[0.05275041] 0.0\n",
      "[0.056839585] 0.28284271247461895\n",
      "[0.038961053] 0.0\n",
      "[0.06944412] 0.33806170189140655\n",
      "[0.050156653] 0.0\n",
      "[0.12870705] 0.0\n",
      "[0.09007895] 0.0\n",
      "[0.06954932] 0.2581988897471611\n",
      "[0.076204956] 0.0\n",
      "[0.092069566] 0.0\n",
      "[0.07806921] 0.0\n",
      "[0.04992032] 0.0\n",
      "[0.07183361] 0.0\n",
      "[0.07156837] 0.24806946917841693\n",
      "[0.07361925] 0.4082482904638631\n",
      "[0.07088834] 0.0\n",
      "[0.08329564] 0.0\n",
      "[0.06594485] 0.26967994498529685\n",
      "[0.06690127] 0.0\n",
      "[0.04961288] 0.24806946917841693\n",
      "[0.051830888] 0.0\n",
      "[0.093251586] 0.0\n",
      "[0.054249167] 0.3651483716701107\n",
      "[0.055698633] 0.0\n",
      "[0.081731796] 0.0\n",
      "[0.0721069] 0.0\n",
      "[0.07731056] 0.24806946917841693\n",
      "[0.100420535] 0.26967994498529685\n",
      "[0.09408808] 0.0\n",
      "[0.09249002] 0.0\n",
      "[0.08392739] 0.0\n",
      "[0.084700406] 0.23094010767585027\n",
      "[0.059120595] 0.3651483716701107\n",
      "[0.08268106] 0.23094010767585027\n",
      "[0.0727011] 0.0\n",
      "[0.050673425] 0.0\n",
      "[0.07221496] 0.29814239699997197\n",
      "[0.086604595] 0.26967994498529685\n",
      "[0.065062344] 0.0\n",
      "[0.076592624] 0.3651483716701107\n",
      "[0.076633334] 0.0\n",
      "[0.07541686] 0.0\n",
      "[0.06474525] 0.0\n",
      "[0.04700184] 0.0\n",
      "[0.058535457] 0.0\n",
      "[0.09232092] 0.0\n",
      "[0.057132483] 0.47140452079103173\n",
      "[0.053046525] 0.5443310539518174\n",
      "[0.044850707] 0.33806170189140655\n",
      "[0.08700168] 0.0\n",
      "[0.0765658] 0.0\n",
      "[0.05682051] 0.0\n",
      "[0.06460065] 0.0\n",
      "[0.06489056] 0.0\n",
      "[0.076667786] 0.26967994498529685\n",
      "[0.07606053] 0.0\n",
      "[0.0655458] 0.0\n",
      "[0.03800553] 0.2051956704170308\n",
      "[0.06375635] 0.0\n",
      "[0.06539345] 0.0\n",
      "[0.08479059] 0.0\n",
      "[0.068599105] 0.23094010767585027\n",
      "[0.03650272] 0.24806946917841693\n",
      "[0.11143899] 0.0\n",
      "[0.077301204] 0.0\n",
      "[0.08736336] 0.0\n",
      "[0.06329423] 0.0\n",
      "[0.064273715] 0.29814239699997197\n",
      "[0.10554755] 0.0\n",
      "[0.0795694] 0.2581988897471611\n",
      "[0.082093894] 0.0\n",
      "[0.08412826] 0.0\n",
      "[0.04617077] 0.28284271247461895\n",
      "[0.0612967] 0.0\n",
      "[0.09061682] 0.0\n",
      "[0.071382225] 0.0\n",
      "[0.059434175] 0.0\n",
      "[0.056406736] 0.23904572186687872\n",
      "[0.053012133] 0.0\n",
      "[0.06469315] 0.28284271247461895\n",
      "[0.05712986] 0.0\n",
      "[0.08679342] 0.28284271247461895\n",
      "[0.091689825] 0.23094010767585027\n",
      "[0.07746631] 0.0\n",
      "[0.06496656] 0.4082482904638631\n",
      "[0.0794155] 0.0\n",
      "[0.085476935] 0.19518001458970666\n",
      "[0.062043786] 0.0\n",
      "[0.0585317] 0.0\n",
      "[0.057500422] 0.26967994498529685\n",
      "[0.076939404] 0.26967994498529685\n",
      "[0.07895613] 0.45291081365783836\n",
      "[0.08148301] 0.4714045207910317\n",
      "[0.097798705] 0.17213259316477406\n",
      "[0.05444151] 0.0\n",
      "[0.05475247] 0.0\n",
      "[0.04159969] 0.3999999999999999\n",
      "[0.12159383] 0.19999999999999996\n",
      "[0.09910953] 0.0\n",
      "[0.10509521] 0.22360679774997896\n",
      "[0.06458628] 0.3265986323710904\n",
      "[0.12671453] 0.3265986323710904\n",
      "[0.065986514] 0.29814239699997197\n",
      "[0.07824373] 0.0\n",
      "[0.06894565] 0.19518001458970666\n",
      "[0.07118565] 0.45291081365783836\n",
      "[0.11437607] 0.19069251784911842\n",
      "[0.120310724] 0.23094010767585027\n",
      "[0.062818885] 0.0\n",
      "[0.070278704] 0.0\n",
      "[0.06345719] 0.0\n",
      "[0.054829597] 0.28284271247461895\n",
      "[0.057691336] 0.0\n",
      "[0.041910768] 0.5773502691896258\n",
      "[0.06817329] 0.0\n",
      "[0.07651907] 0.3405026123034995\n",
      "[0.06401062] 0.0\n",
      "[0.08045077] 0.26967994498529685\n",
      "[0.06401098] 0.26967994498529685\n",
      "[0.059023917] 0.0\n",
      "[0.07750791] 0.29814239699997197\n",
      "[0.054296017] 0.33806170189140655\n",
      "[0.069493234] 0.0\n",
      "[0.09478855] 0.0\n",
      "[0.09572983] 0.19069251784911842\n",
      "[0.08300841] 0.0\n",
      "[0.059893608] 0.24806946917841693\n",
      "[0.0617615] 0.0\n",
      "[0.061938107] 0.28284271247461895\n",
      "[0.07279581] 0.0\n",
      "[0.07493669] 0.0\n",
      "[0.08484894] 0.2581988897471611\n",
      "[0.06742656] 0.0\n",
      "[0.07566297] 0.24806946917841693\n",
      "[0.07228279] 0.0\n",
      "[0.0753634] 0.3746343246326776\n",
      "[0.08151102] 0.0\n",
      "[0.07406455] 0.0\n",
      "[0.058962286] 0.0\n",
      "[0.07157272] 0.3651483716701107\n",
      "[0.067142785] 0.0\n",
      "[0.065422654] 0.4338609156373123\n",
      "[0.08035803] 0.0\n",
      "[0.06760299] 0.0\n",
      "[0.08191067] 0.29814239699997197\n",
      "[0.063317] 0.0\n",
      "[0.07504034] 0.28284271247461895\n",
      "[0.0752933] 0.3999999999999999\n",
      "[0.08816582] 0.0\n",
      "[0.08396983] 0.3849001794597506\n",
      "[0.10253805] 0.21081851067789195\n",
      "[0.10295349] 0.4216370213557839\n",
      "[0.0664531] 0.31622776601683794\n",
      "[0.073210895] 0.26967994498529685\n",
      "[0.060067773] 0.0\n",
      "[0.06507641] 0.24806946917841693\n",
      "[0.07199901] 0.0\n",
      "[0.06326097] 0.0\n",
      "[0.04480046] 0.0\n",
      "[0.054799914] 0.0\n",
      "[0.047805488] 0.0\n",
      "[0.0697605] 0.0\n",
      "[0.068309724] 0.0\n",
      "[0.0562917] 0.0\n",
      "[0.07308632] 0.0\n",
      "[0.07661998] 0.0\n",
      "[0.0654788] 0.28284271247461895\n",
      "[0.07266772] 0.0\n",
      "[0.066345334] 0.0\n",
      "[0.04916519] 0.28284271247461895\n",
      "[0.067415714] 0.0\n",
      "[0.10319692] 0.0\n",
      "[0.07591224] 0.2581988897471611\n",
      "[0.09639704] 0.0\n",
      "[0.06038618] 0.33806170189140655\n",
      "[0.11078751] 0.0\n",
      "[0.062861264] 0.0\n",
      "[0.077352166] 0.0\n",
      "[0.08011329] 0.29814239699997197\n",
      "[0.049369395] 0.29814239699997197\n",
      "[0.05658108] 0.0\n",
      "[0.0508734] 0.0\n",
      "[0.07913613] 0.26967994498529685\n",
      "[0.09476143] 0.23904572186687872\n",
      "[0.07640004] 0.45291081365783836\n",
      "[0.06622684] 0.0\n",
      "[0.055429697] 0.0\n",
      "[0.08440977] 0.0\n",
      "[0.07122505] 0.31622776601683794\n",
      "[0.08234352] 0.0\n",
      "[0.061825752] 0.28284271247461895\n",
      "[0.08608949] 0.0\n",
      "[0.08044183] 0.24806946917841693\n",
      "[0.059664965] 0.0\n",
      "[0.07035172] 0.0\n",
      "[0.08815205] 0.31622776601683794\n",
      "[0.07967585] 0.0\n",
      "[0.06382382] 0.0\n",
      "[0.066486835] 0.0\n",
      "[0.05816239] 0.0\n",
      "[0.06494993] 0.26967994498529685\n",
      "[0.05185783] 0.0\n",
      "[0.062610865] 0.0\n",
      "[0.051780283] 0.0\n",
      "[0.042131186] 0.33806170189140655\n",
      "[0.05978024] 0.0\n",
      "[0.11176109] 0.0\n",
      "[0.100296676] 0.21693045781865616\n",
      "[0.058277786] 0.0\n",
      "[0.060010493] 0.0\n",
      "[0.058344543] 0.0\n",
      "[0.07833403] 0.28284271247461895\n",
      "[0.060773432] 0.0\n",
      "[0.102357924] 0.23904572186687872\n",
      "[0.033414662] 0.0\n",
      "[0.054361045] 0.0\n",
      "[0.08215016] 0.0\n",
      "[0.10787648] 0.3903600291794133\n",
      "[0.061982453] 0.4082482904638631\n",
      "[0.07610637] 0.0\n",
      "[0.065933764] 0.0\n",
      "[0.08208394] 0.21081851067789195\n",
      "[0.06198907] 0.0\n",
      "[0.06949568] 0.4472135954999579\n",
      "[0.05856508] 0.26967994498529685\n",
      "[0.060097456] 0.21693045781865616\n",
      "[0.0711723] 0.2581988897471611\n",
      "[0.05293852] 0.3651483716701107\n",
      "[0.047442615] 0.0\n",
      "[0.089512944] 0.0\n",
      "[0.07502419] 0.26967994498529685\n",
      "[0.087469935] 0.0\n",
      "[0.051558137] 0.0\n",
      "[0.12585878] 0.28284271247461895\n",
      "[0.079833984] 0.31622776601683794\n",
      "[0.048329115] 0.28284271247461895\n",
      "[0.06651378] 0.28284271247461895\n",
      "[0.07637811] 0.0\n",
      "[0.075499475] 0.28284271247461895\n",
      "[0.079357505] 0.2581988897471611\n",
      "[0.07232839] 0.0\n",
      "[0.13090473] 0.0\n",
      "[0.05694276] 0.0\n",
      "[0.04605341] 0.0\n",
      "[0.080815256] 0.47140452079103173\n",
      "[0.060420394] 0.0\n",
      "[0.07908237] 0.0\n",
      "[0.07635051] 0.5345224838248488\n",
      "[0.08007789] 0.28284271247461895\n",
      "[0.09754449] 0.0\n",
      "[0.08671296] 0.26967994498529685\n",
      "[0.07378185] 0.28284271247461895\n",
      "[0.040511966] 0.0\n",
      "[0.079409] 0.31622776601683794\n",
      "[0.05147308] 0.0\n",
      "[0.048725605] 0.29814239699997197\n",
      "[0.08875096] 0.0\n",
      "[0.0804497] 0.0\n",
      "[0.064379215] 0.0\n",
      "[0.07785374] 0.0\n",
      "[0.079354525] 0.0\n",
      "[0.11271858] 0.28284271247461895\n",
      "[0.0914762] 0.4338609156373123\n",
      "[0.08229399] 0.5773502691896258\n",
      "[0.06846124] 0.0\n",
      "[0.077133656] 0.24806946917841693\n",
      "[0.085178256] 0.0\n",
      "[0.08936691] 0.0\n",
      "[0.0868507] 0.0\n",
      "[0.06649798] 0.0\n",
      "[0.078354836] 0.21081851067789195\n",
      "[0.068483055] 0.28284271247461895\n",
      "[0.06529635] 0.0\n",
      "[0.068036914] 0.0\n",
      "[0.08874017] 0.0\n",
      "[0.060813904] 0.0\n",
      "[0.08193296] 0.0\n",
      "[0.09370518] 0.2581988897471611\n",
      "[0.06636387] 0.0\n",
      "[0.07586032] 0.0\n",
      "[0.07189989] 0.0\n",
      "[0.06112957] 0.0\n",
      "[0.07466024] 0.0\n",
      "[0.055671573] 0.0\n",
      "[0.073164225] 0.0\n",
      "[0.07797402] 0.2581988897471611\n",
      "[0.063340366] 0.2581988897471611\n",
      "[0.09884304] 0.2581988897471611\n",
      "[0.05549395] 0.0\n",
      "[0.085276484] 0.0\n",
      "[0.0935123] 0.21693045781865616\n",
      "[0.059984684] 0.3651483716701107\n",
      "[0.054504275] 0.28284271247461895\n",
      "[0.0712474] 0.26967994498529685\n",
      "[0.087890565] 0.28284271247461895\n",
      "[0.039036393] 0.0\n",
      "[0.06884068] 0.26967994498529685\n",
      "[0.0771597] 0.0\n",
      "[0.0575881] 0.0\n",
      "[0.04627353] 0.0\n",
      "[0.06667429] 0.0\n",
      "[0.08088988] 0.3849001794597506\n",
      "[0.07642466] 0.19069251784911842\n",
      "[0.06428248] 0.0\n",
      "[0.08615011] 0.0\n",
      "[0.070379496] 0.0\n",
      "[0.060889482] 0.0\n",
      "[0.086791515] 0.29814239699997197\n",
      "[0.049346805] 0.47140452079103173\n",
      "[0.07256007] 0.0\n",
      "[0.075276315] 0.0\n",
      "[0.09179914] 0.0\n",
      "[0.06323475] 0.0\n",
      "[0.10295433] 0.0\n",
      "[0.08682966] 0.0\n",
      "[0.107150674] 0.29814239699997197\n",
      "[0.07289314] 0.31622776601683794\n",
      "[0.047666073] 0.0\n",
      "[0.061924934] 0.0\n",
      "[0.07147294] 0.31622776601683794\n",
      "[0.118002236] 0.21081851067789195\n",
      "[0.06465989] 0.0\n",
      "[0.09672922] 0.0\n",
      "[0.056434214] 0.0\n",
      "[0.1121825] 0.23904572186687872\n",
      "[0.078813314] 0.29814239699997197\n",
      "[0.07221073] 0.33806170189140655\n",
      "[0.042819083] 0.31622776601683794\n",
      "[0.07945776] 0.26967994498529685\n",
      "[0.07146245] 0.24806946917841693\n",
      "[0.06722635] 0.0\n",
      "[0.05926335] 0.3651483716701107\n",
      "[0.049839497] 0.0\n",
      "[0.067480505] 0.0\n",
      "[0.05033779] 0.0\n",
      "[0.059366345] 0.5163977794943223\n",
      "[0.06812924] 0.29814239699997197\n",
      "[0.0694257] 0.5614899250748772\n",
      "[0.06623697] 0.3651483716701107\n",
      "[0.06716663] 0.5000000000000001\n",
      "[0.08902556] 0.24806946917841693\n",
      "[0.087106526] 0.3730019232961255\n",
      "[0.120916486] 0.0\n",
      "[0.06539053] 0.0\n",
      "[0.04546851] 0.0\n",
      "[0.087916076] 0.0\n",
      "[0.07521433] 0.24806946917841693\n",
      "[0.055936098] 0.0\n",
      "[0.080574274] 0.19999999999999996\n",
      "[0.059993386] 0.18650096164806276\n",
      "[0.056971133] 0.28284271247461895\n",
      "[0.058482945] 0.4082482904638631\n",
      "[0.07321477] 0.24806946917841693\n",
      "[0.07447976] 0.0\n",
      "[0.092689574] 0.18257418583505536\n",
      "[0.09138167] 0.2581988897471611\n",
      "[0.08157855] 0.2581988897471611\n",
      "[0.09657693] 0.0\n",
      "[0.08360314] 0.0\n",
      "[0.06840587] 0.0\n",
      "[0.06184125] 0.0\n",
      "[0.07587087] 0.23904572186687872\n",
      "[0.040278196] 0.2581988897471611\n",
      "[0.05686009] 0.0\n",
      "[0.04620737] 0.45291081365783836\n",
      "[0.07247847] 0.0\n",
      "[0.06630319] 0.0\n",
      "[0.082496285] 0.24806946917841693\n",
      "[0.05682689] 0.24806946917841693\n",
      "[0.103494644] 0.18257418583505536\n",
      "[0.08533126] 0.0\n",
      "[0.06955826] 0.21081851067789195\n",
      "[0.07246941] 0.26967994498529685\n",
      "[0.09398812] 0.21081851067789195\n",
      "[0.0863058] 0.0\n",
      "[0.051558018] 0.0\n",
      "[0.14015639] 0.0\n",
      "[0.069123805] 0.0\n",
      "[0.07483435] 0.0\n",
      "[0.08672953] 0.0\n",
      "[0.09596014] 0.0\n",
      "[0.0854457] 0.3086066999241838\n",
      "[0.08571267] 0.0\n",
      "[0.08837438] 0.23094010767585027\n",
      "[0.09962511] 0.21693045781865616\n",
      "[0.06601131] 0.0\n",
      "[0.07790238] 0.0\n",
      "[0.061847568] 0.28284271247461895\n",
      "[0.049194038] 0.3651483716701107\n",
      "[0.10400182] 0.0\n",
      "[0.11469507] 0.0\n",
      "[0.06687945] 0.26967994498529685\n",
      "[0.079129875] 0.0\n",
      "[0.0801363] 0.0\n",
      "[0.093936265] 0.0\n",
      "[0.07654685] 0.26967994498529685\n",
      "[0.093936265] 0.0\n",
      "[0.07568759] 0.0\n",
      "[0.12285143] 0.17888543819998318\n",
      "[0.061228096] 0.0\n",
      "[0.07227397] 0.3849001794597506\n",
      "[0.06611705] 0.19518001458970666\n",
      "[0.082258165] 0.24806946917841693\n",
      "[0.04255706] 0.0\n",
      "[0.08639026] 0.22360679774997896\n",
      "[0.067496] 0.29814239699997197\n",
      "[0.10210937] 0.0\n",
      "[0.080839634] 0.2581988897471611\n",
      "[0.05856991] 0.0\n",
      "[0.059993625] 0.0\n",
      "[0.07205224] 0.26967994498529685\n",
      "[0.06514233] 0.0\n",
      "[0.05358827] 0.0\n",
      "[0.05538869] 0.0\n",
      "[0.046999097] 0.0\n",
      "[0.06016749] 0.32128773156099955\n",
      "[0.055966258] 0.31622776601683794\n",
      "[0.06335592] 0.23904572186687872\n",
      "[0.061932087] 0.0\n",
      "[0.0745523] 0.2581988897471611\n",
      "[0.079708695] 0.47140452079103173\n",
      "[0.07002646] 0.0\n",
      "[0.090257525] 0.0\n",
      "[0.101713955] 0.0\n",
      "[0.07324976] 0.26967994498529685\n",
      "[0.052972615] 0.31622776601683794\n",
      "[0.099852145] 0.31622776601683794\n",
      "[0.08667022] 0.3903600291794133\n",
      "[0.072713315] 0.29814239699997197\n",
      "[0.06683332] 0.18650096164806276\n",
      "[0.07436746] 0.0\n",
      "[0.07442337] 0.0\n",
      "[0.0709576] 0.0\n",
      "[0.053501964] 0.23904572186687872\n",
      "[0.08531588] 0.31622776601683794\n",
      "[0.07172781] 0.18257418583505536\n",
      "[0.08146471] 0.0\n",
      "[0.05525756] 0.4364357804719847\n",
      "[0.07812208] 0.24806946917841693\n",
      "[0.066633046] 0.0\n",
      "[0.09433395] 0.0\n",
      "[0.063005984] 0.0\n",
      "[0.07071257] 0.2581988897471611\n",
      "[0.068519235] 0.24806946917841693\n",
      "[0.057385087] 0.0\n",
      "[0.07893515] 0.0\n",
      "[0.0929001] 0.23904572186687872\n",
      "[0.08609116] 0.4216370213557839\n",
      "[0.055871367] 0.6172133998483676\n",
      "[0.0916515] 0.24806946917841693\n",
      "[0.06586844] 0.0\n",
      "[0.0865224] 0.0\n",
      "[0.06101328] 0.23904572186687872\n",
      "[0.08513421] 0.0\n",
      "[0.07152045] 0.28284271247461895\n",
      "[0.06437951] 0.0\n",
      "[0.07264799] 0.0\n",
      "[0.075068] 0.23904572186687872\n",
      "[0.067594886] 0.0\n",
      "[0.09012079] 0.0\n",
      "[0.06381822] 0.0\n",
      "[0.101662874] 0.0\n",
      "[0.107557654] 0.0\n",
      "[0.056849837] 0.0\n",
      "[0.07392216] 0.0\n",
      "[0.056494176] 0.3999999999999999\n",
      "[0.069791436] 0.4338609156373123\n",
      "[0.07500702] 0.0\n",
      "[0.070841074] 0.0\n",
      "[0.088068604] 0.0\n",
      "[0.07055199] 0.47140452079103173\n",
      "[0.074710846] 0.19999999999999996\n",
      "[0.07930273] 0.0\n",
      "[0.07233077] 0.0\n",
      "[0.057489634] 0.0\n",
      "[0.087317705] 0.0\n",
      "[0.062835634] 0.19518001458970666\n",
      "[0.06580627] 0.0\n",
      "[0.08033347] 0.0\n",
      "[0.092846096] 0.31622776601683794\n",
      "[0.066624105] 0.492365963917331\n",
      "[0.0643813] 0.19518001458970666\n",
      "[0.057020247] 0.0\n",
      "[0.05276984] 0.24806946917841693\n",
      "[0.065157235] 0.0\n",
      "[0.072087765] 0.47140452079103173\n",
      "[0.08247799] 0.0\n",
      "[0.09476262] 0.38138503569823684\n",
      "[0.07698411] 0.24806946917841693\n",
      "[0.11867833] 0.0\n",
      "[0.05982375] 0.0\n",
      "[0.07122159] 0.0\n",
      "[0.09250015] 0.0\n",
      "[0.08954656] 0.0\n",
      "[0.117673516] 0.0\n",
      "[0.09375495] 0.0\n",
      "[0.07894957] 0.3730019232961255\n",
      "[0.086414814] 0.0\n",
      "[0.07771164] 0.29814239699997197\n",
      "[0.048592567] 0.3651483716701107\n",
      "[0.046503186] 0.0\n",
      "[0.14106083] 0.0\n",
      "[0.1091972] 0.0\n",
      "[0.047964156] 0.0\n",
      "[0.10997665] 0.39605901719066977\n",
      "[0.05733186] 0.23094010767585027\n",
      "[0.09066635] 0.0\n",
      "[0.08274883] 0.0\n",
      "[0.043313682] 0.0\n",
      "[0.07265127] 0.28284271247461895\n",
      "[0.06940436] 0.28284271247461895\n",
      "[0.113667905] 0.17888543819998318\n",
      "[0.087940216] 0.21693045781865616\n",
      "[0.12942469] 0.0\n",
      "[0.08526468] 0.0\n",
      "[0.06541443] 0.21693045781865616\n",
      "[0.04548073] 0.5443310539518174\n",
      "[0.103705406] 0.0\n",
      "[0.07217163] 0.0\n",
      "[0.075235665] 0.0\n",
      "[0.0868848] 0.0\n",
      "[0.09398341] 0.0\n",
      "[0.055675864] 0.0\n",
      "[0.047884345] 0.0\n",
      "[0.05929339] 0.23904572186687872\n",
      "[0.060380757] 0.0\n",
      "[0.06717378] 0.19999999999999996\n",
      "[0.09162408] 0.0\n",
      "[0.069545686] 0.23904572186687872\n",
      "[0.071816444] 0.0\n",
      "[0.07552439] 0.0\n",
      "[0.087706745] 0.3730019232961255\n",
      "[0.067895174] 0.0\n",
      "[0.047245204] 0.0\n",
      "[0.047488928] 0.0\n",
      "[0.16375172] 0.29019050004400465\n",
      "[0.06498605] 0.0\n",
      "[0.055307627] 0.0\n",
      "[0.080627084] 0.0\n",
      "[0.087622225] 0.0\n",
      "[0.060436547] 0.0\n",
      "[0.076099336] 0.0\n",
      "[0.092906535] 0.0\n",
      "[0.050673127] 0.22360679774997896\n",
      "[0.04964608] 0.29814239699997197\n",
      "[0.053537667] 0.0\n",
      "[0.051947594] 0.0\n",
      "[0.09281182] 0.0\n",
      "[0.045288444] 0.29814239699997197\n",
      "[0.080886304] 0.0\n",
      "[0.06264627] 0.0\n",
      "[0.06511909] 0.0\n",
      "[0.055225074] 0.0\n",
      "[0.07010102] 0.23904572186687872\n",
      "[0.07115471] 0.0\n",
      "[0.09774417] 0.0\n",
      "[0.06589389] 0.0\n",
      "[0.079806805] 0.0\n",
      "[0.09305787] 0.0\n",
      "[0.048896432] 0.0\n",
      "[0.056574762] 0.3903600291794133\n",
      "[0.08653867] 0.0\n",
      "[0.09555143] 0.0\n",
      "[0.050725818] 0.0\n",
      "[0.051710546] 0.0\n",
      "[0.11134517] 0.0\n",
      "[0.06664789] 0.0\n",
      "[0.05787033] 0.0\n",
      "[0.08091074] 0.0\n",
      "[0.06649065] 0.0\n",
      "[0.07412696] 0.33806170189140655\n",
      "[0.08773315] 0.0\n",
      "[0.080902755] 0.0\n",
      "[0.072227836] 0.447213595499958\n",
      "[0.060231745] 0.0\n",
      "[0.059617102] 0.23904572186687872\n",
      "[0.06235808] 0.0\n",
      "[0.06818068] 0.0\n",
      "[0.05591339] 0.0\n",
      "[0.060004175] 0.4216370213557839\n",
      "[0.07041615] 0.0\n",
      "[0.061842322] 0.0\n",
      "[0.0787375] 0.5619514869490164\n",
      "[0.060979843] 0.2581988897471611\n",
      "[0.07558644] 0.0\n",
      "[0.1009084] 0.0\n",
      "[0.05099356] 0.0\n",
      "[0.05110556] 0.0\n",
      "[0.087462604] 0.0\n",
      "[0.05069357] 0.0\n",
      "[0.09516287] 0.24806946917841693\n",
      "[0.11054289] 0.28284271247461895\n",
      "[0.09609884] 0.0\n",
      "[0.08876473] 0.0\n",
      "[0.063713014] 0.22360679774997896\n",
      "[0.07606673] 0.2581988897471611\n",
      "[0.07628906] 0.24806946917841693\n",
      "[0.07830274] 0.23904572186687872\n",
      "[0.08192402] 0.0\n",
      "[0.08003646] 0.0\n",
      "[0.06224245] 0.0\n",
      "[0.08609933] 0.26967994498529685\n",
      "[0.087801576] 0.0\n",
      "[0.052825868] 0.0\n",
      "[0.08671838] 0.0\n",
      "[0.07853341] 0.0\n",
      "[0.085343] 0.0\n",
      "[0.052461267] 0.45291081365783836\n",
      "[0.08117205] 0.0\n",
      "[0.054611802] 0.0\n",
      "[0.06522763] 0.24806946917841693\n",
      "[0.058184624] 0.0\n",
      "[0.082731485] 0.29814239699997197\n",
      "[0.066812634] 0.0\n",
      "[0.06355506] 0.0\n",
      "[0.076429605] 0.4472135954999579\n",
      "[0.07804352] 0.0\n",
      "[0.06663787] 0.29814239699997197\n",
      "[0.07185596] 0.26967994498529685\n",
      "[0.052158237] 0.4364357804719848\n",
      "[0.106307864] 0.17888543819998318\n",
      "[0.07494801] 0.0\n",
      "[0.08460927] 0.0\n",
      "[0.10652983] 0.0\n",
      "[0.0618726] 0.0\n",
      "[0.08849996] 0.0\n",
      "[0.07598251] 0.0\n",
      "[0.057435334] 0.0\n",
      "[0.0860101] 0.26967994498529685\n",
      "[0.08064169] 0.0\n",
      "[0.08106506] 0.0\n",
      "[0.08496916] 0.23904572186687872\n",
      "[0.06124729] 0.5669467095138409\n",
      "[0.0782423] 0.0\n",
      "[0.049679577] 0.23904572186687872\n",
      "[0.067242324] 0.31622776601683794\n",
      "[0.07109493] 0.33806170189140655\n",
      "[0.030085683] 0.3999999999999999\n",
      "[0.0862748] 0.2581988897471611\n",
      "[0.068511605] 0.3651483716701107\n",
      "[0.043434203] 0.0\n",
      "[0.071291625] 0.4364357804719848\n",
      "[0.03911656] 0.0\n",
      "[0.06724626] 0.5163977794943223\n",
      "[0.1037482] 0.21693045781865616\n",
      "[0.08261663] 0.0\n",
      "[0.08158535] 0.29814239699997197\n",
      "[0.0494892] 0.33806170189140655\n",
      "[0.09483248] 0.18650096164806276\n",
      "[0.07273632] 0.0\n",
      "[0.09799516] 0.0\n",
      "[0.034032762] 0.3999999999999999\n",
      "[0.040287375] 0.24806946917841693\n",
      "[0.0657835] 0.28284271247461895\n",
      "[0.07053131] 0.0\n",
      "[0.07034451] 0.2581988897471611\n",
      "[0.09291965] 0.6837634587578276\n",
      "[0.06252575] 0.492365963917331\n",
      "[0.07858616] 0.0\n",
      "[0.04544097] 0.0\n",
      "[0.09924561] 0.2581988897471611\n",
      "[0.0924288] 0.24806946917841693\n",
      "[0.07323831] 0.0\n",
      "[0.06732911] 0.0\n",
      "[0.07833123] 0.2581988897471611\n",
      "[0.0635705] 0.29814239699997197\n",
      "[0.09535968] 0.18257418583505536\n",
      "[0.054793417] 0.0\n",
      "[0.06157893] 0.0\n",
      "[0.0688082] 0.4338609156373123\n",
      "[0.060842097] 0.2581988897471611\n",
      "[0.07788688] 0.0\n",
      "[0.050700903] 0.0\n",
      "[0.08002025] 0.0\n",
      "[0.056429327] 0.0\n",
      "[0.054443717] 0.0\n",
      "[0.09362757] 0.0\n",
      "[0.06710249] 0.0\n",
      "[0.083663225] 0.0\n",
      "[0.073995054] 0.0\n",
      "[0.06265664] 0.26967994498529685\n",
      "[0.06743854] 0.4507489358552088\n",
      "[0.057308853] 0.0\n",
      "[0.0788787] 0.0\n",
      "[0.0993526] 0.0\n",
      "[0.077166915] 0.29814239699997197\n",
      "[0.06563312] 0.0\n",
      "[0.09929901] 0.0\n",
      "[0.058360636] 0.19069251784911842\n",
      "[0.0837661] 0.21081851067789195\n",
      "[0.074736774] 0.29814239699997197\n",
      "[0.06847113] 0.28284271247461895\n",
      "[0.07993269] 0.26967994498529685\n",
      "[0.08647835] 0.0\n",
      "[0.07051921] 0.33806170189140655\n",
      "[0.067350805] 0.492365963917331\n",
      "[0.055786967] 0.4472135954999579\n",
      "[0.10408932] 0.5222329678670935\n",
      "[0.079414725] 0.4216370213557839\n",
      "[0.112044096] 0.0\n",
      "[0.06611782] 0.3481553119113957\n",
      "[0.06947082] 0.0\n",
      "[0.12537497] 0.18257418583505536\n",
      "[0.08436108] 0.24806946917841693\n",
      "[0.08067733] 0.4216370213557839\n",
      "[0.060540557] 0.0\n",
      "[0.06310481] 0.3730019232961255\n",
      "[0.13417202] 0.18257418583505536\n",
      "[0.05037868] 0.33806170189140655\n",
      "[0.073955595] 0.31622776601683794\n",
      "[0.052506924] 0.0\n",
      "[0.084288895] 0.17888543819998318\n",
      "[0.06362206] 0.0\n",
      "[0.061355054] 0.26967994498529685\n",
      "[0.10286343] 0.18257418583505536\n",
      "[0.061185837] 0.0\n",
      "[0.06559956] 0.23904572186687872\n",
      "[0.066770375] 0.0\n",
      "[0.06328225] 0.29814239699997197\n",
      "[0.040666163] 0.5163977794943223\n",
      "[0.080409825] 0.4803844614152615\n",
      "[0.054304957] 0.0\n",
      "[0.052019] 0.0\n",
      "[0.10766059] 0.3651483716701107\n",
      "[0.076377034] 0.3651483716701107\n",
      "[0.11229229] 0.4548588261473421\n",
      "[0.07948786] 0.47140452079103173\n",
      "[0.059073806] 0.26967994498529685\n",
      "[0.09805781] 0.3730019232961255\n",
      "[0.043268263] 0.3651483716701107\n",
      "[0.049259245] 0.0\n",
      "[0.058175564] 0.0\n",
      "[0.09208739] 0.0\n",
      "[0.07088894] 0.26967994498529685\n",
      "[0.070631266] 0.3999999999999999\n",
      "[0.056295753] 0.0\n",
      "[0.037373185] 0.6172133998483676\n",
      "[0.07155579] 0.0\n",
      "[0.06507558] 0.17888543819998318\n",
      "[0.07967943] 0.0\n",
      "[0.07823306] 0.4103913408340616\n",
      "[0.07949263] 0.22360679774997896\n",
      "[0.05980438] 0.0\n",
      "[0.05156201] 0.0\n",
      "[0.076975584] 0.0\n",
      "[0.059159577] 0.28284271247461895\n",
      "[0.05715096] 0.0\n",
      "[0.06184274] 0.0\n",
      "[0.07787377] 0.0\n",
      "[0.07338989] 0.0\n",
      "[0.07990587] 0.0\n",
      "[0.056700528] 0.0\n",
      "[0.066758215] 0.0\n",
      "[0.07571226] 0.0\n",
      "[0.06565976] 0.29814239699997197\n",
      "[0.08475959] 0.19069251784911842\n",
      "[0.06747496] 0.23904572186687872\n",
      "[0.068264246] 0.0\n",
      "[0.10019636] 0.0\n",
      "[0.07148993] 0.47140452079103173\n",
      "[0.048010647] 0.0\n",
      "[0.06813818] 0.21693045781865616\n",
      "[0.06817466] 0.23904572186687872\n",
      "[0.07936543] 0.2581988897471611\n",
      "[0.06648594] 0.0\n",
      "[0.11385453] 0.16903085094570328\n",
      "[0.067174435] 0.0\n",
      "[0.05608833] 0.0\n",
      "[0.07085794] 0.2581988897471611\n",
      "[0.10675573] 0.31622776601683794\n",
      "[0.067592025] 0.0\n",
      "[0.07040185] 0.0\n",
      "[0.07490456] 0.0\n",
      "[0.06920481] 0.28284271247461895\n",
      "[0.07244611] 0.0\n",
      "[0.1085521] 0.38138503569823684\n",
      "[0.06901419] 0.0\n",
      "[0.08494723] 0.17541160386140586\n",
      "[0.074148774] 0.0\n",
      "[0.08513999] 0.24806946917841693\n",
      "[0.06794673] 0.4338609156373123\n",
      "[0.09916836] 0.0\n",
      "[0.100825965] 0.0\n",
      "[0.054769278] 0.0\n",
      "[0.09237021] 0.0\n",
      "[0.09512597] 0.0\n",
      "[0.050647795] 0.5855400437691198\n",
      "[0.07832229] 0.21693045781865616\n",
      "[0.053270638] 0.31622776601683794\n",
      "[0.06481397] 0.26967994498529685\n",
      "[0.048854172] 0.492365963917331\n",
      "[0.07702458] 0.0\n",
      "[0.06666565] 0.0\n",
      "[0.083042026] 0.0\n",
      "[0.048692346] 0.47140452079103173\n",
      "[0.08668107] 0.0\n",
      "[0.05386144] 0.0\n",
      "[0.08350295] 0.0\n",
      "[0.05602163] 0.0\n",
      "[0.094430566] 0.2051956704170308\n",
      "[0.05296141] 0.31622776601683794\n",
      "[0.05793798] 0.17888543819998318\n",
      "[0.05376941] 0.0\n",
      "[0.07479495] 0.0\n",
      "[0.08544427] 0.38138503569823684\n",
      "[0.07238662] 0.0\n",
      "[0.049179673] 0.31622776601683794\n",
      "[0.069658756] 0.45291081365783836\n",
      "[0.053171217] 0.3903600291794133\n",
      "[0.086110175] 0.0\n",
      "[0.05353123] 0.29814239699997197\n",
      "[0.10441369] 0.0\n",
      "[0.09083778] 0.0\n",
      "[0.06077242] 0.0\n",
      "[0.06678003] 0.0\n",
      "[0.07280767] 0.0\n",
      "[0.053034186] 0.3651483716701107\n",
      "[0.086235106] 0.23094010767585027\n",
      "[0.075294614] 0.0\n",
      "[0.06772834] 0.0\n",
      "[0.044267833] 0.0\n",
      "[0.15288967] 0.3849001794597506\n",
      "[0.06832147] 0.45291081365783836\n",
      "[0.067433774] 0.0\n",
      "[0.08801609] 0.22360679774997896\n",
      "[0.103375256] 0.33806170189140655\n",
      "[0.07098591] 0.0\n",
      "[0.056321144] 0.29814239699997197\n",
      "[0.06433523] 0.24806946917841693\n",
      "[0.070084095] 0.0\n",
      "[0.056314707] 0.0\n",
      "[0.080256164] 0.0\n",
      "[0.091784] 0.1632993161855452\n",
      "[0.06753653] 0.22360679774997896\n",
      "[0.07756698] 0.24806946917841693\n",
      "[0.09332055] 0.0\n",
      "[0.075627804] 0.22360679774997896\n",
      "[0.060473382] 0.0\n",
      "[0.07637292] 0.0\n",
      "[0.083002806] 0.23904572186687872\n",
      "[0.07631785] 0.0\n",
      "[0.079829514] 0.0\n",
      "[0.07595134] 0.0\n",
      "[0.0842644] 0.0\n",
      "[0.057399273] 0.0\n",
      "[0.06574464] 0.0\n",
      "[0.08013606] 0.3999999999999999\n",
      "[0.07278216] 0.45291081365783836\n",
      "[0.078437686] 0.18257418583505536\n",
      "[0.050930083] 0.3999999999999999\n",
      "[0.05593133] 0.0\n",
      "[0.08739406] 0.0\n",
      "[0.07991576] 0.0\n",
      "[0.093958676] 0.0\n",
      "[0.07702458] 0.0\n",
      "[0.06246859] 0.31622776601683794\n",
      "[0.07713574] 0.24806946917841693\n",
      "[0.059375048] 0.4082482904638631\n",
      "[0.06461775] 0.29814239699997197\n",
      "[0.08838242] 0.0\n",
      "[0.045413315] 0.0\n",
      "[0.09750587] 0.0\n",
      "[0.059362233] 0.0\n",
      "[0.06966859] 0.0\n",
      "[0.062438667] 0.0\n",
      "[0.073991895] 0.23904572186687872\n",
      "[0.09126651] 0.29814239699997197\n",
      "[0.06437302] 0.0\n",
      "[0.075396955] 0.31622776601683794\n",
      "[0.069081604] 0.0\n",
      "[0.08368009] 0.0\n",
      "[0.068288684] 0.0\n",
      "[0.06541246] 0.0\n",
      "[0.07131857] 0.46188021535170054\n",
      "[0.058402836] 0.0\n",
      "[0.06259924] 0.19999999999999996\n",
      "[0.051952183] 0.0\n",
      "[0.085221946] 0.22360679774997896\n",
      "[0.12755716] 0.45291081365783836\n",
      "[0.08824134] 0.0\n",
      "[0.07076663] 0.0\n",
      "[0.07139403] 0.46188021535170054\n",
      "[0.07809061] 0.0\n",
      "[0.08374977] 0.21693045781865616\n",
      "[0.077963054] 0.24806946917841693\n",
      "[0.07678503] 0.33806170189140655\n",
      "[0.07511598] 0.4364357804719848\n",
      "[0.063084126] 0.4364357804719848\n",
      "[0.09326869] 0.0\n",
      "[0.06531799] 0.0\n",
      "[0.055905342] 0.0\n",
      "[0.086248815] 0.24806946917841693\n",
      "[0.09420478] 0.29814239699997197\n",
      "[0.06517488] 0.0\n",
      "[0.05951065] 0.28284271247461895\n",
      "[0.0600639] 0.0\n",
      "[0.07124603] 0.0\n",
      "[0.099846125] 0.3849001794597506\n",
      "[0.08230728] 0.0\n",
      "[0.05773163] 0.0\n",
      "[0.10051781] 0.0\n",
      "[0.09494156] 0.3405026123034995\n",
      "[0.073586404] 0.3746343246326776\n",
      "[0.0819518] 0.19999999999999996\n",
      "[0.055999458] 0.0\n",
      "[0.039931536] 0.0\n",
      "[0.08607024] 0.0\n",
      "[0.06366438] 0.0\n",
      "[0.063275814] 0.0\n",
      "[0.092342496] 0.0\n",
      "[0.08498472] 0.22360679774997896\n",
      "[0.073052526] 0.5163977794943223\n",
      "[0.099948645] 0.0\n",
      "[0.090703666] 0.28284271247461895\n",
      "[0.06568706] 0.17888543819998318\n",
      "[0.07210624] 0.0\n",
      "[0.05796343] 0.0\n",
      "[0.064914584] 0.0\n",
      "[0.058905482] 0.45291081365783836\n",
      "[0.0844093] 0.0\n",
      "[0.06389731] 0.0\n",
      "[0.08755088] 0.21081851067789195\n",
      "[0.061650753] 0.33806170189140655\n",
      "[0.05406016] 0.3651483716701107\n",
      "[0.07043487] 0.2581988897471611\n",
      "[0.07624346] 0.0\n",
      "[0.061085463] 0.0\n",
      "[0.06967169] 0.45291081365783836\n",
      "[0.06543845] 0.0\n",
      "[0.056022882] 0.23904572186687872\n",
      "[0.062038124] 0.0\n",
      "[0.049786508] 0.3730019232961255\n",
      "[0.11557567] 0.23094010767585027\n",
      "[0.08133745] 0.2051956704170308\n"
     ]
    }
   ],
   "source": [
    "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HRw_LzLeiSQ3",
    "outputId": "12452166-294d-436d-fc48-66ec07829b8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is your favorite quotation from Henry David Thoreau ?'"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainQues[310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "HH-Mq9yrhtSj",
    "outputId": "e3341554-6a34-4f94-90f9-b987202f17a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['what is a double layer DVD burner?', 'What is a blog?',\n",
      "       'What is a motherboard?'], dtype=object), [0.906596964597702, 0.7176139222742814, 0.9066686511039734, 0.4407647506115104, 0.5811438564537791, 0.6808461927967979, 0.9057490468025208, 0.9063768267631531, 0.638382586604139, 0.682548102100463, 0.506942440220387, 0.9081088483333588, 0.904227364063263, 0.9075647711753846, 0.9070854544639587, 0.6629312444270969, 0.44268318536001716, 0.9076114535331726, 0.4150200187628529, 0.6647983897270084, 0.6667506743015171, 0.5774374191044596, 0.5147715316297792, 0.710352055250273, 0.9052924990653992, 0.5290949259522952, 0.5439692103862764, 0.9053568065166474, 0.9069312572479248, 0.6505804209058692, 0.9076180577278138, 0.9065580606460572, 0.6747268086777045, 0.9055615842342377, 0.6763892895088508, 0.9075970172882081, 0.51539744596503, 0.5941453288473861, 0.6385130075273721, 0.904669862985611, 0.9067753493785858, 0.46740694458470294, 0.6986404858477773, 0.5034107279267622, 0.5044888269391371, 0.9086041510105133, 0.9068536341190339, 0.9062474787235261, 0.4450559568615583, 0.48595333565361093, 0.9084524810314178, 0.905832713842392, 0.9066415727138519, 0.9076255440711976, 0.41535124177386973, 0.9077017545700073, 0.9062370002269745, 0.9047141909599304, 0.9074808716773987, 0.6756012863025977, 0.5870808097908269, 0.6373650101480691, 0.6992498718150318, 0.9073010206222535, 0.9077498435974122, 0.9054582118988037, 0.9052750408649445, 0.6511255173032691, 0.9038961052894593, 0.602688880290761, 0.9050156652927399, 0.9128707051277161, 0.9090078949928284, 0.6745759314403846, 0.907620495557785, 0.9092069566249847, 0.9078069210052491, 0.9049920320510865, 0.9071833610534669, 0.6838943147259666, 0.5399384632308081, 0.9070888340473175, 0.9083295643329621, 0.6638825345577122, 0.9066901266574859, 0.6816987657147361, 0.9051830887794495, 0.9093251585960388, 0.5767913822411326, 0.905569863319397, 0.9081731796264648, 0.9072106897830964, 0.6844685339528037, 0.6673301029743076, 0.9094088077545166, 0.9092490017414093, 0.9083927392959595, 0.7006239436515034, 0.5772785250424174, 0.7004220090754689, 0.9072701096534729, 0.9050673425197602, 0.6388933388052194, 0.6659485090316655, 0.906506234407425, 0.5790257279156473, 0.9076633334159852, 0.907541686296463, 0.9064745247364044, 0.9047001838684082, 0.905853545665741, 0.9092320919036866, 0.4814491795409401, 0.41540670389583323, 0.600229539003148, 0.908700168132782, 0.9076565802097321, 0.9056820511817932, 0.906460064649582, 0.9064890563488007, 0.6649548280776859, 0.907606053352356, 0.9065545797348022, 0.7191244497080921, 0.9063756346702576, 0.9065393447875977, 0.9084790587425232, 0.6990138135798634, 0.6803877496319725, 0.9111438989639282, 0.907730120420456, 0.9087363362312317, 0.9063294231891632, 0.6380992142019479, 0.9105547547340394, 0.6755779391632393, 0.9082093894481659, 0.9084128260612488, 0.6500586358850409, 0.9061296701431275, 0.9090616822242737, 0.9071382224559784, 0.9059434175491333, 0.6904995239571994, 0.9053012132644653, 0.6519108740632941, 0.9057129859924317, 0.6541209010427405, 0.7013228855975331, 0.9077466309070588, 0.5390731945235328, 0.9079415500164032, 0.732885680360246, 0.9062043786048889, 0.9058531701564789, 0.6630380917133213, 0.6649819899143101, 0.5002758804246202, 0.4838842319358071, 0.7548605366618047, 0.9054441511631012, 0.9054752469062806, 0.5441599690914155, 0.7321593832969666, 0.9099109530448913, 0.7092634027941382, 0.6125198590436616, 0.6187326836267151, 0.6382704941091745, 0.9078243732452392, 0.731232551497865, 0.49949883255207694, 0.7398143407473168, 0.7041849754698933, 0.9062818884849548, 0.9070278704166412, 0.9063457190990448, 0.6509245185201575, 0.9057691335678101, 0.38457583448486043, 0.9068173289299012, 0.601199556132432, 0.9064010620117188, 0.6653331268371464, 0.6636891472877384, 0.9059023916721344, 0.6394226340112893, 0.6011740699670457, 0.9069493234157563, 0.9094788551330567, 0.7379497167238793, 0.9083008408546448, 0.682726838548751, 0.9061761498451233, 0.6516353694742133, 0.9072795808315277, 0.907493668794632, 0.6761058932648017, 0.9067426562309265, 0.6843037747937156, 0.9072282791137696, 0.570365447590417, 0.9081511020660401, 0.9074064552783966, 0.9058962285518647, 0.578523737597349, 0.906714278459549, 0.5160674413416105, 0.9080358028411866, 0.90676029920578, 0.6398629096803873, 0.9063317000865937, 0.6529455928152015, 0.5475293302536012, 0.9088165819644928, 0.5619868216328919, 0.7205171453120012, 0.530822030139307, 0.6220403205099252, 0.6646091390193821, 0.9060067772865296, 0.6832451188164664, 0.907199901342392, 0.9063260972499848, 0.9044800460338592, 0.9054799914360047, 0.9047805488109589, 0.9069760501384735, 0.9068309724330902, 0.9056291699409486, 0.9073086321353913, 0.9076619982719422, 0.6519894389455725, 0.9072667717933655, 0.906634533405304, 0.6503580776994635, 0.9067415714263917, 0.9103196918964386, 0.6752122229442908, 0.9096397042274476, 0.6017830863855027, 0.9110787510871887, 0.9062861263751983, 0.9077352166175843, 0.6396831718740671, 0.6366087821779458, 0.905658107995987, 0.9050873398780823, 0.6652016628326298, 0.6943349934415286, 0.5000202718659654, 0.9066226840019226, 0.9055429697036743, 0.9084409773349762, 0.622517515295997, 0.9082343518733978, 0.6516241339986731, 0.9086089491844177, 0.6847816609936668, 0.9059664964675903, 0.9070351719856262, 0.6242102156820443, 0.9079675853252411, 0.9063823819160461, 0.9066486835479737, 0.9058162391185761, 0.6637830424846531, 0.9051857829093933, 0.9062610864639282, 0.9051780283451081, 0.5999575868508958, 0.9059780240058899, 0.9111761093139649, 0.7147922555791, 0.9058277785778046, 0.9060010492801667, 0.9058344542980195, 0.6532749621217657, 0.9060773432254792, 0.6950946427182619, 0.9033414661884308, 0.9054361045360565, 0.9082150161266327, 0.5594636217010981, 0.5387747838694648, 0.9076106369495393, 0.9065933763980866, 0.7184717344404954, 0.9061989068984986, 0.5044573318448378, 0.6631445575298192, 0.7107723335610489, 0.6747382289276436, 0.576660317587736, 0.9047442615032196, 0.9089512944221497, 0.6647904682697178, 0.9087469935417175, 0.9051558136940002, 0.6580274371450354, 0.6233784090223458, 0.6502744702642371, 0.6520929364507605, 0.9076378107070923, 0.6529915062730719, 0.6755567497120216, 0.9072328388690949, 0.9130904734134674, 0.9056942760944366, 0.9046053409576417, 0.4838174568522651, 0.9060420393943787, 0.9079082369804382, 0.4265648155696753, 0.6534493474309852, 0.9097544491291046, 0.6659593451560856, 0.6528197435682227, 0.9040511965751649, 0.6233359109106209, 0.9051473081111908, 0.6365444032011239, 0.9088750958442688, 0.908044970035553, 0.9064379215240479, 0.9077853739261628, 0.9079354524612427, 0.6567134169881751, 0.5186727961275297, 0.3886141564567538, 0.9068461239337922, 0.6844508432942344, 0.9085178256034852, 0.9089366912841798, 0.9086850702762604, 0.9066497981548309, 0.7180988239409226, 0.6522898642366339, 0.9065296351909637, 0.9068036913871765, 0.9088740170001984, 0.9060813903808594, 0.9081932961940765, 0.6769915169582679, 0.9066363871097565, 0.9075860321521759, 0.9071899890899658, 0.9061129570007325, 0.9074660241603851, 0.9055671572685242, 0.9073164224624634, 0.6754184013710334, 0.6739550358162238, 0.6775053030357673, 0.9055493950843811, 0.9085276484489441, 0.714113817630873, 0.5773649338959482, 0.6508919863050391, 0.6644127893985631, 0.6542306153123786, 0.9039036393165588, 0.664172117763889, 0.9077159702777863, 0.905758810043335, 0.9046273529529572, 0.9066674292087555, 0.561678826551944, 0.7360191997656427, 0.9064282476902008, 0.9086150109767914, 0.9070379495620727, 0.9060889482498169, 0.6403509942350595, 0.48067061174995496, 0.907256007194519, 0.907527631521225, 0.9091799139976502, 0.9063234746456147, 0.910295432806015, 0.9086829662322998, 0.6423869100866525, 0.6226843248548654, 0.9047666072845459, 0.9061924934387208, 0.622542304867759, 0.7220635639788408, 0.9064659893512726, 0.9096729218959808, 0.9056434214115143, 0.6960771001176302, 0.639553174143812, 0.6029655412099504, 0.6196769188585427, 0.6652338254989506, 0.68388372298059, 0.9067226350307465, 0.5772928003548411, 0.904983949661255, 0.9067480504512787, 0.9050337791442872, 0.4411786329956678, 0.6384847668466775, 0.40160163764211376, 0.5779901627777841, 0.4567166626453399, 0.6856400334435416, 0.5730089216232851, 0.9120916485786438, 0.9065390527248383, 0.9045468509197235, 0.9087916076183319, 0.6842589103775931, 0.9055936098098755, 0.7280574274063111, 0.7381484731436051, 0.651138672048371, 0.5384248330790594, 0.6840589546757652, 0.9074479758739472, 0.7449521901249303, 0.6767591661320045, 0.6757788544998481, 0.9096576929092407, 0.9083603143692017, 0.9068405866622925, 0.9061841249465943, 0.6924459374741976, 0.6716488188610389, 0.9056860089302063, 0.4970010045453325, 0.9072478473186493, 0.9066303193569184, 0.6849871062832786, 0.6824201666909171, 0.746032697164954, 0.908533126115799, 0.7172191666723985, 0.6645349908412815, 0.719662152445485, 0.9086305797100067, 0.9051558017730713, 0.9140156388282776, 0.9069123804569245, 0.907483434677124, 0.9086729526519776, 0.9095960140228272, 0.6307985402758304, 0.9085712671279907, 0.7009913407214344, 0.7147250990258315, 0.9066011309623718, 0.9077902376651764, 0.6516263155286719, 0.576285869288328, 0.9104001820087433, 0.9114695072174073, 0.66397599464072, 0.9079129874706269, 0.9080136299133301, 0.9093936264514924, 0.6649427342952611, 0.9093936264514924, 0.90756875872612, 0.7512882487569933, 0.9061228096485138, 0.5608172354512513, 0.730949691695619, 0.6849632942276909, 0.9042557060718537, 0.7073929077131904, 0.6384214428720681, 0.9102109372615814, 0.6757049626217201, 0.905856990814209, 0.9059993624687195, 0.6644932735504032, 0.9065142333507538, 0.9053588271141052, 0.9055388689041138, 0.9046999096870423, 0.616857790738701, 0.620991636389747, 0.6911944425897066, 0.9061932086944581, 0.6750762289867713, 0.4837068008292396, 0.9070026457309723, 0.9090257525444031, 0.9101713955402374, 0.6646130252422214, 0.6206922720613626, 0.6253802250566628, 0.5573429957283503, 0.638943174248716, 0.7388324662447251, 0.9074367463588715, 0.9074423372745514, 0.9070957601070404, 0.690209046681351, 0.6239265989008096, 0.7428560139774694, 0.908146470785141, 0.512733553457477, 0.6845496856766654, 0.906663304567337, 0.9094333946704865, 0.9063005983829499, 0.6746922558651283, 0.6835894012051535, 0.9057385087013244, 0.9078935146331787, 0.694148860056824, 0.5291357968572177, 0.350095076881922, 0.6859026276665641, 0.9065868437290192, 0.9086522400379181, 0.690960178454346, 0.9085134208202362, 0.6525936035459449, 0.9064379513263703, 0.9072647988796234, 0.6923656500176851, 0.9067594885826111, 0.9090120792388916, 0.9063818216323852, 0.9101662874221802, 0.9107557654380799, 0.9056849837303161, 0.9073922157287598, 0.5456494176387788, 0.5165043195459562, 0.9075007021427155, 0.9070841073989868, 0.9088068604469299, 0.48279113043434213, 0.7274710845947266, 0.9079302728176117, 0.907233077287674, 0.9057489633560181, 0.908731770515442, 0.730621550244737, 0.9065806269645691, 0.9080333471298218, 0.624679620141044, 0.46353304297206754, 0.7307761170095686, 0.9057020246982574, 0.6820144617157889, 0.9065157234668731, 0.48294470776207044, 0.9082477986812592, 0.5662297302025958, 0.6844358884888603, 0.9118678331375122, 0.9059823751449585, 0.9071221590042114, 0.9092500150203705, 0.908954656124115, 0.9117673516273499, 0.9093754947185516, 0.5721932260990694, 0.9086414813995362, 0.6394430068788736, 0.5762257222412852, 0.9046503186225892, 0.9141060829162598, 0.9109197199344635, 0.9047964155673981, 0.5545445494568335, 0.6978870890982808, 0.9090666353702546, 0.9082748830318451, 0.9043313682079316, 0.6527066854780127, 0.6523819951360632, 0.7503698961530809, 0.7135566095696547, 0.9129424691200256, 0.9085264682769776, 0.7113040308343032, 0.4146501242583058, 0.9103705406188966, 0.907217162847519, 0.9075235664844513, 0.9086884796619416, 0.9093983411788941, 0.9055675864219666, 0.9047884345054626, 0.6907881892518466, 0.9060380756855011, 0.7267173779010774, 0.9091624081134796, 0.6918134189443057, 0.9071816444396973, 0.9075524389743805, 0.5730689435005739, 0.9067895174026489, 0.9047245204448701, 0.9047488927841186, 0.65520372209861, 0.9064986050128937, 0.9055307626724244, 0.9080627083778382, 0.9087622225284576, 0.906043654680252, 0.9076099336147309, 0.9092906534671784, 0.7038211947424567, 0.6366364506540506, 0.9053537666797639, 0.9051947593688965, 0.9092811822891236, 0.6362006870565622, 0.9080886304378509, 0.9062646269798279, 0.9065119087696075, 0.905522507429123, 0.6918689525918429, 0.9071154713630677, 0.9097744166851044, 0.9065893888473511, 0.9079806804656982, 0.9093057870864868, 0.9048896431922913, 0.5543334499252802, 0.9086538672447205, 0.9095551431179046, 0.905072581768036, 0.9051710546016694, 0.9111345171928406, 0.9066647887229919, 0.9057870328426362, 0.9080910742282867, 0.9066490650177003, 0.6031571641824388, 0.9087733149528504, 0.9080902755260468, 0.504730547615559, 0.9060231745243073, 0.6908205605344241, 0.906235808134079, 0.9068180680274963, 0.9055913388729095, 0.5265270982507266, 0.9070416152477264, 0.9061842322349548, 0.4021174114788564, 0.6737189835415198, 0.9075586438179016, 0.9100908398628235, 0.905099356174469, 0.9051105558872223, 0.9087462604045868, 0.9050693571567535, 0.6862537645894005, 0.6564958481138159, 0.9096098840236664, 0.9088764727115631, 0.7051251834376013, 0.6752276724682167, 0.6843663835125877, 0.6926891244248812, 0.9081924021244049, 0.9080036461353302, 0.9062242448329926, 0.6658979821742893, 0.9087801575660706, 0.905282586812973, 0.9086718380451203, 0.9078533411026001, 0.9085343003273011, 0.4976263943597094, 0.9081172049045563, 0.9054611802101136, 0.683260240514846, 0.9058184623718262, 0.6399449912367074, 0.9066812634468079, 0.9063555061817169, 0.5051507245984388, 0.9078043520450593, 0.6383356300649851, 0.6644736457408787, 0.5124236212255738, 0.7496338920389299, 0.9074948012828827, 0.9084609270095826, 0.9106529831886292, 0.9061872601509094, 0.908849996328354, 0.9075982511043549, 0.905743533372879, 0.6658890593589665, 0.9080641686916352, 0.9081065058708191, 0.6933557666139071, 0.39587269035561884, 0.9078242301940919, 0.6898268080548708, 0.6221192430200723, 0.6028539613149309, 0.5430085682868959, 0.6762484794960334, 0.5782176260231761, 0.9043434202671051, 0.5143369601251863, 0.9039116561412811, 0.44196662428099187, 0.7151374081956008, 0.9082616627216339, 0.6398303774652688, 0.6006933883092546, 0.7416323825121202, 0.9072736322879792, 0.9097995162010193, 0.543403276205063, 0.6807662152844383, 0.6520199088399817, 0.9070531308650971, 0.6746554499969795, 0.2939048518873185, 0.4631232073950564, 0.9078586161136627, 0.904544097185135, 0.6775455600128486, 0.6859803580838157, 0.907323831319809, 0.9067329108715058, 0.6754541224346473, 0.6380288926420419, 0.7452192010521306, 0.9054793417453766, 0.9061578929424287, 0.5164059957239348, 0.6737052089081123, 0.9077886879444123, 0.9050700902938843, 0.9080020248889923, 0.9056429326534271, 0.9054443717002869, 0.9093627572059632, 0.9067102491855622, 0.908366322517395, 0.9073995053768158, 0.6635537136138798, 0.5010698120145985, 0.9057308852672578, 0.9078878700733185, 0.9099352598190308, 0.6393885341940133, 0.9065633118152618, 0.9099299013614655, 0.7342127975592218, 0.7186399506689805, 0.6391455200967996, 0.6522886721437384, 0.6652813184799077, 0.9086478352546692, 0.6027963891885423, 0.4636057129549777, 0.5030864607777906, 0.4403992608902122, 0.5284681533101596, 0.9112044095993043, 0.5932720015921371, 0.9069470822811126, 0.7482207300304784, 0.6851735853749229, 0.5285944138291873, 0.9060540556907654, 0.5706087498665408, 0.7491004349827184, 0.6007823363206529, 0.6227905701341775, 0.9052506923675537, 0.7474319951330309, 0.9063622057437897, 0.6634235549510837, 0.7459695758461369, 0.9061185836791993, 0.6914188063935702, 0.9066770374774933, 0.6380000678358285, 0.4393086147518781, 0.4756949672110822, 0.9054304957389832, 0.9052019000053406, 0.5821325246571329, 0.5790041689156321, 0.5018562854407808, 0.48368471730835033, 0.663195430094135, 0.5741040496826723, 0.5756932918309, 0.9049259245395661, 0.9058175563812256, 0.9092087388038635, 0.6643769431651951, 0.547063126564026, 0.905629575252533, 0.3482452586522467, 0.907155579328537, 0.745510663250554, 0.9079679429531098, 0.5384710995714425, 0.7067031448824561, 0.9059804379940033, 0.9051562011241913, 0.9076975584030151, 0.6513575164621284, 0.9057150959968567, 0.9061842739582062, 0.9077873766422272, 0.9073389887809754, 0.90799058675766, 0.9056700527667999, 0.9066758215427398, 0.9075712263584137, 0.6382378188429086, 0.7368526932367883, 0.6916063464478914, 0.9068264245986939, 0.9100196361541748, 0.4828849243033607, 0.9048010647296906, 0.7115764061795333, 0.6916763163403933, 0.6755575424537971, 0.9066485941410065, 0.759257686896212, 0.9067174434661865, 0.9056088328361511, 0.674706793437989, 0.6260705839338448, 0.9067592024803162, 0.9070401847362518, 0.9074904561042786, 0.6523620395009925, 0.907244610786438, 0.5676086776990099, 0.9069014191627502, 0.750624279367905, 0.9074148774147034, 0.6852514767247153, 0.5163198491308408, 0.9099168360233307, 0.9100825965404511, 0.9054769277572632, 0.9092370212078095, 0.909512597322464, 0.378078740127827, 0.7125948171006301, 0.6207220743837503, 0.6637694466651799, 0.4617560496973788, 0.9077024579048157, 0.9066665649414063, 0.9083042025566102, 0.4806051658499916, 0.9086681067943573, 0.9053861439228058, 0.9083502948284149, 0.9056021630764007, 0.7247669532080768, 0.6206911514940407, 0.7447969035898333, 0.9053769409656525, 0.9074794948101044, 0.5652978950280352, 0.9072386622428894, 0.6203129779043344, 0.4993461433335558, 0.5539930954826838, 0.9086110174655915, 0.6370249656496255, 0.9104413688182831, 0.9090837776660919, 0.906077241897583, 0.9066780030727387, 0.9072807669639588, 0.5766698841332224, 0.7007774136908711, 0.9075294613838196, 0.9067728340625764, 0.904426783323288, 0.5688788053803743, 0.49921241435253777, 0.9067433774471283, 0.7075554913027442, 0.6060819939038896, 0.9070985913276672, 0.6373039571104256, 0.6831710004406882, 0.9070084095001221, 0.9056314706802369, 0.9080256164073944, 0.7622090154726823, 0.7055075353129066, 0.6844941758709862, 0.9093320548534394, 0.706316662405268, 0.9060473382472992, 0.9076372921466828, 0.6931591308907931, 0.9076317846775055, 0.9079829514026642, 0.9075951337814331, 0.9084264397621155, 0.9057399272918701, 0.9065744638442993, 0.5480136060714723, 0.4996584835931078, 0.7435270013450993, 0.5450930082798006, 0.9055931329727173, 0.9087394058704377, 0.9079915761947632, 0.9093958675861359, 0.9077024579048157, 0.6216418694200662, 0.6844510519104912, 0.5385140433508948, 0.6381336180029122, 0.9088382422924042, 0.9045413315296174, 0.9097505867481231, 0.905936223268509, 0.9069668591022492, 0.9062438666820526, 0.6922580397920077, 0.6407984939871042, 0.9064373016357422, 0.6229347060861733, 0.9069081604480743, 0.9083680093288422, 0.9068288683891297, 0.906541246175766, 0.4914396628633859, 0.9058402836322784, 0.7262599241733552, 0.9051952183246613, 0.7072760766489661, 0.5051359835549608, 0.9088241338729859, 0.9070766627788543, 0.49144720881141446, 0.9078090608119965, 0.7131375650750259, 0.6845337831574394, 0.6034229710958147, 0.5147193954469941, 0.5135162101270936, 0.9093268692493439, 0.9065317988395691, 0.9055905342102051, 0.685362359245391, 0.6410923210439889, 0.9065174877643586, 0.6513926235979011, 0.9060063898563385, 0.9071246027946472, 0.5635744509511292, 0.9082307279109955, 0.9057731628417969, 0.9100517809391022, 0.6030418045722209, 0.570187748262948, 0.7281951797008516, 0.9055999457836151, 0.9039931535720825, 0.9086070239543915, 0.9063664376735687, 0.9063275814056396, 0.9092342495918274, 0.7072523540003455, 0.4425472510071424, 0.9099948644638062, 0.6545119253938605, 0.7455718116556291, 0.907210624217987, 0.90579634308815, 0.9064914584159851, 0.498270815937163, 0.9084409296512604, 0.906389731168747, 0.7190184282423753, 0.6019095435998583, 0.5767724816082743, 0.6746644860611274, 0.9076243460178376, 0.9061085462570191, 0.49934743675434745, 0.9065438449382782, 0.690461138565964, 0.9062038123607635, 0.5692769198418215, 0.7037114702113332, 0.7234576418181538])\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "id": "oadBrReYKBPU",
    "outputId": "b53a87c0-f7cc-4d45-f85b-a319a168ef52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1947)\n",
      "(?, 1947)\n",
      "step 0: loss 9.297\n",
      "step 10: loss 0.676\n",
      "step 20: loss 0.385\n",
      "step 30: loss 0.253\n",
      "step 40: loss 0.169\n",
      "step 50: loss 0.113\n",
      "step 60: loss 0.075\n",
      "step 70: loss 0.050\n",
      "step 80: loss 0.034\n",
      "step 90: loss 0.022\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-3db983ea131f>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesVec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# setup siamese network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msiamese\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Siamese\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64881\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mactivated_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmaxpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxp_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(self, name, inputs, cur_channel)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(prev_channel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_w\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable Siamese/conv_1_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-19-91aa96e5ce29>\", line 36, in conv_layer\n    w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 21, in network\n    activated_conv1 = self.conv_layer('conv_1',x,3)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 9, in __init__\n    self.o1 = self.network(self.x1)\n"
     ]
    }
   ],
   "source": [
    "# prepare data and tf.session\n",
    "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession() \n",
    "vectorizer = CountVectorizer()\n",
    "tokenizer=vectorizer.build_tokenizer()\n",
    "rowcount = 0\n",
    "qList,aList = [],[]\n",
    "for row in data[0:100]:\n",
    "  rowcount += 1\n",
    "  quesVec,ansVec = [0]*64881,[0]*64881\n",
    "  quesHash = getThreeHash(row[1].lower())\n",
    "  ansHash = getThreeHash(row[3].lower())\n",
    "#   print(quesHash)\n",
    "  for token in tokenizer(quesHash.lower()):\n",
    "     quesVec[three_hash_dict[token]] += 1\n",
    "  for token in tokenizer(ansHash.lower()):\n",
    "     ansVec[three_hash_dict[token]] += 1\n",
    "  qList.append(quesVec)\n",
    "  aList.append(ansVec)\n",
    "  if rowcount % 10 == 0:    \n",
    "    trainSiamese(qList,aList)\n",
    "    qList,aList = [],[]\n",
    "sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdLocomkdJ4p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Siamese.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
