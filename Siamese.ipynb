{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sWwewmyoKBOK",
        "colab_type": "code",
        "outputId": "e7aaf3e9-72e0-4da2-ba0b-54a45a198d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "c89eaaf1-a857-4d05-aec6-8cdca0c1aad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self,dname=\"Siamese\"):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
        "\n",
        "        with tf.variable_scope(dname) as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = tf.nn.relu(conv+b)\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\")  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.4, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNX5a8KeKBOu",
        "colab_type": "code",
        "outputId": "3b5f276d-9367-46d7-c95a-bc4d9cec4ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1AycFi7m8_NYsK0zTuaAFrci0cLQdE51i\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n",
        "# quest = \"where do i find hot girls\"\n",
        "# hash_var = getThreeHash(quest)\n",
        "#print(hash_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFQrr4tINCx2",
        "colab_type": "code",
        "outputId": "0bb3c7ea-d3b3-4992-a9a3-61c0c08187e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4OzxqKw47mcT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[:2000,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qM5IVsGE8Ifq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "4f137ce0-5959-4d55-bca9-eae2d7347cd5"
      },
      "cell_type": "code",
      "source": [
        "data"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Why are yawns contagious?',\n",
              "        'When your body need more oxygen  you yawn.When you yawn  you take more oxygen in the air.If the density of the oxygen in the air becomes lower  other people (their bodies) can feel that and start to yawn to get more oxygen.That\\'s why yawns are contagious. Yawning is extremely contagious -- 55% of people who witness someone yawn will yawn within five minutes. If a visually impaired person hears a tape of someone yawning  he or she is likely to yawn as well. Face it  the likelihood of you making it to the end of this answer without looking like one of these gaping maws is unlikely.Although the contagious nature of yawning is well established  we know less about why this is so. Researchers are currently giving the topic some serious attention. One theory suggests it\\'s a holdover from a period in evolutionary history when yawning served to coordinate the social behavior of a group of animals. A recent study postulates that contagious yawning could be part of the \"neural network involved in empathy.\"While the mystery of contagious yawning has yet to be solved  perhaps researchers are closing in on an answer. On the other hand  given the subject matter  we wouldn\\'t blame them for falling asleep at the wheel. In the meantime  give the \"yawn challenge\" a try -- it\\'s tougher than it looks.'],\n",
              "       [\"What's the best way to heat up a cold hamburger (In&Out)?\",\n",
              "        'If you must eat a heated In&Out hamburger then I suggest the follow:First  scrape the ketchup  mayo and any other sauce that is on the hamburger and put it a side. Also take the vegetables out.Second  pre-heat a steak pan; put the hamburger separate form the bun on the hot panThird  heat up both parts of the bun near the hamburger on the same hot pan (you can get some souce from the hamburger on it)Fourth  you can either put back the original ketchup  mayo and the vegetables or  better yet  get new ones from what you have at home .Bon Appetite'],\n",
              "       ['Vacation rentals in the Turks and Caicos',\n",
              "        \"I like Providenciales best. Beautiful beaches are scattered on all sides of Providenciales  the most spectacular of which is a 12 mile stretch located on Grace Bay  which is protected by a healthy barrier reef. Provo has an 18 hole golf course  a casino  shopping centres  three marinas  a growing number of of bars and excellent restaurants. Provo is also a divers' and water lovers' paradise.\"],\n",
              "       ...,\n",
              "       ['I am using McAfee firewall  and I can not sign in to Yahoo messenger unless I disable the firewall.Any help.?',\n",
              "        'I have the same problem trying to upload files through FTP to a website I built. I have to temporarily disable both the McAfee and Windows firewalls.'],\n",
              "       [\"if i pay for a item at yahoo auction with money order and don't recieve item can i get money back?\",\n",
              "        'yea  you have to complain to yahoo tho. they will look into it and if they catch the guy they will suspend his yahoo account...'],\n",
              "       ['I want to buy an engagement ring. It is all overwhelming. How can I make this process simple?',\n",
              "        'There are 2 things you need to decide on: The stone and the setting.When it comes to the setting (the ring itself) then you need to know what the person you are buying it for likes. There are lots of variations like model  material (gold  platinum)  color.When it comes to the stone  I think the best thing is to stick to a budget. Once you have a budget then you need to decide what is most important for you  the size or the quality. In my opinion quality is more important so you should look for a diamond with good clarity and color  and since you have a budget  the size will be determine by it. If size is most important the decide what size of a stone you want and the qaulity will be determine by the budget you set for yourself.I am sure no matter what you choose  as long as you keep her taste in mind  she will love it.Good luck']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "lCr5Z1wWKyzj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[ndata[:,4] == 'en']\n",
        "data = data[:1500000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9qXk85hdLIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiamese(quesVec,ansVec,siamese,sess,iteration=100):\n",
        "  \n",
        "  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(siamese.loss)\n",
        "  tf.initialize_all_variables().run()\n",
        "\n",
        "  #for step in range(iteration):\n",
        "  batch_x1, batch_y1 = quesVec,np.array([1]*10)\n",
        "  #       print(batch_x1)\n",
        "  #       print(batch_y1)\n",
        "  batch_x2, batch_y2 = ansVec,np.array([1]*10)\n",
        "  batch_y = (batch_y1 == batch_y2).astype('float')\n",
        "\n",
        "  _, loss_v = sess.run([train_step,siamese.loss], feed_dict={\n",
        "                      siamese.x1: batch_x1,\n",
        "                      siamese.x2: batch_x2,\n",
        "                      siamese.y_: batch_y})\n",
        "\n",
        "  print(\"Loss \",loss_v)\n",
        "#       if np.isnan(loss_v):\n",
        "#           print('Model diverged with loss = NaN')\n",
        "#           quit()\n",
        "\n",
        "#       if step % 10 == 0:\n",
        "#           print ('step %d: loss %.3f' % (step, loss_v))\n",
        "\n",
        "#       if step % 1000 == 0 and step > 0:\n",
        "#           #saver.save(sess, './model')\n",
        "#           embed = siamese.o1.eval({siamese.x1: mnist.test.images})\n",
        "#           embed.tofile('embed.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwGPkK3SON0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(hashString,dictionary):\n",
        "  \n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer = vectorizer.build_tokenizer()\n",
        "  vec = [0]*64881\n",
        "  \n",
        "  for token in tokenizer(hashString):\n",
        "    try:\n",
        "      vec[dictionary[token]] += 1\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "  return vec\n",
        "\n",
        "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
        "  \n",
        "  good_data = data[goodSet]\n",
        "  bad_quest = data[badSetQues,0]\n",
        "  bad_anser = data[badSetAns,1]\n",
        "  questions = np.concatenate((good_data[:,0],bad_quest))\n",
        "  answers = np.concatenate((good_data[:,1],bad_anser))\n",
        "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
        "  \n",
        "  ques,ans = [],[]\n",
        "  for d in questions:\n",
        "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
        "  for i,d in enumerate(answers):\n",
        "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
        "  return np.array(ques),np.array(ans),label\n",
        "    \n",
        "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
        "  \n",
        "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
        "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXFS2FUwq1K0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81dbf905-6268-4721-ec03-8cb27a955eae"
      },
      "cell_type": "code",
      "source": [
        "siamese = Siamese('siamese2')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXmBCVN0V3hv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiameseNetwork(data,siamese,batchSize,epochs,dictionary,lrate):\n",
        "  \n",
        "  #Siamese Network\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    train_Step = tf.train.MomentumOptimizer(lrate,0.05).minimize(siamese.loss)\n",
        "    tf.initialize_all_variables().run()\n",
        "    \n",
        "    while epochs > 0:\n",
        "      \n",
        "      permSet = np.random.permutation(data.shape[0])\n",
        "\n",
        "      for p in range(0,permSet.shape[0],batchSize):\n",
        "\n",
        "        goodSet = np.array(list(range(p,p+batchSize)))\n",
        "        badSetQ = goodSet.copy()\n",
        "        badSetA = np.array(permSet[p:p+batchSize])\n",
        "        ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
        "        \n",
        "        #print(ques.shape,ans.shape,labl.shape)\n",
        "        _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
        "                      siamese.x1: ques,\n",
        "                      siamese.x2: ans,\n",
        "                      siamese.y_: labl})\n",
        "        \n",
        "        print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
        "      \n",
        "      epochs -= 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scF-b7JY8NQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        },
        "outputId": "1a8f91de-23ff-4d1f-90dc-439a10d98b0f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "trainSiameseNetwork(data,siamese,100,10,three_hash_dict,0.01)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 Batch 0.0 Loss 0.3118469\n",
            "Epoch 10 Batch 1.0 Loss 0.2973416\n",
            "Epoch 10 Batch 2.0 Loss 0.29913765\n",
            "Epoch 10 Batch 3.0 Loss 0.30713034\n",
            "Epoch 10 Batch 4.0 Loss 0.30141324\n",
            "Epoch 10 Batch 5.0 Loss 0.29696393\n",
            "Epoch 10 Batch 6.0 Loss 0.297805\n",
            "Epoch 10 Batch 7.0 Loss 0.2989009\n",
            "Epoch 10 Batch 8.0 Loss 0.3006249\n",
            "Epoch 10 Batch 9.0 Loss 0.29673183\n",
            "Epoch 10 Batch 10.0 Loss 0.29655465\n",
            "Epoch 10 Batch 11.0 Loss 0.29040053\n",
            "Epoch 10 Batch 12.0 Loss 0.29937923\n",
            "Epoch 10 Batch 13.0 Loss 0.29456383\n",
            "Epoch 10 Batch 14.0 Loss 0.29643893\n",
            "Epoch 10 Batch 15.0 Loss 0.28750452\n",
            "Epoch 10 Batch 16.0 Loss 0.289497\n",
            "Epoch 10 Batch 17.0 Loss 0.29305884\n",
            "Epoch 10 Batch 18.0 Loss 0.29107133\n",
            "Epoch 10 Batch 19.0 Loss 0.28824154\n",
            "Epoch 9 Batch 0.0 Loss 0.30671227\n",
            "Epoch 9 Batch 1.0 Loss 0.3073527\n",
            "Epoch 9 Batch 2.0 Loss 0.3031133\n",
            "Epoch 9 Batch 3.0 Loss 0.29941955\n",
            "Epoch 9 Batch 4.0 Loss 0.29454526\n",
            "Epoch 9 Batch 5.0 Loss 0.29197878\n",
            "Epoch 9 Batch 6.0 Loss 0.29311746\n",
            "Epoch 9 Batch 7.0 Loss 0.29163057\n",
            "Epoch 9 Batch 8.0 Loss 0.29936364\n",
            "Epoch 9 Batch 9.0 Loss 0.299213\n",
            "Epoch 9 Batch 10.0 Loss 0.29281095\n",
            "Epoch 9 Batch 11.0 Loss 0.29346427\n",
            "Epoch 9 Batch 12.0 Loss 0.3021374\n",
            "Epoch 9 Batch 13.0 Loss 0.29469997\n",
            "Epoch 9 Batch 14.0 Loss 0.2961053\n",
            "Epoch 9 Batch 15.0 Loss 0.29169753\n",
            "Epoch 9 Batch 16.0 Loss 0.28920007\n",
            "Epoch 9 Batch 17.0 Loss 0.28301877\n",
            "Epoch 9 Batch 18.0 Loss 0.290933\n",
            "Epoch 9 Batch 19.0 Loss 0.29487392\n",
            "Epoch 8 Batch 0.0 Loss 0.30458575\n",
            "Epoch 8 Batch 1.0 Loss 0.3075524\n",
            "Epoch 8 Batch 2.0 Loss 0.29459187\n",
            "Epoch 8 Batch 3.0 Loss 0.29910675\n",
            "Epoch 8 Batch 4.0 Loss 0.29593486\n",
            "Epoch 8 Batch 5.0 Loss 0.2949986\n",
            "Epoch 8 Batch 6.0 Loss 0.29446825\n",
            "Epoch 8 Batch 7.0 Loss 0.29997557\n",
            "Epoch 8 Batch 8.0 Loss 0.30241597\n",
            "Epoch 8 Batch 9.0 Loss 0.29986793\n",
            "Epoch 8 Batch 10.0 Loss 0.29593635\n",
            "Epoch 8 Batch 11.0 Loss 0.28835252\n",
            "Epoch 8 Batch 12.0 Loss 0.29948097\n",
            "Epoch 8 Batch 13.0 Loss 0.30029413\n",
            "Epoch 8 Batch 14.0 Loss 0.28972793\n",
            "Epoch 8 Batch 15.0 Loss 0.2846824\n",
            "Epoch 8 Batch 16.0 Loss 0.27978462\n",
            "Epoch 8 Batch 17.0 Loss 0.2840965\n",
            "Epoch 8 Batch 18.0 Loss 0.28122848\n",
            "Epoch 8 Batch 19.0 Loss 0.27724025\n",
            "Epoch 7 Batch 0.0 Loss 0.30331323\n",
            "Epoch 7 Batch 1.0 Loss 0.30696326\n",
            "Epoch 7 Batch 2.0 Loss 0.29591912\n",
            "Epoch 7 Batch 3.0 Loss 0.31360406\n",
            "Epoch 7 Batch 4.0 Loss 0.2961794\n",
            "Epoch 7 Batch 5.0 Loss 0.29087532\n",
            "Epoch 7 Batch 6.0 Loss 0.28320166\n",
            "Epoch 7 Batch 7.0 Loss 0.2878312\n",
            "Epoch 7 Batch 8.0 Loss 0.28710613\n",
            "Epoch 7 Batch 9.0 Loss 0.29236034\n",
            "Epoch 7 Batch 10.0 Loss 0.29323268\n",
            "Epoch 7 Batch 11.0 Loss 0.28563052\n",
            "Epoch 7 Batch 12.0 Loss 0.27233174\n",
            "Epoch 7 Batch 13.0 Loss 0.2588605\n",
            "Epoch 7 Batch 14.0 Loss 0.27644864\n",
            "Epoch 7 Batch 15.0 Loss 0.27237308\n",
            "Epoch 7 Batch 16.0 Loss 0.27157447\n",
            "Epoch 7 Batch 17.0 Loss 0.26079026\n",
            "Epoch 7 Batch 18.0 Loss 0.27201718\n",
            "Epoch 7 Batch 19.0 Loss 0.27526233\n",
            "Epoch 6 Batch 0.0 Loss 0.30544326\n",
            "Epoch 6 Batch 1.0 Loss 0.29038098\n",
            "Epoch 6 Batch 2.0 Loss 0.29408017\n",
            "Epoch 6 Batch 3.0 Loss 0.3037959\n",
            "Epoch 6 Batch 4.0 Loss 0.28904516\n",
            "Epoch 6 Batch 5.0 Loss 0.28217456\n",
            "Epoch 6 Batch 6.0 Loss 0.28175843\n",
            "Epoch 6 Batch 7.0 Loss 0.27721542\n",
            "Epoch 6 Batch 8.0 Loss 0.25336862\n",
            "Epoch 6 Batch 9.0 Loss 0.27884683\n",
            "Epoch 6 Batch 10.0 Loss 0.24606828\n",
            "Epoch 6 Batch 11.0 Loss 0.3212769\n",
            "Epoch 6 Batch 12.0 Loss 0.2936409\n",
            "Epoch 6 Batch 13.0 Loss 0.29345414\n",
            "Epoch 6 Batch 14.0 Loss 0.29448265\n",
            "Epoch 6 Batch 15.0 Loss 0.2810066\n",
            "Epoch 6 Batch 16.0 Loss 0.28801048\n",
            "Epoch 6 Batch 17.0 Loss 0.28548178\n",
            "Epoch 6 Batch 18.0 Loss 0.2840378\n",
            "Epoch 6 Batch 19.0 Loss 0.28902403\n",
            "Epoch 5 Batch 0.0 Loss 0.29677972\n",
            "Epoch 5 Batch 1.0 Loss 0.30301583\n",
            "Epoch 5 Batch 2.0 Loss 0.28825942\n",
            "Epoch 5 Batch 3.0 Loss 0.290094\n",
            "Epoch 5 Batch 4.0 Loss 0.27990994\n",
            "Epoch 5 Batch 5.0 Loss 0.27191338\n",
            "Epoch 5 Batch 6.0 Loss 0.25970054\n",
            "Epoch 5 Batch 7.0 Loss 0.2543445\n",
            "Epoch 5 Batch 8.0 Loss 0.25633028\n",
            "Epoch 5 Batch 9.0 Loss 0.26545706\n",
            "Epoch 5 Batch 10.0 Loss 0.25110167\n",
            "Epoch 5 Batch 11.0 Loss 0.26847288\n",
            "Epoch 5 Batch 12.0 Loss 0.27513465\n",
            "Epoch 5 Batch 13.0 Loss 0.25003505\n",
            "Epoch 5 Batch 14.0 Loss 0.2559614\n",
            "Epoch 5 Batch 15.0 Loss 0.25299892\n",
            "Epoch 5 Batch 16.0 Loss 0.27642268\n",
            "Epoch 5 Batch 17.0 Loss 0.2592636\n",
            "Epoch 5 Batch 18.0 Loss 0.25624108\n",
            "Epoch 5 Batch 19.0 Loss 0.2764854\n",
            "Epoch 4 Batch 0.0 Loss 0.29904008\n",
            "Epoch 4 Batch 1.0 Loss 0.30096284\n",
            "Epoch 4 Batch 2.0 Loss 0.3004101\n",
            "Epoch 4 Batch 3.0 Loss 0.3021841\n",
            "Epoch 4 Batch 4.0 Loss 0.29659018\n",
            "Epoch 4 Batch 5.0 Loss 0.29545376\n",
            "Epoch 4 Batch 6.0 Loss 0.2905516\n",
            "Epoch 4 Batch 7.0 Loss 0.29796985\n",
            "Epoch 4 Batch 8.0 Loss 0.29097706\n",
            "Epoch 4 Batch 9.0 Loss 0.29602093\n",
            "Epoch 4 Batch 10.0 Loss 0.2960664\n",
            "Epoch 4 Batch 11.0 Loss 0.29496998\n",
            "Epoch 4 Batch 12.0 Loss 0.29797906\n",
            "Epoch 4 Batch 13.0 Loss 0.29543373\n",
            "Epoch 4 Batch 14.0 Loss 0.29442835\n",
            "Epoch 4 Batch 15.0 Loss 0.2868676\n",
            "Epoch 4 Batch 16.0 Loss 0.28885904\n",
            "Epoch 4 Batch 17.0 Loss 0.2907066\n",
            "Epoch 4 Batch 18.0 Loss 0.2885257\n",
            "Epoch 4 Batch 19.0 Loss 0.28958204\n",
            "Epoch 3 Batch 0.0 Loss 0.29759878\n",
            "Epoch 3 Batch 1.0 Loss 0.29986385\n",
            "Epoch 3 Batch 2.0 Loss 0.30133703\n",
            "Epoch 3 Batch 3.0 Loss 0.30375165\n",
            "Epoch 3 Batch 4.0 Loss 0.2957052\n",
            "Epoch 3 Batch 5.0 Loss 0.2914758\n",
            "Epoch 3 Batch 6.0 Loss 0.2931794\n",
            "Epoch 3 Batch 7.0 Loss 0.2961131\n",
            "Epoch 3 Batch 8.0 Loss 0.29925492\n",
            "Epoch 3 Batch 9.0 Loss 0.2914865\n",
            "Epoch 3 Batch 10.0 Loss 0.29177722\n",
            "Epoch 3 Batch 11.0 Loss 0.28849068\n",
            "Epoch 3 Batch 12.0 Loss 0.2906159\n",
            "Epoch 3 Batch 13.0 Loss 0.29422763\n",
            "Epoch 3 Batch 14.0 Loss 0.28170025\n",
            "Epoch 3 Batch 15.0 Loss 0.26953897\n",
            "Epoch 3 Batch 16.0 Loss 0.2788394\n",
            "Epoch 3 Batch 17.0 Loss 0.2698732\n",
            "Epoch 3 Batch 18.0 Loss 0.2538404\n",
            "Epoch 3 Batch 19.0 Loss 0.24559176\n",
            "Epoch 2 Batch 0.0 Loss 0.29630816\n",
            "Epoch 2 Batch 1.0 Loss 0.29342794\n",
            "Epoch 2 Batch 2.0 Loss 0.28311977\n",
            "Epoch 2 Batch 3.0 Loss 0.25906095\n",
            "Epoch 2 Batch 4.0 Loss 0.28237227\n",
            "Epoch 2 Batch 5.0 Loss 0.2707793\n",
            "Epoch 2 Batch 6.0 Loss 0.26686364\n",
            "Epoch 2 Batch 7.0 Loss 0.2508098\n",
            "Epoch 2 Batch 8.0 Loss 0.25874326\n",
            "Epoch 2 Batch 9.0 Loss 0.29865223\n",
            "Epoch 2 Batch 10.0 Loss 0.2931693\n",
            "Epoch 2 Batch 11.0 Loss 0.28996724\n",
            "Epoch 2 Batch 12.0 Loss 0.29471081\n",
            "Epoch 2 Batch 13.0 Loss 0.28849182\n",
            "Epoch 2 Batch 14.0 Loss 0.2883307\n",
            "Epoch 2 Batch 15.0 Loss 0.28557283\n",
            "Epoch 2 Batch 16.0 Loss 0.2898363\n",
            "Epoch 2 Batch 17.0 Loss 0.29205763\n",
            "Epoch 2 Batch 18.0 Loss 0.29033476\n",
            "Epoch 2 Batch 19.0 Loss 0.2874962\n",
            "Epoch 1 Batch 0.0 Loss 0.29864615\n",
            "Epoch 1 Batch 1.0 Loss 0.29575562\n",
            "Epoch 1 Batch 2.0 Loss 0.29471272\n",
            "Epoch 1 Batch 3.0 Loss 0.29370278\n",
            "Epoch 1 Batch 4.0 Loss 0.28489858\n",
            "Epoch 1 Batch 5.0 Loss 0.29111746\n",
            "Epoch 1 Batch 6.0 Loss 0.28493586\n",
            "Epoch 1 Batch 7.0 Loss 0.28784412\n",
            "Epoch 1 Batch 8.0 Loss 0.2776975\n",
            "Epoch 1 Batch 9.0 Loss 0.27101848\n",
            "Epoch 1 Batch 10.0 Loss 0.25062528\n",
            "Epoch 1 Batch 11.0 Loss 0.24459523\n",
            "Epoch 1 Batch 12.0 Loss 0.27377254\n",
            "Epoch 1 Batch 13.0 Loss 0.26270735\n",
            "Epoch 1 Batch 14.0 Loss 0.2713145\n",
            "Epoch 1 Batch 15.0 Loss 0.28140357\n",
            "Epoch 1 Batch 16.0 Loss 0.2838875\n",
            "Epoch 1 Batch 17.0 Loss 0.28880224\n",
            "Epoch 1 Batch 18.0 Loss 0.28663495\n",
            "Epoch 1 Batch 19.0 Loss 0.27994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "InihtcorSzMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "17c5f10c-e626-4fe7-e64c-890d0ba454eb"
      },
      "cell_type": "code",
      "source": [
        "label"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "oadBrReYKBPU",
        "colab_type": "code",
        "outputId": "b53a87c0-f7cc-4d45-f85b-a319a168ef52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare data and tf.session\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() \n",
        "vectorizer = CountVectorizer()\n",
        "tokenizer=vectorizer.build_tokenizer()\n",
        "rowcount = 0\n",
        "qList,aList = [],[]\n",
        "for row in data[0:100]:\n",
        "  rowcount += 1\n",
        "  quesVec,ansVec = [0]*64881,[0]*64881\n",
        "  quesHash = getThreeHash(row[1].lower())\n",
        "  ansHash = getThreeHash(row[3].lower())\n",
        "#   print(quesHash)\n",
        "  for token in tokenizer(quesHash.lower()):\n",
        "     quesVec[three_hash_dict[token]] += 1\n",
        "  for token in tokenizer(ansHash.lower()):\n",
        "     ansVec[three_hash_dict[token]] += 1\n",
        "  qList.append(quesVec)\n",
        "  aList.append(ansVec)\n",
        "  if rowcount % 10 == 0:    \n",
        "    trainSiamese(qList,aList)\n",
        "    qList,aList = [],[]\n",
        "sess.close()\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n",
            "step 0: loss 9.297\n",
            "step 10: loss 0.676\n",
            "step 20: loss 0.385\n",
            "step 30: loss 0.253\n",
            "step 40: loss 0.169\n",
            "step 50: loss 0.113\n",
            "step 60: loss 0.075\n",
            "step 70: loss 0.050\n",
            "step 80: loss 0.034\n",
            "step 90: loss 0.022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3db983ea131f>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesVec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# setup siamese network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msiamese\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Siamese\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64881\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mactivated_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmaxpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxp_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(self, name, inputs, cur_channel)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(prev_channel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_w\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable Siamese/conv_1_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-19-91aa96e5ce29>\", line 36, in conv_layer\n    w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 21, in network\n    activated_conv1 = self.conv_layer('conv_1',x,3)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 9, in __init__\n    self.o1 = self.network(self.x1)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qdLocomkdJ4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}