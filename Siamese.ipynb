{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sWwewmyoKBOK",
        "colab_type": "code",
        "outputId": "c14388b3-6c11-4d1d-f989-23c923262143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.3MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 4.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 5.0MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 6.8MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 6.8MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 12.3MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 12.3MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 12.2MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 12.3MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 12.4MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 12.5MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 14.0MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 14.2MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 14.3MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 14.7MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 14.6MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 14.7MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.1MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 14.3MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 14.3MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 14.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 49.1MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 50.3MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 49.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 50.8MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 46.2MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 47.0MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 55.6MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 54.9MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 55.2MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 20.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 20.3MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 20.3MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 20.3MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 20.3MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 20.7MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 19.3MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 19.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 19.1MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 18.9MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 45.4MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 42.4MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 42.1MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 42.6MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 42.5MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 45.8MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 54.6MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 53.0MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 53.8MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 53.9MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 53.7MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 59.6MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 58.0MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 58.6MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 59.5MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 58.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 45.8MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 46.6MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 46.9MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 47.8MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 48.1MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 47.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 48.1MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 47.3MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 46.6MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 45.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 58.0MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 56.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 54.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 54.6MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 54.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 55.8MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 56.9MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 56.7MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 57.1MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 51.6MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 51.5MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 53.2MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 55.9MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 56.2MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 55.9MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 55.7MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 56.1MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 56.6MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 56.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 65.3MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 65.5MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 65.4MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 21.0MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "45ed188d-f1a4-4da9-d1c1-817ca4da1473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self,dname=\"Siamese\"):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, 64881])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, 64881])\n",
        "\n",
        "        with tf.variable_scope(dname) as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,64881,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = tf.nn.relu(conv+b)\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\")  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.4, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNX5a8KeKBOu",
        "colab_type": "code",
        "outputId": "f82b6463-ef66-481e-d9c1-a92ef007b348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1AycFi7m8_NYsK0zTuaAFrci0cLQdE51i\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n",
        "# quest = \"where do i find hot girls\"\n",
        "# hash_var = getThreeHash(quest)\n",
        "#print(hash_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFQrr4tINCx2",
        "colab_type": "code",
        "outputId": "59c6f371-9315-4da5-b527-8d31084252dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1tpMcShoAyQ-uxJFz4SMDPzrg8glfbh1Z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCr5Z1wWKyzj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[ndata[:,4] == 'en']\n",
        "data = data[:1500000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9qXk85hdLIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiamese(quesVec,ansVec,siamese,sess,iteration=100):\n",
        "  \n",
        "  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(siamese.loss)\n",
        "  tf.initialize_all_variables().run()\n",
        "\n",
        "  #for step in range(iteration):\n",
        "  batch_x1, batch_y1 = quesVec,np.array([1]*10)\n",
        "  #       print(batch_x1)\n",
        "  #       print(batch_y1)\n",
        "  batch_x2, batch_y2 = ansVec,np.array([1]*10)\n",
        "  batch_y = (batch_y1 == batch_y2).astype('float')\n",
        "\n",
        "  _, loss_v = sess.run([train_step,siamese.loss], feed_dict={\n",
        "                      siamese.x1: batch_x1,\n",
        "                      siamese.x2: batch_x2,\n",
        "                      siamese.y_: batch_y})\n",
        "\n",
        "  print(\"Loss \",loss_v)\n",
        "#       if np.isnan(loss_v):\n",
        "#           print('Model diverged with loss = NaN')\n",
        "#           quit()\n",
        "\n",
        "#       if step % 10 == 0:\n",
        "#           print ('step %d: loss %.3f' % (step, loss_v))\n",
        "\n",
        "#       if step % 1000 == 0 and step > 0:\n",
        "#           #saver.save(sess, './model')\n",
        "#           embed = siamese.o1.eval({siamese.x1: mnist.test.images})\n",
        "#           embed.tofile('embed.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwGPkK3SON0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(hashString,dictionary):\n",
        "  \n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer = vectorizer.build_tokenizer()\n",
        "  vec = [0]*64881\n",
        "  \n",
        "  for token in tokenizer(hashString):\n",
        "    try:\n",
        "      vec[dictionary[token]] += 1\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "  return vec\n",
        "\n",
        "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
        "  \n",
        "  good_data = data[goodSet]\n",
        "  bad_quest = data[badSetQues,1]\n",
        "  bad_anser = data[badSetAns,3]\n",
        "  questions = np.concatenate((good_data[:,1],bad_quest))\n",
        "  answers = np.concatenate((good_data[:,3],bad_anser))\n",
        "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
        "  \n",
        "  ques,ans = [],[]\n",
        "  for d in questions:\n",
        "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
        "  for i,d in enumerate(answers):\n",
        "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
        "  return np.array(ques),np.array(ans),label\n",
        "    \n",
        "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
        "  \n",
        "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
        "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXFS2FUwq1K0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "00715f43-eede-49ba-9fe7-225902e546d5"
      },
      "cell_type": "code",
      "source": [
        "siamese = Siamese('siamese4')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-4-191f04d6fbd2>:24: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "(?, 1947)\n",
            "(?, 1947)\n",
            "WARNING:tensorflow:From <ipython-input-4-191f04d6fbd2>:62: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXmBCVN0V3hv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiameseNetwork(data,siamese,batchSize,epochs,dictionary,lrate):\n",
        "  \n",
        "  #Siamese Network\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    train_Step = tf.train.MomentumOptimizer(lrate,0.05).minimize(siamese.loss)\n",
        "    tf.initialize_all_variables().run()\n",
        "    \n",
        "    while epochs > 0:\n",
        "      \n",
        "      permSet = np.random.permutation(data.shape[0])\n",
        "\n",
        "      for p in range(0,permSet.shape[0],batchSize):\n",
        "\n",
        "        goodSet = np.array(list(range(p,p+batchSize)))\n",
        "        badSetQ = goodSet.copy()\n",
        "        badSetA = np.array(permSet[p:p+batchSize])\n",
        "        ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
        "\n",
        "        _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
        "                      siamese.x1: ques,\n",
        "                      siamese.x2: ans,\n",
        "                      siamese.y_: labl})\n",
        "        \n",
        "        print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
        "      \n",
        "      epochs -= 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scF-b7JY8NQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5579
        },
        "outputId": "ecbefbd6-5223-4cb5-f347-31c6b06e5903"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "trainSiameseNetwork(data,siamese,50,1,three_hash_dict,0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Epoch 1 Batch 0.0 Loss 0.30860764\n",
            "Epoch 1 Batch 1.0 Loss 0.28746507\n",
            "Epoch 1 Batch 2.0 Loss 0.27178162\n",
            "Epoch 1 Batch 3.0 Loss 0.25825813\n",
            "Epoch 1 Batch 4.0 Loss 0.24843316\n",
            "Epoch 1 Batch 5.0 Loss 0.23481923\n",
            "Epoch 1 Batch 6.0 Loss 0.23515265\n",
            "Epoch 1 Batch 7.0 Loss 0.23116662\n",
            "Epoch 1 Batch 8.0 Loss 0.26708224\n",
            "Epoch 1 Batch 9.0 Loss 0.25450057\n",
            "Epoch 1 Batch 10.0 Loss 0.25506774\n",
            "Epoch 1 Batch 11.0 Loss 0.23043825\n",
            "Epoch 1 Batch 12.0 Loss 0.24643072\n",
            "Epoch 1 Batch 13.0 Loss 0.2329013\n",
            "Epoch 1 Batch 14.0 Loss 0.2521363\n",
            "Epoch 1 Batch 15.0 Loss 0.22486123\n",
            "Epoch 1 Batch 16.0 Loss 0.24227102\n",
            "Epoch 1 Batch 17.0 Loss 0.23887943\n",
            "Epoch 1 Batch 18.0 Loss 0.24492584\n",
            "Epoch 1 Batch 19.0 Loss 0.23502699\n",
            "Epoch 1 Batch 20.0 Loss 0.25119036\n",
            "Epoch 1 Batch 21.0 Loss 0.22271456\n",
            "Epoch 1 Batch 22.0 Loss 0.24778572\n",
            "Epoch 1 Batch 23.0 Loss 0.22958587\n",
            "Epoch 1 Batch 24.0 Loss 0.24627209\n",
            "Epoch 1 Batch 25.0 Loss 0.23383701\n",
            "Epoch 1 Batch 26.0 Loss 0.241024\n",
            "Epoch 1 Batch 27.0 Loss 0.24001378\n",
            "Epoch 1 Batch 28.0 Loss 0.23656252\n",
            "Epoch 1 Batch 29.0 Loss 0.26597306\n",
            "Epoch 1 Batch 30.0 Loss 0.27194542\n",
            "Epoch 1 Batch 31.0 Loss 0.24129549\n",
            "Epoch 1 Batch 32.0 Loss 0.2521588\n",
            "Epoch 1 Batch 33.0 Loss 0.26064616\n",
            "Epoch 1 Batch 34.0 Loss 0.27029693\n",
            "Epoch 1 Batch 35.0 Loss 0.2447347\n",
            "Epoch 1 Batch 36.0 Loss 0.25485605\n",
            "Epoch 1 Batch 37.0 Loss 0.25807455\n",
            "Epoch 1 Batch 38.0 Loss 0.2611837\n",
            "Epoch 1 Batch 39.0 Loss 0.2611436\n",
            "Epoch 1 Batch 40.0 Loss 0.23179209\n",
            "Epoch 1 Batch 41.0 Loss 0.24222176\n",
            "Epoch 1 Batch 42.0 Loss 0.25564528\n",
            "Epoch 1 Batch 43.0 Loss 0.24142097\n",
            "Epoch 1 Batch 44.0 Loss 0.26962802\n",
            "Epoch 1 Batch 45.0 Loss 0.24844669\n",
            "Epoch 1 Batch 46.0 Loss 0.24828823\n",
            "Epoch 1 Batch 47.0 Loss 0.25338387\n",
            "Epoch 1 Batch 48.0 Loss 0.263376\n",
            "Epoch 1 Batch 49.0 Loss 0.2645236\n",
            "Epoch 1 Batch 50.0 Loss 0.23742014\n",
            "Epoch 1 Batch 51.0 Loss 0.26534554\n",
            "Epoch 1 Batch 52.0 Loss 0.27456447\n",
            "Epoch 1 Batch 53.0 Loss 0.23581387\n",
            "Epoch 1 Batch 54.0 Loss 0.26076967\n",
            "Epoch 1 Batch 55.0 Loss 0.25328103\n",
            "Epoch 1 Batch 56.0 Loss 0.24482353\n",
            "Epoch 1 Batch 57.0 Loss 0.25036862\n",
            "Epoch 1 Batch 58.0 Loss 0.2669668\n",
            "Epoch 1 Batch 59.0 Loss 0.25447842\n",
            "Epoch 1 Batch 60.0 Loss 0.25985485\n",
            "Epoch 1 Batch 61.0 Loss 0.24938013\n",
            "Epoch 1 Batch 62.0 Loss 0.24598587\n",
            "Epoch 1 Batch 63.0 Loss 0.2271964\n",
            "Epoch 1 Batch 64.0 Loss 0.25884017\n",
            "Epoch 1 Batch 65.0 Loss 0.2433518\n",
            "Epoch 1 Batch 66.0 Loss 0.24496739\n",
            "Epoch 1 Batch 67.0 Loss 0.26150188\n",
            "Epoch 1 Batch 68.0 Loss 0.25587127\n",
            "Epoch 1 Batch 69.0 Loss 0.25769073\n",
            "Epoch 1 Batch 70.0 Loss 0.23445411\n",
            "Epoch 1 Batch 71.0 Loss 0.2517738\n",
            "Epoch 1 Batch 72.0 Loss 0.2549932\n",
            "Epoch 1 Batch 73.0 Loss 0.25967225\n",
            "Epoch 1 Batch 74.0 Loss 0.26249126\n",
            "Epoch 1 Batch 75.0 Loss 0.25929883\n",
            "Epoch 1 Batch 76.0 Loss 0.27199727\n",
            "Epoch 1 Batch 77.0 Loss 0.24547046\n",
            "Epoch 1 Batch 78.0 Loss 0.26387495\n",
            "Epoch 1 Batch 79.0 Loss 0.25588948\n",
            "Epoch 1 Batch 80.0 Loss 0.2771334\n",
            "Epoch 1 Batch 81.0 Loss 0.26218694\n",
            "Epoch 1 Batch 82.0 Loss 0.25796258\n",
            "Epoch 1 Batch 83.0 Loss 0.242484\n",
            "Epoch 1 Batch 84.0 Loss 0.2554872\n",
            "Epoch 1 Batch 85.0 Loss 0.25402024\n",
            "Epoch 1 Batch 86.0 Loss 0.2540664\n",
            "Epoch 1 Batch 87.0 Loss 0.25609857\n",
            "Epoch 1 Batch 88.0 Loss 0.24934964\n",
            "Epoch 1 Batch 89.0 Loss 0.26475084\n",
            "Epoch 1 Batch 90.0 Loss 0.26995328\n",
            "Epoch 1 Batch 91.0 Loss 0.2469454\n",
            "Epoch 1 Batch 92.0 Loss 0.24824014\n",
            "Epoch 1 Batch 93.0 Loss 0.2582794\n",
            "Epoch 1 Batch 94.0 Loss 0.24817187\n",
            "Epoch 1 Batch 95.0 Loss 0.25225085\n",
            "Epoch 1 Batch 96.0 Loss 0.23992853\n",
            "Epoch 1 Batch 97.0 Loss 0.23907785\n",
            "Epoch 1 Batch 98.0 Loss 0.2558391\n",
            "Epoch 1 Batch 99.0 Loss 0.24956442\n",
            "Epoch 1 Batch 100.0 Loss 0.27290696\n",
            "Epoch 1 Batch 101.0 Loss 0.2601806\n",
            "Epoch 1 Batch 102.0 Loss 0.2729498\n",
            "Epoch 1 Batch 103.0 Loss 0.2513393\n",
            "Epoch 1 Batch 104.0 Loss 0.25294852\n",
            "Epoch 1 Batch 105.0 Loss 0.2752969\n",
            "Epoch 1 Batch 106.0 Loss 0.26674137\n",
            "Epoch 1 Batch 107.0 Loss 0.25944734\n",
            "Epoch 1 Batch 108.0 Loss 0.2603536\n",
            "Epoch 1 Batch 109.0 Loss 0.26036578\n",
            "Epoch 1 Batch 110.0 Loss 0.2532569\n",
            "Epoch 1 Batch 111.0 Loss 0.24418758\n",
            "Epoch 1 Batch 112.0 Loss 0.24047785\n",
            "Epoch 1 Batch 113.0 Loss 0.259908\n",
            "Epoch 1 Batch 114.0 Loss 0.25872996\n",
            "Epoch 1 Batch 115.0 Loss 0.24597895\n",
            "Epoch 1 Batch 116.0 Loss 0.25271225\n",
            "Epoch 1 Batch 117.0 Loss 0.2433652\n",
            "Epoch 1 Batch 118.0 Loss 0.25716242\n",
            "Epoch 1 Batch 119.0 Loss 0.24352448\n",
            "Epoch 1 Batch 120.0 Loss 0.25711906\n",
            "Epoch 1 Batch 121.0 Loss 0.25596273\n",
            "Epoch 1 Batch 122.0 Loss 0.25498715\n",
            "Epoch 1 Batch 123.0 Loss 0.25521\n",
            "Epoch 1 Batch 124.0 Loss 0.2538375\n",
            "Epoch 1 Batch 125.0 Loss 0.24721913\n",
            "Epoch 1 Batch 126.0 Loss 0.26363865\n",
            "Epoch 1 Batch 127.0 Loss 0.24350648\n",
            "Epoch 1 Batch 128.0 Loss 0.23532076\n",
            "Epoch 1 Batch 129.0 Loss 0.26633084\n",
            "Epoch 1 Batch 130.0 Loss 0.25148714\n",
            "Epoch 1 Batch 131.0 Loss 0.24664217\n",
            "Epoch 1 Batch 132.0 Loss 0.24917835\n",
            "Epoch 1 Batch 133.0 Loss 0.2589042\n",
            "Epoch 1 Batch 134.0 Loss 0.2472607\n",
            "Epoch 1 Batch 135.0 Loss 0.2637475\n",
            "Epoch 1 Batch 136.0 Loss 0.24664114\n",
            "Epoch 1 Batch 137.0 Loss 0.25541055\n",
            "Epoch 1 Batch 138.0 Loss 0.25159326\n",
            "Epoch 1 Batch 139.0 Loss 0.26088998\n",
            "Epoch 1 Batch 140.0 Loss 0.24125236\n",
            "Epoch 1 Batch 141.0 Loss 0.25899127\n",
            "Epoch 1 Batch 142.0 Loss 0.26853374\n",
            "Epoch 1 Batch 143.0 Loss 0.2535225\n",
            "Epoch 1 Batch 144.0 Loss 0.2481983\n",
            "Epoch 1 Batch 145.0 Loss 0.25102836\n",
            "Epoch 1 Batch 146.0 Loss 0.24563152\n",
            "Epoch 1 Batch 147.0 Loss 0.2573582\n",
            "Epoch 1 Batch 148.0 Loss 0.24565025\n",
            "Epoch 1 Batch 149.0 Loss 0.2721954\n",
            "Epoch 1 Batch 150.0 Loss 0.2530547\n",
            "Epoch 1 Batch 151.0 Loss 0.2545554\n",
            "Epoch 1 Batch 152.0 Loss 0.25519073\n",
            "Epoch 1 Batch 153.0 Loss 0.2503933\n",
            "Epoch 1 Batch 154.0 Loss 0.24879624\n",
            "Epoch 1 Batch 155.0 Loss 0.23707987\n",
            "Epoch 1 Batch 156.0 Loss 0.2561832\n",
            "Epoch 1 Batch 157.0 Loss 0.24404179\n",
            "Epoch 1 Batch 158.0 Loss 0.2620017\n",
            "Epoch 1 Batch 159.0 Loss 0.26449817\n",
            "Epoch 1 Batch 160.0 Loss 0.26815435\n",
            "Epoch 1 Batch 161.0 Loss 0.2621597\n",
            "Epoch 1 Batch 162.0 Loss 0.2581367\n",
            "Epoch 1 Batch 163.0 Loss 0.25117925\n",
            "Epoch 1 Batch 164.0 Loss 0.255752\n",
            "Epoch 1 Batch 165.0 Loss 0.25901952\n",
            "Epoch 1 Batch 166.0 Loss 0.27341014\n",
            "Epoch 1 Batch 167.0 Loss 0.23880598\n",
            "Epoch 1 Batch 168.0 Loss 0.2523174\n",
            "Epoch 1 Batch 169.0 Loss 0.2621187\n",
            "Epoch 1 Batch 170.0 Loss 0.25305662\n",
            "Epoch 1 Batch 171.0 Loss 0.24434103\n",
            "Epoch 1 Batch 172.0 Loss 0.2595426\n",
            "Epoch 1 Batch 173.0 Loss 0.25543326\n",
            "Epoch 1 Batch 174.0 Loss 0.25802398\n",
            "Epoch 1 Batch 175.0 Loss 0.26541948\n",
            "Epoch 1 Batch 176.0 Loss 0.24674264\n",
            "Epoch 1 Batch 177.0 Loss 0.25494972\n",
            "Epoch 1 Batch 178.0 Loss 0.254252\n",
            "Epoch 1 Batch 179.0 Loss 0.2664979\n",
            "Epoch 1 Batch 180.0 Loss 0.233976\n",
            "Epoch 1 Batch 181.0 Loss 0.25986907\n",
            "Epoch 1 Batch 182.0 Loss 0.25463524\n",
            "Epoch 1 Batch 183.0 Loss 0.26369545\n",
            "Epoch 1 Batch 184.0 Loss 0.2601766\n",
            "Epoch 1 Batch 185.0 Loss 0.25335717\n",
            "Epoch 1 Batch 186.0 Loss 0.24869928\n",
            "Epoch 1 Batch 187.0 Loss 0.24022001\n",
            "Epoch 1 Batch 188.0 Loss 0.26201296\n",
            "Epoch 1 Batch 189.0 Loss 0.24561991\n",
            "Epoch 1 Batch 190.0 Loss 0.24633743\n",
            "Epoch 1 Batch 191.0 Loss 0.2549318\n",
            "Epoch 1 Batch 192.0 Loss 0.25651282\n",
            "Epoch 1 Batch 193.0 Loss 0.2646015\n",
            "Epoch 1 Batch 194.0 Loss 0.25035256\n",
            "Epoch 1 Batch 195.0 Loss 0.2566625\n",
            "Epoch 1 Batch 196.0 Loss 0.25304553\n",
            "Epoch 1 Batch 197.0 Loss 0.2535254\n",
            "Epoch 1 Batch 198.0 Loss 0.2515636\n",
            "Epoch 1 Batch 199.0 Loss 0.24799041\n",
            "Epoch 1 Batch 200.0 Loss 0.2517428\n",
            "Epoch 1 Batch 201.0 Loss 0.25064942\n",
            "Epoch 1 Batch 202.0 Loss 0.25020596\n",
            "Epoch 1 Batch 203.0 Loss 0.2430429\n",
            "Epoch 1 Batch 204.0 Loss 0.27268046\n",
            "Epoch 1 Batch 205.0 Loss 0.26416394\n",
            "Epoch 1 Batch 206.0 Loss 0.26339546\n",
            "Epoch 1 Batch 207.0 Loss 0.25324363\n",
            "Epoch 1 Batch 208.0 Loss 0.25514406\n",
            "Epoch 1 Batch 209.0 Loss 0.25178424\n",
            "Epoch 1 Batch 210.0 Loss 0.25929698\n",
            "Epoch 1 Batch 211.0 Loss 0.24362093\n",
            "Epoch 1 Batch 212.0 Loss 0.24547003\n",
            "Epoch 1 Batch 213.0 Loss 0.251627\n",
            "Epoch 1 Batch 214.0 Loss 0.24679075\n",
            "Epoch 1 Batch 215.0 Loss 0.27019233\n",
            "Epoch 1 Batch 216.0 Loss 0.2653742\n",
            "Epoch 1 Batch 217.0 Loss 0.24462265\n",
            "Epoch 1 Batch 218.0 Loss 0.24196756\n",
            "Epoch 1 Batch 219.0 Loss 0.25837594\n",
            "Epoch 1 Batch 220.0 Loss 0.2366106\n",
            "Epoch 1 Batch 221.0 Loss 0.24641618\n",
            "Epoch 1 Batch 222.0 Loss 0.26370963\n",
            "Epoch 1 Batch 223.0 Loss 0.25166696\n",
            "Epoch 1 Batch 224.0 Loss 0.26198807\n",
            "Epoch 1 Batch 225.0 Loss 0.27316803\n",
            "Epoch 1 Batch 226.0 Loss 0.2615433\n",
            "Epoch 1 Batch 227.0 Loss 0.24969387\n",
            "Epoch 1 Batch 228.0 Loss 0.25235936\n",
            "Epoch 1 Batch 229.0 Loss 0.24805099\n",
            "Epoch 1 Batch 230.0 Loss 0.25730887\n",
            "Epoch 1 Batch 231.0 Loss 0.24702758\n",
            "Epoch 1 Batch 232.0 Loss 0.25507143\n",
            "Epoch 1 Batch 233.0 Loss 0.2612871\n",
            "Epoch 1 Batch 234.0 Loss 0.23150724\n",
            "Epoch 1 Batch 235.0 Loss 0.24619333\n",
            "Epoch 1 Batch 236.0 Loss 0.25212348\n",
            "Epoch 1 Batch 237.0 Loss 0.2612492\n",
            "Epoch 1 Batch 238.0 Loss 0.2469171\n",
            "Epoch 1 Batch 239.0 Loss 0.23115925\n",
            "Epoch 1 Batch 240.0 Loss 0.2599335\n",
            "Epoch 1 Batch 241.0 Loss 0.26375568\n",
            "Epoch 1 Batch 242.0 Loss 0.2429846\n",
            "Epoch 1 Batch 243.0 Loss 0.2599401\n",
            "Epoch 1 Batch 244.0 Loss 0.25077334\n",
            "Epoch 1 Batch 245.0 Loss 0.27510324\n",
            "Epoch 1 Batch 246.0 Loss 0.24739845\n",
            "Epoch 1 Batch 247.0 Loss 0.25609434\n",
            "Epoch 1 Batch 248.0 Loss 0.26702422\n",
            "Epoch 1 Batch 249.0 Loss 0.262572\n",
            "Epoch 1 Batch 250.0 Loss 0.24167942\n",
            "Epoch 1 Batch 251.0 Loss 0.25303993\n",
            "Epoch 1 Batch 252.0 Loss 0.26314765\n",
            "Epoch 1 Batch 253.0 Loss 0.2516503\n",
            "Epoch 1 Batch 254.0 Loss 0.24834208\n",
            "Epoch 1 Batch 255.0 Loss 0.2453277\n",
            "Epoch 1 Batch 256.0 Loss 0.26478288\n",
            "Epoch 1 Batch 257.0 Loss 0.2560256\n",
            "Epoch 1 Batch 258.0 Loss 0.24418764\n",
            "Epoch 1 Batch 259.0 Loss 0.26061252\n",
            "Epoch 1 Batch 260.0 Loss 0.25272626\n",
            "Epoch 1 Batch 261.0 Loss 0.2660261\n",
            "Epoch 1 Batch 262.0 Loss 0.25507846\n",
            "Epoch 1 Batch 263.0 Loss 0.2545809\n",
            "Epoch 1 Batch 264.0 Loss 0.2505049\n",
            "Epoch 1 Batch 265.0 Loss 0.24700508\n",
            "Epoch 1 Batch 266.0 Loss 0.256333\n",
            "Epoch 1 Batch 267.0 Loss 0.24496937\n",
            "Epoch 1 Batch 268.0 Loss 0.2527054\n",
            "Epoch 1 Batch 269.0 Loss 0.23376256\n",
            "Epoch 1 Batch 270.0 Loss 0.26889592\n",
            "Epoch 1 Batch 271.0 Loss 0.26559192\n",
            "Epoch 1 Batch 272.0 Loss 0.24980801\n",
            "Epoch 1 Batch 273.0 Loss 0.24923152\n",
            "Epoch 1 Batch 274.0 Loss 0.25521046\n",
            "Epoch 1 Batch 275.0 Loss 0.24577469\n",
            "Epoch 1 Batch 276.0 Loss 0.24572381\n",
            "Epoch 1 Batch 277.0 Loss 0.24152558\n",
            "Epoch 1 Batch 278.0 Loss 0.2553526\n",
            "Epoch 1 Batch 279.0 Loss 0.2562064\n",
            "Epoch 1 Batch 280.0 Loss 0.2523599\n",
            "Epoch 1 Batch 281.0 Loss 0.24905656\n",
            "Epoch 1 Batch 282.0 Loss 0.23982738\n",
            "Epoch 1 Batch 283.0 Loss 0.25339448\n",
            "Epoch 1 Batch 284.0 Loss 0.2507455\n",
            "Epoch 1 Batch 285.0 Loss 0.25747478\n",
            "Epoch 1 Batch 286.0 Loss 0.24825014\n",
            "Epoch 1 Batch 287.0 Loss 0.25237373\n",
            "Epoch 1 Batch 288.0 Loss 0.23755594\n",
            "Epoch 1 Batch 289.0 Loss 0.24563004\n",
            "Epoch 1 Batch 290.0 Loss 0.25603673\n",
            "Epoch 1 Batch 291.0 Loss 0.24580096\n",
            "Epoch 1 Batch 292.0 Loss 0.2449565\n",
            "Epoch 1 Batch 293.0 Loss 0.24485575\n",
            "Epoch 1 Batch 294.0 Loss 0.23771687\n",
            "Epoch 1 Batch 295.0 Loss 0.25202644\n",
            "Epoch 1 Batch 296.0 Loss 0.23863654\n",
            "Epoch 1 Batch 297.0 Loss 0.24074723\n",
            "Epoch 1 Batch 298.0 Loss 0.25325158\n",
            "Epoch 1 Batch 299.0 Loss 0.25680986\n",
            "Epoch 1 Batch 300.0 Loss 0.26076078\n",
            "Epoch 1 Batch 301.0 Loss 0.25186384\n",
            "Epoch 1 Batch 302.0 Loss 0.25792554\n",
            "Epoch 1 Batch 303.0 Loss 0.25172883\n",
            "Epoch 1 Batch 304.0 Loss 0.25139213\n",
            "Epoch 1 Batch 305.0 Loss 0.25642273\n",
            "Epoch 1 Batch 306.0 Loss 0.25554913\n",
            "Epoch 1 Batch 307.0 Loss 0.2560557\n",
            "Epoch 1 Batch 308.0 Loss 0.27352315\n",
            "Epoch 1 Batch 309.0 Loss 0.2592825\n",
            "Epoch 1 Batch 310.0 Loss 0.27541757\n",
            "Epoch 1 Batch 311.0 Loss 0.24434416\n",
            "Epoch 1 Batch 312.0 Loss 0.2432588\n",
            "Epoch 1 Batch 313.0 Loss 0.24484095\n",
            "Epoch 1 Batch 314.0 Loss 0.25771037\n",
            "Epoch 1 Batch 315.0 Loss 0.25142136\n",
            "Epoch 1 Batch 316.0 Loss 0.25006297\n",
            "Epoch 1 Batch 317.0 Loss 0.24336721\n",
            "Epoch 1 Batch 318.0 Loss 0.24566635\n",
            "Epoch 1 Batch 319.0 Loss 0.24730873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "InihtcorSzMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "17c5f10c-e626-4fe7-e64c-890d0ba454eb"
      },
      "cell_type": "code",
      "source": [
        "label"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "oadBrReYKBPU",
        "colab_type": "code",
        "outputId": "b53a87c0-f7cc-4d45-f85b-a319a168ef52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare data and tf.session\n",
        "# mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() \n",
        "vectorizer = CountVectorizer()\n",
        "tokenizer=vectorizer.build_tokenizer()\n",
        "rowcount = 0\n",
        "qList,aList = [],[]\n",
        "for row in data[0:100]:\n",
        "  rowcount += 1\n",
        "  quesVec,ansVec = [0]*64881,[0]*64881\n",
        "  quesHash = getThreeHash(row[1].lower())\n",
        "  ansHash = getThreeHash(row[3].lower())\n",
        "#   print(quesHash)\n",
        "  for token in tokenizer(quesHash.lower()):\n",
        "     quesVec[three_hash_dict[token]] += 1\n",
        "  for token in tokenizer(ansHash.lower()):\n",
        "     ansVec[three_hash_dict[token]] += 1\n",
        "  qList.append(quesVec)\n",
        "  aList.append(ansVec)\n",
        "  if rowcount % 10 == 0:    \n",
        "    trainSiamese(qList,aList)\n",
        "    qList,aList = [],[]\n",
        "sess.close()\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 1947)\n",
            "(?, 1947)\n",
            "step 0: loss 9.297\n",
            "step 10: loss 0.676\n",
            "step 20: loss 0.385\n",
            "step 30: loss 0.253\n",
            "step 40: loss 0.169\n",
            "step 50: loss 0.113\n",
            "step 60: loss 0.075\n",
            "step 70: loss 0.050\n",
            "step 80: loss 0.034\n",
            "step 90: loss 0.022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0445ed5850b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0maList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrowcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mqList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3db983ea131f>\u001b[0m in \u001b[0;36mtrainSiamese\u001b[0;34m(quesVec, ansVec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquesVec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mansVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# setup siamese network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msiamese\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Siamese\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64881\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mactivated_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmaxpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxp_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-91aa96e5ce29>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(self, name, inputs, cur_channel)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(prev_channel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_w\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable Siamese/conv_1_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-19-91aa96e5ce29>\", line 36, in conv_layer\n    w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 21, in network\n    activated_conv1 = self.conv_layer('conv_1',x,3)\n  File \"<ipython-input-19-91aa96e5ce29>\", line 9, in __init__\n    self.o1 = self.network(self.x1)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qdLocomkdJ4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}